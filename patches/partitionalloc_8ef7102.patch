diff --git a/BUILD.gn b/BUILD.gn
index 189069b..ca653e9 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -7,6 +7,7 @@ import("//build/buildflag_header.gni")
 import("//build/config/android/config.gni")
 import("//build/config/chromecast_build.gni")
 import("//build/config/chromeos/ui_mode.gni")
+import("//build/config/compiler/compiler.gni")
 import("//build/config/dcheck_always_on.gni")
 import("//build/config/logging.gni")
 
@@ -33,6 +34,46 @@ config("memory_tagging") {
   }
 }
 
+# Used to shim malloc symbols on Android. see //base/allocator/README.md.
+config("wrap_malloc_symbols") {
+  ldflags = [
+    "-Wl,-wrap,calloc",
+    "-Wl,-wrap,free",
+    "-Wl,-wrap,malloc",
+    "-Wl,-wrap,memalign",
+    "-Wl,-wrap,posix_memalign",
+    "-Wl,-wrap,pvalloc",
+    "-Wl,-wrap,realloc",
+    "-Wl,-wrap,valloc",
+
+    # Not allocating memory, but part of the API
+    "-Wl,-wrap,malloc_usable_size",
+
+    # <stdlib.h> functions
+    "-Wl,-wrap,realpath",
+
+    # <string.h> functions
+    "-Wl,-wrap,strdup",
+    "-Wl,-wrap,strndup",
+
+    # <unistd.h> functions
+    "-Wl,-wrap,getcwd",
+
+    # <stdio.h> functions
+    "-Wl,-wrap,asprintf",
+    "-Wl,-wrap,vasprintf",
+  ]
+}
+
+config("mac_no_default_new_delete_symbols") {
+  if (!is_component_build) {
+    # This is already set when we compile libc++, see
+    # buildtools/third_party/libc++/BUILD.gn. But it needs to be set here as
+    # well, since the shim defines the symbols, to prevent them being exported.
+    cflags = [ "-fvisibility-global-new-delete-hidden" ]
+  }
+}
+
 if (is_fuchsia) {
   config("fuchsia_sync_lib") {
     libs = [
@@ -47,7 +88,29 @@ if (enable_pkeys && is_debug) {
   }
 }
 
+_remove_configs = []
+_add_configs = []
+if (!is_debug || partition_alloc_optimized_debug) {
+  _remove_configs += [ "//build/config/compiler:default_optimization" ]
+
+  # Partition alloc is relatively hot (>1% of cycles for users of CrOS).
+  # Use speed-focused optimizations for it.
+  _add_configs += [ "//build/config/compiler:optimize_speed" ]
+} else {
+  _remove_configs += [ "//build/config/compiler:default_optimization" ]
+  _add_configs += [ "//build/config/compiler:no_optimize" ]
+}
+
 component("partition_alloc") {
+  public_deps = [
+    ":allocator_core",
+    ":allocator_shim",
+  ]
+}
+
+source_set("allocator_core") {
+  visibility = [ ":*" ]
+
   sources = [
     "address_pool_manager.cc",
     "address_pool_manager.h",
@@ -317,6 +380,9 @@ component("partition_alloc") {
     } else if (current_cpu == "arm64") {
       assert(pcscan_stack_supported)
       sources += [ "starscan/stack/asm/arm64/push_registers_asm.cc" ]
+    } else if (current_cpu == "riscv64") {
+      assert(pcscan_stack_supported)
+      sources += [ "starscan/stack/asm/riscv64/push_registers_asm.cc" ]
     } else {
       # To support a trampoline for another arch, please refer to v8/src/heap/base.
       assert(!pcscan_stack_supported)
@@ -370,13 +436,8 @@ component("partition_alloc") {
   }
 
   configs += [ "//build/config/compiler:wexit_time_destructors" ]
-
-  # Partition alloc is relatively hot (>1% of cycles for users of CrOS). Use speed-focused
-  # optimizations for it.
-  if (!is_debug) {
-    configs -= [ "//build/config/compiler:default_optimization" ]
-    configs += [ "//build/config/compiler:optimize_speed" ]
-  }
+  configs -= _remove_configs
+  configs += _add_configs
 
   # We want to be able to test pkey mode without access to the default pkey.
   # This is incompatible with stack protectors since the TLS won't be pkey-tagged.
@@ -385,6 +446,99 @@ component("partition_alloc") {
   }
 }
 
+source_set("allocator_shim") {
+  visibility = [ ":*" ]
+
+  sources = []
+  deps = []
+  all_dependent_configs = []
+  configs += [ ":partition_alloc_implementation" ]
+
+  configs -= _remove_configs
+  configs += _add_configs
+
+  if (use_allocator_shim) {
+    sources += [
+      "shim/allocator_shim.cc",
+      "shim/allocator_shim.h",
+      "shim/allocator_shim_internals.h",
+    ]
+    if (use_partition_alloc) {
+      sources += [
+        "shim/allocator_shim_default_dispatch_to_partition_alloc.cc",
+        "shim/allocator_shim_default_dispatch_to_partition_alloc.h",
+        "shim/nonscannable_allocator.cc",
+        "shim/nonscannable_allocator.h",
+      ]
+    }
+    if (is_android) {
+      sources += [
+        "shim/allocator_shim_override_cpp_symbols.h",
+        "shim/allocator_shim_override_linker_wrapped_symbols.h",
+      ]
+      all_dependent_configs += [ ":wrap_malloc_symbols" ]
+    }
+    if (is_apple) {
+      sources += [
+        "shim/allocator_shim_override_mac_default_zone.h",
+        "shim/allocator_shim_override_mac_symbols.h",
+        "shim/early_zone_registration_constants.h",
+      ]
+      configs += [ ":mac_no_default_new_delete_symbols" ]
+    }
+    if (is_chromeos || is_linux) {
+      sources += [
+        "shim/allocator_shim_override_cpp_symbols.h",
+        "shim/allocator_shim_override_glibc_weak_symbols.h",
+        "shim/allocator_shim_override_libc_symbols.h",
+      ]
+    }
+    if (is_win) {
+      sources += [
+        "shim/allocator_shim_override_ucrt_symbols_win.h",
+        "shim/winheap_stubs_win.cc",
+        "shim/winheap_stubs_win.h",
+      ]
+    }
+
+    if (!use_partition_alloc_as_malloc) {
+      if (is_android) {
+        sources += [
+          "shim/allocator_shim_default_dispatch_to_linker_wrapped_symbols.cc",
+        ]
+      }
+      if (is_apple) {
+        sources +=
+            [ "shim/allocator_shim_default_dispatch_to_mac_zoned_malloc.cc" ]
+      }
+      if (is_chromeos || is_linux) {
+        sources += [ "shim/allocator_shim_default_dispatch_to_glibc.cc" ]
+      }
+      if (is_win) {
+        sources += [ "shim/allocator_shim_default_dispatch_to_winheap.cc" ]
+      }
+    }
+
+    deps += [
+      ":allocator_core",
+      ":buildflags",
+    ]
+  }
+
+  if (is_apple) {
+    sources += [
+      "shim/allocator_interception_mac.h",
+      "shim/allocator_interception_mac.mm",
+      "shim/malloc_zone_functions_mac.cc",
+      "shim/malloc_zone_functions_mac.h",
+    ]
+    deps += [
+      ":allocator_core",
+      ":buildflags",
+    ]
+  }
+}
+
 source_set("raw_ptr") {
   # `gn check` is unhappy with most `#includes` when PA isn't
   # actually built.
@@ -411,6 +565,8 @@ source_set("raw_ptr") {
       "pointers/raw_ptr_asan_unowned_impl.cc",
       "pointers/raw_ptr_asan_unowned_impl.h",
     ]
+  } else {
+    sources += [ "pointers/raw_ptr_noop_impl.h" ]
   }
   if (use_partition_alloc) {
     public_deps = [ ":partition_alloc" ]
@@ -425,6 +581,9 @@ source_set("raw_ptr") {
   if (build_with_chromium) {
     visibility = [ "//base" ]
   }
+
+  configs -= _remove_configs
+  configs += _add_configs
 }
 
 buildflag_header("partition_alloc_buildflags") {
@@ -491,6 +650,31 @@ buildflag_header("partition_alloc_buildflags") {
 
     "ENABLE_PKEYS=$enable_pkeys",
     "ENABLE_THREAD_ISOLATION=$enable_pkeys",
+
+    "TMEMK_ENABLE=$tmemk_enable",
+    "TMEMK_ALIGN_AT_SHIM=$tmemk_align_at_shim",
+    "TMEMK_STARTING_TAGS=$tmemk_starting_tags",
+    "TMEMK_STARTING_TAGS_SAME=$tmemk_starting_tags_same",
+    "TMEMK_STARTING_TAGS_RANDOM=$tmemk_starting_tags_random",
+    "TMEMK_INTEGRITY=$tmemk_integrity",
+    "TMEMK_QUARANTINING=$tmemk_quarantining",
+    "TMEMK_ENABLE_DEBUG=$tmemk_enable_debug",
+    "TMEMK_RETURNNULL=$tmemk_returnnull",
+    "TMEMK_NOTAGGING=$tmemk_notagging",
+    "TMEMK_ENCRYPTION=$tmemk_encryption",
+    "TMEMK_NOALIAS=$tmemk_noalias",
+    "TMEMK_PADDING=$tmemk_padding",
+    "TMEMK_KEYBITS=$tmemk_keybits",
+    "TMEMK_MAXTAGSIZE=$tmemk_maxtagsize",
+    "TMEMK_THREAD_ISOLATION=$tmemk_thread_isolation",
+    "TMEMK_TRACE=$tmemk_trace",
+    "TMEMK_TRIPWIRES=$tmemk_tripwires",
+    "TMEMK_NEVER_INCREMENT=$tmemk_never_increment",
+    "TMEMK_FLUSH_OLD_AFTER_FREE=$tmemk_flush_old_after_free",
+    "TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT=$tmemk_skip_memset_when_never_increment",
+    "TMEMK_TRIPWIRE_MAX=$tmemk_tripwire_max",
+    "TMEMK_MEMZERO_INSTEAD_OF_MEMCPY_AT_FREE=$tmemk_memzero_instead_of_memcpy_at_free",
+    "TMEMK_MEMZERO_BEFORE_DECOMMIT=$tmemk_memzero_before_decommit",
   ]
 
   if (is_apple) {
@@ -522,7 +706,10 @@ buildflag_header("debugging_buildflags") {
 
   # Duplicates the setup Chromium uses to define `DCHECK_IS_ON()`,
   # but avails it as a buildflag.
-  _dcheck_is_on = is_debug || dcheck_always_on
+  _dcheck_is_on = is_debug || dcheck_always_on || tmemk_override_debug
+  _dcheck_is_on = false || tmemk_override_debug
+  # enable_expensive_dchecks = _dcheck_is_on
+  enable_expensive_dchecks = false || tmemk_override_debug_expensive_checks
 
   flags = [
     "PA_DCHECK_IS_ON=$_dcheck_is_on",
diff --git a/address_pool_manager.cc b/address_pool_manager.cc
index 63a9c46..2ec0336 100644
--- a/address_pool_manager.cc
+++ b/address_pool_manager.cc
@@ -34,6 +34,12 @@ AddressPoolManager& AddressPoolManager::GetInstance() {
   return singleton_;
 }
 
+namespace {
+// Allocations are all performed on behalf of PartitionAlloc.
+constexpr PageTag kPageTag = PageTag::kPartitionAlloc;
+
+}  // namespace
+
 #if BUILDFLAG(HAS_64_BIT_POINTERS)
 
 namespace {
@@ -43,7 +49,7 @@ void DecommitPages(uintptr_t address, size_t size) {
   // Callers rely on the pages being zero-initialized when recommitting them.
   // |DecommitSystemPages| doesn't guarantee this on all operating systems, in
   // particular on macOS, but |DecommitAndZeroSystemPages| does.
-  DecommitAndZeroSystemPages(address, size);
+  DecommitAndZeroSystemPages(address, size, kPageTag);
 }
 
 }  // namespace
@@ -357,7 +363,7 @@ uintptr_t AddressPoolManager::Reserve(pool_handle handle,
       AllocPages(requested_address, length, kSuperPageSize,
                  PageAccessibilityConfiguration(
                      PageAccessibilityConfiguration::kInaccessible),
-                 PageTag::kPartitionAlloc);
+                 kPageTag);
   return address;
 }
 
diff --git a/address_pool_manager_bitmap.h b/address_pool_manager_bitmap.h
index e0f75ae..9459a0a 100644
--- a/address_pool_manager_bitmap.h
+++ b/address_pool_manager_bitmap.h
@@ -147,6 +147,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) AddressPoolManagerBitmap {
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAlloc(uintptr_t address) {
+  address = internal::UntagAddr(address);
   // When ENABLE_BACKUP_REF_PTR_SUPPORT is off, BRP pool isn't used.
   // No need to add IsManagedByConfigurablePool, because Configurable Pool
   // doesn't exist on 32-bit.
@@ -162,16 +163,19 @@ PA_ALWAYS_INLINE bool IsManagedByPartitionAlloc(uintptr_t address) {
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocRegularPool(uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::AddressPoolManagerBitmap::IsManagedByRegularPool(address);
 }
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocBRPPool(uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::AddressPoolManagerBitmap::IsManagedByBRPPool(address);
 }
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocConfigurablePool(
+  address = internal::UntagAddr(address);
     uintptr_t address) {
   // The Configurable Pool is only available on 64-bit builds.
   return false;
diff --git a/address_pool_manager_unittest.cc b/address_pool_manager_unittest.cc
index 21be980..c020c08 100644
--- a/address_pool_manager_unittest.cc
+++ b/address_pool_manager_unittest.cc
@@ -19,7 +19,7 @@ namespace partition_alloc::internal {
 class AddressSpaceStatsDumperForTesting final : public AddressSpaceStatsDumper {
  public:
   AddressSpaceStatsDumperForTesting() = default;
-  ~AddressSpaceStatsDumperForTesting() = default;
+  ~AddressSpaceStatsDumperForTesting() final = default;
 
   void DumpStats(
       const partition_alloc::AddressSpaceStats* address_space_stats) override {
diff --git a/address_space_randomization.h b/address_space_randomization.h
index cc69f0d..78e667c 100644
--- a/address_space_randomization.h
+++ b/address_space_randomization.h
@@ -38,7 +38,19 @@ AslrMask(uintptr_t bits) {
 
 #if defined(ARCH_CPU_64_BITS)
 
-  #if defined(MEMORY_TOOL_REPLACES_ALLOCATOR)
+  #if 1 /*BUILDFLAG(TMEMK_ENABLE)*/
+
+      PA_ALWAYS_INLINE constexpr uintptr_t ASLRMask() {
+        return AslrMask(38);
+        // return AslrMask(0);
+      // return AslrAddress(0x4000000000ULL);
+      }
+      PA_ALWAYS_INLINE constexpr uintptr_t ASLROffset() {
+        return AslrAddress(0);
+        // return AslrAddress(0x4000000000ULL);
+      }
+
+  #elif defined(MEMORY_TOOL_REPLACES_ALLOCATOR)
 
     // We shouldn't allocate system pages at all for sanitizer builds. However,
     // we do, and if random hint addresses interfere with address ranges
diff --git a/address_space_stats.h b/address_space_stats.h
index a603f43..0c3c205 100644
--- a/address_space_stats.h
+++ b/address_space_stats.h
@@ -47,6 +47,7 @@ struct AddressSpaceStats {
 class PA_COMPONENT_EXPORT(PARTITION_ALLOC) AddressSpaceStatsDumper {
  public:
   virtual void DumpStats(const AddressSpaceStats* address_space_stats) = 0;
+  virtual ~AddressSpaceStatsDumper() = default;
 };
 
 }  // namespace partition_alloc
diff --git a/build_overrides/partition_alloc.gni b/build_overrides/partition_alloc.gni
index 19fd2de..f508cbd 100644
--- a/build_overrides/partition_alloc.gni
+++ b/build_overrides/partition_alloc.gni
@@ -10,8 +10,10 @@ import("//build_overrides/build.gni")
 # //build_overrides/partition_alloc.gni and define their own PartitionAlloc
 # configuration.
 
-use_partition_alloc_as_malloc_default = false
-use_allocator_shim_default = false
+#use_partition_alloc_as_malloc_default = false
+use_partition_alloc_as_malloc_default = true
+#use_allocator_shim_default = false
+use_allocator_shim_default = true
 enable_backup_ref_ptr_support_default = false
 put_ref_count_in_previous_slot_default = true
 enable_backup_ref_ptr_slow_checks_default = false
diff --git a/compressed_pointer_unittest.cc b/compressed_pointer_unittest.cc
index 03418a8..874529b 100644
--- a/compressed_pointer_unittest.cc
+++ b/compressed_pointer_unittest.cc
@@ -26,15 +26,13 @@ struct DerivedWithMixin : Base, Mixin {
   double d;
 };
 
-using PAAllocator = internal::PartitionAllocator;
-
 struct PADeleter final {
   void operator()(void* ptr) const { allocator_.root()->Free(ptr); }
-  PAAllocator& allocator_;
+  PartitionAllocator& allocator_;
 };
 
 template <typename T, typename... Args>
-std::unique_ptr<T, PADeleter> make_pa_unique(PAAllocator& alloc,
+std::unique_ptr<T, PADeleter> make_pa_unique(PartitionAllocator& alloc,
                                              Args&&... args) {
   T* result = new (alloc.root()->Alloc(sizeof(T), nullptr))
       T(std::forward<Args>(args)...);
@@ -42,7 +40,7 @@ std::unique_ptr<T, PADeleter> make_pa_unique(PAAllocator& alloc,
 }
 
 template <typename T>
-std::unique_ptr<T[], PADeleter> make_pa_array_unique(PAAllocator& alloc,
+std::unique_ptr<T[], PADeleter> make_pa_array_unique(PartitionAllocator& alloc,
                                                      size_t num) {
   T* result = new (alloc.root()->Alloc(sizeof(T) * num, nullptr)) T();
   return std::unique_ptr<T[], PADeleter>(result, PADeleter{alloc});
@@ -88,7 +86,7 @@ class CompressedPointerTest : public ::testing::Test {
   CompressedPointerTest() { allocator_.init(PartitionOptions{}); }
 
  protected:
-  internal::PartitionAllocator allocator_;
+  PartitionAllocator allocator_;
 };
 
 #if BUILDFLAG(ENABLE_POINTER_COMPRESSION)
diff --git a/extended_api.cc b/extended_api.cc
index 79bbafe..d0e30f5 100644
--- a/extended_api.cc
+++ b/extended_api.cc
@@ -15,7 +15,7 @@ namespace partition_alloc::internal {
 
 namespace {
 
-void DisableThreadCacheForRootIfEnabled(ThreadSafePartitionRoot* root) {
+void DisableThreadCacheForRootIfEnabled(PartitionRoot* root) {
   // Some platforms don't have a thread cache, or it could already have been
   // disabled.
   if (!root || !root->settings.with_thread_cache) {
@@ -29,14 +29,12 @@ void DisableThreadCacheForRootIfEnabled(ThreadSafePartitionRoot* root) {
   // time. For the main thread, we leak it.
 }
 
-void EnablePartitionAllocThreadCacheForRootIfDisabled(
-    ThreadSafePartitionRoot* root) {
+void EnablePartitionAllocThreadCacheForRootIfDisabled(PartitionRoot* root) {
   if (!root) {
     return;
   }
   root->settings.with_thread_cache = true;
 }
-
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
 void DisablePartitionAllocThreadCacheForProcess() {
   auto* regular_allocator =
@@ -66,7 +64,7 @@ ThreadAllocStats GetAllocStatsForCurrentThread() {
 
 #if PA_CONFIG(THREAD_CACHE_SUPPORTED)
 ThreadCacheProcessScopeForTesting::ThreadCacheProcessScopeForTesting(
-    ThreadSafePartitionRoot* root)
+    PartitionRoot* root)
     : root_(root) {
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
   auto* regular_allocator =
diff --git a/extended_api.h b/extended_api.h
index 7146c10..a99677d 100644
--- a/extended_api.h
+++ b/extended_api.h
@@ -25,13 +25,13 @@ ThreadAllocStats GetAllocStatsForCurrentThread();
 // in the process.
 class ThreadCacheProcessScopeForTesting {
  public:
-  explicit ThreadCacheProcessScopeForTesting(ThreadSafePartitionRoot* root);
+  explicit ThreadCacheProcessScopeForTesting(PartitionRoot* root);
   ~ThreadCacheProcessScopeForTesting();
 
   ThreadCacheProcessScopeForTesting() = delete;
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
   bool regular_was_enabled_ = false;
 #endif
diff --git a/freeslot_bitmap.h b/freeslot_bitmap.h
index f86ef59..f6d5feb 100644
--- a/freeslot_bitmap.h
+++ b/freeslot_bitmap.h
@@ -72,6 +72,7 @@ PA_ALWAYS_INLINE void FreeSlotBitmapMarkSlotAsUsed(uintptr_t slot_start) {
 
 // Mark the bit corresponding to |slot_start| as free( = 1).
 PA_ALWAYS_INLINE void FreeSlotBitmapMarkSlotAsFree(uintptr_t slot_start) {
+  PA_DCHECK(slot_start == internal::UntagAddr(slot_start));
   PA_CHECK(FreeSlotBitmapSlotIsUsed(slot_start));
   auto [cell, bit_index] = GetFreeSlotBitmapCellPtrAndBitIndex(slot_start);
   *cell |= CellWithAOne(bit_index);
diff --git a/glossary.md b/glossary.md
index ded74bd..763164c 100644
--- a/glossary.md
+++ b/glossary.md
@@ -109,6 +109,9 @@ each term depends mainly upon previously defined ones.
     per-thread permissions. At the moment, this is implemented for pkeys on x64.
     It's primary user is [V8 CFI][v8-cfi].
 
+![The singular AddressPoolManager mediates access to the separate pools
+  for each PartitionRoot.](./dot/address-space.png)
+
 *** promo
 Pools are downgraded into a logical concept in 32-bit environments,
 tracking a non-contiguous set of allocations using a bitmap.
diff --git a/gwp_asan_support.cc b/gwp_asan_support.cc
index 0e88024..99bde65 100644
--- a/gwp_asan_support.cc
+++ b/gwp_asan_support.cc
@@ -27,7 +27,7 @@ void* GwpAsanSupport::MapRegion(size_t slot_count,
   constexpr PartitionOptions kConfig{
       .backup_ref_ptr = PartitionOptions::BackupRefPtr::kEnabled,
   };
-  static internal::base::NoDestructor<ThreadSafePartitionRoot> root(kConfig);
+  static internal::base::NoDestructor<PartitionRoot> root(kConfig);
 
   const size_t kSlotSize = 2 * internal::SystemPageSize();
   uint16_t bucket_index = PartitionRoot::SizeToBucketIndex(
diff --git a/page_allocator.cc b/page_allocator.cc
index aaa6906..6268ffa 100644
--- a/page_allocator.cc
+++ b/page_allocator.cc
@@ -304,13 +304,19 @@ void DecommitSystemPages(
                       accessibility_disposition);
 }
 
-void DecommitAndZeroSystemPages(uintptr_t address, size_t length) {
+void DecommitAndZeroSystemPages(uintptr_t address,
+                                size_t length,
+                                PageTag page_tag) {
   PA_DCHECK(!(address & internal::SystemPageOffsetMask()));
   PA_DCHECK(!(length & internal::SystemPageOffsetMask()));
-  internal::DecommitAndZeroSystemPagesInternal(address, length);
+  internal::DecommitAndZeroSystemPagesInternal(address, length, page_tag);
 }
-void DecommitAndZeroSystemPages(void* address, size_t length) {
-  DecommitAndZeroSystemPages(reinterpret_cast<uintptr_t>(address), length);
+
+void DecommitAndZeroSystemPages(void* address,
+                                size_t length,
+                                PageTag page_tag) {
+  DecommitAndZeroSystemPages(reinterpret_cast<uintptr_t>(address), length,
+                             page_tag);
 }
 
 void RecommitSystemPages(
diff --git a/page_allocator.h b/page_allocator.h
index fd958dd..8e366cc 100644
--- a/page_allocator.h
+++ b/page_allocator.h
@@ -69,20 +69,29 @@ enum class PageAccessibilityDisposition {
   kAllowKeepForPerf,
 };
 
-// macOS supports tagged memory regions, to help in debugging. On Android,
-// these tags are used to name anonymous mappings.
+// Some platforms (including macOS and some Linux-based ones) support tagged
+// memory regions, to help in debugging. On Android, these tags are used to name
+// anonymous mappings.
+//
+// kChromium is the default value, used to distinguish general
+// Chromium-originated allocations from other ones (e.g. from platform
+// libraries).
 enum class PageTag {
-  kFirst = 240,           // Minimum tag value.
   kSimulation = 251,      // Memory simulator tool.
   kBlinkGC = 252,         // Blink GC pages.
   kPartitionAlloc = 253,  // PartitionAlloc, no matter the partition.
   kChromium = 254,        // Chromium page.
   kV8 = 255,              // V8 heap pages.
-  kLast = kV8             // Maximum tag value.
+
+  kFirst = kSimulation,  // Minimum tag value.
+  kLast = kV8            // Maximum tag value.
 };
 
 // See
 // https://github.com/apple-oss-distributions/xnu/blob/5c2921b07a2480ab43ec66f5b9e41cb872bc554f/osfmk/mach/vm_statistics.h#L687
+static_assert(static_cast<int>(PageTag::kLast) >= 240,
+              "The first application-reserved tag on macOS is 240, see "
+              "vm_statistics.h in XNU.");
 static_assert(
     static_cast<int>(PageTag::kLast) < 256,
     "Tags are only 1 byte long on macOS, see vm_statistics.h in XNU.");
@@ -109,7 +118,7 @@ uintptr_t NextAlignedWithOffset(uintptr_t ptr,
 // PageAccessibilityConfiguration::kInaccessible means uncommitted.
 //
 // |page_tag| is used on some platforms to identify the source of the
-// allocation. Use PageTag::kChromium as a catch-all category.
+// allocation.
 //
 // |file_descriptor_for_shared_alloc| is only used in mapping the shadow
 // pools to the same physical address as the real one in
@@ -120,20 +129,20 @@ PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 uintptr_t AllocPages(size_t length,
                      size_t align,
                      PageAccessibilityConfiguration accessibility,
-                     PageTag page_tag,
+                     PageTag page_tag = PageTag::kChromium,
                      int file_descriptor_for_shared_alloc = -1);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 uintptr_t AllocPages(uintptr_t address,
                      size_t length,
                      size_t align,
                      PageAccessibilityConfiguration accessibility,
-                     PageTag page_tag);
+                     PageTag page_tag = PageTag::kChromium);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 void* AllocPages(void* address,
                  size_t length,
                  size_t align,
                  PageAccessibilityConfiguration accessibility,
-                 PageTag page_tag);
+                 PageTag page_tag = PageTag::kChromium);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 uintptr_t AllocPagesWithAlignOffset(
     uintptr_t address,
@@ -141,7 +150,7 @@ uintptr_t AllocPagesWithAlignOffset(
     size_t align,
     size_t align_offset,
     PageAccessibilityConfiguration page_accessibility,
-    PageTag page_tag,
+    PageTag page_tag = PageTag::kChromium,
     int file_descriptor_for_shared_alloc = -1);
 
 // Frees one or more pages starting at |address| and continuing for |length|
@@ -237,9 +246,13 @@ void DecommitSystemPages(
 //
 // This API will crash if the operation cannot be performed.
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
-void DecommitAndZeroSystemPages(uintptr_t address, size_t length);
+void DecommitAndZeroSystemPages(uintptr_t address,
+                                size_t length,
+                                PageTag page_tag = PageTag::kChromium);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
-void DecommitAndZeroSystemPages(void* address, size_t length);
+void DecommitAndZeroSystemPages(void* address,
+                                size_t length,
+                                PageTag page_tag = PageTag::kChromium);
 
 // Whether decommitted memory is guaranteed to be zeroed when it is
 // recommitted. Do not assume that this will not change over time.
diff --git a/page_allocator_constants.h b/page_allocator_constants.h
index 05c99c4..380042d 100644
--- a/page_allocator_constants.h
+++ b/page_allocator_constants.h
@@ -63,6 +63,17 @@ extern PageCharacteristics page_characteristics;
 
 #endif
 
+// Ability to name anonymous VMAs is available on some, but not all Linux-based
+// systems.
+#if BUILDFLAG(IS_ANDROID) || BUILDFLAG(IS_LINUX)
+#include <sys/prctl.h>
+
+#if defined(PR_SET_VMA) && defined(PR_SET_VMA_ANON_NAME)
+#define LINUX_NAME_REGION 1
+#endif
+
+#endif  // BUILDFLAG(IS_ANDROID) || BUILDFLAG(IS_LINUX)
+
 namespace partition_alloc::internal {
 
 // Forward declaration, implementation below
diff --git a/page_allocator_internals_fuchsia.h b/page_allocator_internals_fuchsia.h
index 22cfae4..8c996a6 100644
--- a/page_allocator_internals_fuchsia.h
+++ b/page_allocator_internals_fuchsia.h
@@ -67,7 +67,6 @@ const char* PageTagToName(PageTag tag) {
       return "cr_chromium";
     case PageTag::kV8:
       return "cr_v8";
-    case PageTag::kFirst:
     case PageTag::kSimulation:
       PA_NOTREACHED();
   }
@@ -224,7 +223,9 @@ void DecommitSystemPagesInternal(
   DiscardSystemPagesInternal(address, length);
 }
 
-void DecommitAndZeroSystemPagesInternal(uintptr_t address, size_t length) {
+void DecommitAndZeroSystemPagesInternal(uintptr_t address,
+                                        size_t length,
+                                        PageTag page_tag) {
   SetSystemPagesAccess(address, length,
                        PageAccessibilityConfiguration(
                            PageAccessibilityConfiguration::kInaccessible));
diff --git a/page_allocator_internals_posix.h b/page_allocator_internals_posix.h
index 8bf4a64..fff0ef6 100644
--- a/page_allocator_internals_posix.h
+++ b/page_allocator_internals_posix.h
@@ -15,10 +15,13 @@
 
 #include "base/allocator/partition_allocator/oom.h"
 #include "base/allocator/partition_allocator/page_allocator.h"
+#include "base/allocator/partition_allocator/page_allocator_constants.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/debug/debugging_buildflags.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/posix/eintr_wrapper.h"
 #include "base/allocator/partition_allocator/partition_alloc_check.h"
+#include "base/allocator/partition_allocator/partition_alloc_notreached.h"
 #include "base/allocator/partition_allocator/thread_isolation/thread_isolation.h"
+#include "base/allocator/partition_allocator/tagging.h"
 #include "build/build_config.h"
 
 #if BUILDFLAG(IS_APPLE)
@@ -43,6 +46,10 @@
 #include <sys/resource.h>
 #endif
 
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
+#include "base/allocator/partition_allocator/tagging.h"
+#include "base/allocator/partition_allocator/partition_address_space.h"
+
 #ifndef MAP_ANONYMOUS
 #define MAP_ANONYMOUS MAP_ANON
 #endif
@@ -63,31 +70,54 @@ namespace partition_alloc::internal {
 
 namespace {
 
-#if BUILDFLAG(IS_ANDROID) || BUILDFLAG(IS_LINUX)
-#if defined(PR_SET_VMA) && defined(PR_SET_VMA_ANON_NAME)
-const char* PageTagToName(PageTag tag) {
+#if defined(LINUX_NAME_REGION)
+
+void NameRegion(void* start, size_t length, PageTag page_tag) {
   // Important: All the names should be string literals. As per prctl.h in
   // //third_party/android_toolchain the kernel keeps a pointer to the name
   // instead of copying it.
   //
   // Having the name in .rodata ensures that the pointer remains valid as
   // long as the mapping is alive.
-  switch (tag) {
+  const char* name = nullptr;
+  switch (page_tag) {
+    case PageTag::kSimulation:
+      name = "simulation";
+      break;
     case PageTag::kBlinkGC:
-      return "blink_gc";
+      name = "blink_gc";
+      break;
     case PageTag::kPartitionAlloc:
-      return "partition_alloc";
+      name = "partition_alloc";
+      break;
     case PageTag::kChromium:
-      return "chromium";
+      name = "chromium";
+      break;
     case PageTag::kV8:
-      return "v8";
+      name = "v8";
+      break;
     default:
-      PA_DCHECK(false);
-      return "";
+      PA_NOTREACHED();
+      break;
   }
+
+  // No error checking on purpose, testing only.
+  prctl(PR_SET_VMA, PR_SET_VMA_ANON_NAME, start, length, name);
+}
+
+#endif  // defined(LINUX_NAME_REGION)
+[[maybe_unused]] const char* AccessibilityToName(PageAccessibilityConfiguration c) {
+  auto p = c.permissions;
+  if(p == PageAccessibilityConfiguration::kInaccessible            ) return "kInaccessible";
+  if(p == PageAccessibilityConfiguration::kInaccessibleWillJitLater) return "kInaccessibleWillJitLater";
+  if(p == PageAccessibilityConfiguration::kRead                    ) return "kRead";
+  if(p == PageAccessibilityConfiguration::kReadWrite               ) return "kReadWrite";
+  if(p == PageAccessibilityConfiguration::kReadWriteTagged         ) return "kReadWriteTagged";
+  if(p == PageAccessibilityConfiguration::kReadExecuteProtected    ) return "kReadExecuteProtected";
+  if(p == PageAccessibilityConfiguration::kReadExecute             ) return "kReadExecute";
+  if(p == PageAccessibilityConfiguration::kReadWriteExecute        ) return "kReadWriteExecute";
+  return "";
 }
-#endif
-#endif  // BUILDFLAG(IS_ANDROID)
 
 #if BUILDFLAG(IS_MAC)
 // Tests whether the version of macOS supports the MAP_JIT flag and if the
@@ -168,8 +198,6 @@ uintptr_t SystemAllocPagesInternal(uintptr_t hint,
 #if BUILDFLAG(IS_APPLE)
   // Use a custom tag to make it easier to distinguish Partition Alloc regions
   // in vmmap(1). Tags between 240-255 are supported.
-  PA_DCHECK(PageTag::kFirst <= page_tag);
-  PA_DCHECK(PageTag::kLast >= page_tag);
   int fd = file_descriptor_for_shared_alloc == -1
                ? VM_MAKE_TAG(static_cast<int>(page_tag))
                : file_descriptor_for_shared_alloc;
@@ -180,6 +208,20 @@ uintptr_t SystemAllocPagesInternal(uintptr_t hint,
   int access_flag = GetAccessFlags(accessibility);
   int map_flags = MAP_ANONYMOUS | MAP_PRIVATE;
 
+#if BUILDFLAG(TMEMK_ENABLE)
+  tmemk_allocate_keys(length);
+#if !BUILDFLAG(TMEMK_NOALIAS)
+  assert(fd == -1);
+  fd = tmemk_get_memfd();
+
+
+  if(fd != -1){
+    map_flags = MAP_SHARED;
+    // map_flags += MAP_FIXED;
+  }
+#endif /*BUILDFLAG(TMEMK_NOALIAS)*/
+#endif /*BUILDFLAG(TMEMK_ENABLE)*/
+
 #if BUILDFLAG(IS_APPLE)
   // On macOS 10.14 and higher, executables that are code signed with the
   // "runtime" option cannot execute writable memory by default. They can opt
@@ -195,21 +237,25 @@ uintptr_t SystemAllocPagesInternal(uintptr_t hint,
 
   void* ret = mmap(reinterpret_cast<void*>(hint), length, access_flag,
                    map_flags, fd, 0);
+#if BUILDFLAG(TMEMK_ENABLE)
+  __debug("SystemAllocPagesInternal: do we want to create the aliases already here?. ret=%p hint=%p length=0x%lx page_tag=%d fd=%d permissions=%s", ret, hint, length, page_tag, fd, AccessibilityToName(accessibility));
+  PA_DCHECK(length <= kPoolMaxSize + kSuperPageSize);
+  if(hint & kPtrTagMask || (hint + length) & kPtrTagMask || (hint + kPoolMaxSize) & kPtrTagMask){
+    __debug("ERROR hint is wrong. clashing with tags");
+    ret = nullptr;
+  }
+#endif
   if (ret == MAP_FAILED) {
     s_allocPageErrorCode = errno;
     ret = nullptr;
   }
 
-#if BUILDFLAG(IS_ANDROID) || BUILDFLAG(IS_LINUX)
-#if defined(PR_SET_VMA) && defined(PR_SET_VMA_ANON_NAME)
-  // On Android and Linux, anonymous mappings can have a name attached to them.
-  // This is useful for debugging, and double-checking memory attribution.
+
+
+#if defined(LINUX_NAME_REGION)
   if (ret) {
-    // No error checking on purpose, testing only.
-    prctl(PR_SET_VMA, PR_SET_VMA_ANON_NAME, ret, length,
-          PageTagToName(page_tag));
+    NameRegion(ret, length, page_tag);
   }
-#endif
 #endif
 
   return reinterpret_cast<uintptr_t>(ret);
@@ -219,7 +265,37 @@ bool TrySetSystemPagesAccessInternal(
     uintptr_t address,
     size_t length,
     PageAccessibilityConfiguration accessibility) {
+
+
+#if BUILDFLAG(TMEMK_ENABLE)
+  PartitionAddressSpace::PoolInfo poolinfo = GetPoolAndOffset(address);
+  assert(poolinfo.handle == kRegularPoolHandle);
+  // if(poolinfo.handle == internal::kRegularPoolHandle){..}
+
+  if(accessibility.permissions == PageAccessibilityConfiguration::kReadWriteTagged){
+    int commit = 1;
+    tmemk_create_aliases_for_memory(address, length, GetAccessFlags(accessibility), 0, -1, (size_t)poolinfo.offset, commit);
+  }else if (accessibility.permissions == PageAccessibilityConfiguration::kReadWrite){
+    // __debug("TrySetSystemPagesAccessInternal: not tagged, creating no aliases");
+  }
+  else if(accessibility.permissions == PageAccessibilityConfiguration::kInaccessible
+     || accessibility.permissions == PageAccessibilityConfiguration::kInaccessibleWillJitLater)
+  {
+#if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+
+      internal::MemZeroWithMovdir64B_q(address, length);
+
+#endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+  }else{
+    __debug("TrySetSystemPagesAccessInternal: unknown accessibility.permissions=%s", AccessibilityToName(accessibility));
+  }
+#endif
+
+
 #if BUILDFLAG(ENABLE_THREAD_ISOLATION)
+#if BUILDFLAG(TMEMK_ENABLE)
+XXX not yet implemented
+#endif
   if (accessibility.thread_isolation.enabled) {
     return 0 == MprotectWithThreadIsolation(reinterpret_cast<void*>(address),
                                             length,
@@ -235,8 +311,27 @@ void SetSystemPagesAccessInternal(
     uintptr_t address,
     size_t length,
     PageAccessibilityConfiguration accessibility) {
+
+
+
+
+
+
   int access_flags = GetAccessFlags(accessibility);
   int ret;
+
+#if BUILDFLAG(TMEMK_ENABLE)
+  if(accessibility.permissions == PageAccessibilityConfiguration::kInaccessible
+     || accessibility.permissions == PageAccessibilityConfiguration::kInaccessibleWillJitLater)
+  {
+#if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+
+    internal::MemZeroWithMovdir64B_q(address, length);
+
+#endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+  }
+#endif
+
 #if BUILDFLAG(ENABLE_THREAD_ISOLATION)
   if (accessibility.thread_isolation.enabled) {
     ret = MprotectWithThreadIsolation(reinterpret_cast<void*>(address), length,
@@ -245,6 +340,7 @@ void SetSystemPagesAccessInternal(
   } else
 #endif  // BUILDFLAG(ENABLE_THREAD_ISOLATION)
   {
+
     ret = PA_HANDLE_EINTR(mprotect(reinterpret_cast<void*>(address), length,
                                    GetAccessFlags(accessibility)));
   }
@@ -270,6 +366,7 @@ void SetSystemPagesAccessInternal(
 }
 
 void FreePagesInternal(uintptr_t address, size_t length) {
+
   PA_PCHECK(0 == munmap(reinterpret_cast<void*>(address), length));
 }
 
@@ -336,17 +433,36 @@ void DecommitSystemPagesInternal(
   }
 }
 
-void DecommitAndZeroSystemPagesInternal(uintptr_t address, size_t length) {
+void DecommitAndZeroSystemPagesInternal(uintptr_t address,
+                                        size_t length,
+                                        PageTag page_tag) {
+  int fd = -1;
+#if BUILDFLAG(IS_APPLE)
+  fd = VM_MAKE_TAG(static_cast<int>(page_tag));
+#endif
+
   // https://pubs.opengroup.org/onlinepubs/9699919799/functions/mmap.html: "If
   // a MAP_FIXED request is successful, then any previous mappings [...] for
   // those whole pages containing any part of the address range [pa,pa+len)
   // shall be removed, as if by an appropriate call to munmap(), before the
   // new mapping is established." As a consequence, the memory will be
   // zero-initialized on next access.
-  void* ptr = reinterpret_cast<void*>(address);
-  void* ret = mmap(ptr, length, PROT_NONE,
-                   MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
-  PA_CHECK(ptr == ret);
+  PartitionAddressSpace::PoolInfo poolinfo = GetPoolAndOffset(address);
+  if(poolinfo.handle == internal::kRegularPoolHandle && BUILDFLAG(TMEMK_ENABLE)){ //internal::IsManagedByRegularPool(address)
+    // __debug("DecommitAndZeroSystemPagesInternal address=%p length=0x%lx", address, length);
+
+    uintptr_t offset = poolinfo.offset;
+    int commit = 0;
+    tmemk_create_aliases_for_memory(address, length, PROT_NONE, -1, -1, offset, commit);
+  }else{
+    void* ptr = reinterpret_cast<void*>(address);
+    void* ret = mmap(ptr, length, PROT_NONE,
+                     MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE, fd, 0);
+    PA_CHECK(ptr == ret);  // Since we just remapped the region, need to set is name again.
+#if defined(LINUX_NAME_REGION)
+  NameRegion(ret, length, page_tag);
+#endif
+  }
 }
 
 void RecommitSystemPagesInternal(
@@ -395,7 +511,7 @@ bool TryRecommitSystemPagesInternal(
 }
 
 void DiscardSystemPagesInternal(uintptr_t address, size_t length) {
-  void* ptr = reinterpret_cast<void*>(address);
+  [[maybe_unused]] void* ptr = reinterpret_cast<void*>(address);
 #if BUILDFLAG(IS_APPLE)
   int ret = madvise(ptr, length, MADV_FREE_REUSABLE);
   if (ret) {
@@ -410,6 +526,14 @@ void DiscardSystemPagesInternal(uintptr_t address, size_t length) {
   // performance benefits unclear.
   //
   // Therefore, we just do the simple thing: MADV_DONTNEED.
+  __debug("DiscardSystemPagesInternal address=%p length=0x%lx", ptr, length);
+
+
+#if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT) && BUILDFLAG(TMEMK_ENABLE)
+  __debug("DiscardSystemPagesInternal: zeroing pages address=%p length=0x%lx", ptr, length);
+  internal::MemZeroWithMovdir64B_q((uintptr_t)ptr, length);
+#endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+
   PA_PCHECK(0 == madvise(ptr, length, MADV_DONTNEED));
 #endif  // BUILDFLAG(IS_APPLE)
 }
diff --git a/page_allocator_internals_win.h b/page_allocator_internals_win.h
index a06eb1f..ec70187 100644
--- a/page_allocator_internals_win.h
+++ b/page_allocator_internals_win.h
@@ -189,7 +189,9 @@ void DecommitSystemPagesInternal(
                            PageAccessibilityConfiguration::kInaccessible));
 }
 
-void DecommitAndZeroSystemPagesInternal(uintptr_t address, size_t length) {
+void DecommitAndZeroSystemPagesInternal(uintptr_t address,
+                                        size_t length,
+                                        PageTag page_tag) {
   // https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtualfree:
   // "If a page is decommitted but not released, its state changes to reserved.
   // Subsequently, you can call VirtualAlloc to commit it, or VirtualFree to
diff --git a/page_allocator_unittest.cc b/page_allocator_unittest.cc
index 80b4752..fa20ecc 100644
--- a/page_allocator_unittest.cc
+++ b/page_allocator_unittest.cc
@@ -13,6 +13,7 @@
 #include <vector>
 
 #include "base/allocator/partition_allocator/address_space_randomization.h"
+#include "base/allocator/partition_allocator/page_allocator_constants.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/cpu.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
 #include "base/allocator/partition_allocator/partition_alloc_config.h"
@@ -20,9 +21,10 @@
 #include "base/allocator/partition_allocator/tagging.h"
 #include "build/build_config.h"
 
-#if BUILDFLAG(IS_ANDROID)
+#if defined(LINUX_NAME_REGION)
 #include "base/debug/proc_maps_linux.h"
-#endif  // BUILDFLAG(IS_ANDROID)
+#endif
+
 #include "testing/gtest/include/gtest/gtest.h"
 
 #if BUILDFLAG(IS_POSIX)
@@ -45,16 +47,6 @@
 
 namespace partition_alloc::internal {
 
-#if BUILDFLAG(IS_ANDROID)
-namespace base::debug {
-
-using ::base::debug::MappedMemoryRegion;
-using ::base::debug::ParseProcMaps;
-using ::base::debug::ReadProcMaps;
-
-}  // namespace base::debug
-#endif
-
 namespace {
 
 // Any number of bytes that can be allocated with no trouble.
@@ -490,33 +482,47 @@ TEST(PartitionAllocPageAllocatorTest, MAYBE_ReadExecutePages) {
 
 #endif  // BUILDFLAG(IS_POSIX)
 
-#if BUILDFLAG(IS_ANDROID)
+#if defined(LINUX_NAME_REGION)
 TEST(PartitionAllocPageAllocatorTest, PageTagging) {
+  size_t size = PageAllocationGranularity();
   uintptr_t buffer =
-      AllocPages(PageAllocationGranularity(), PageAllocationGranularity(),
+      AllocPages(size, PageAllocationGranularity(),
                  PageAccessibilityConfiguration(
                      PageAccessibilityConfiguration::kInaccessible),
                  PageTag::kChromium);
-  EXPECT_TRUE(buffer);
+  ASSERT_TRUE(buffer);
 
-  std::string proc_maps;
-  EXPECT_TRUE(base::debug::ReadProcMaps(&proc_maps));
-  std::vector<base::debug::MappedMemoryRegion> regions;
-  EXPECT_TRUE(base::debug::ParseProcMaps(proc_maps, &regions));
-
-  bool found = false;
-  for (const auto& region : regions) {
-    if (region.start == buffer) {
-      found = true;
-      EXPECT_EQ("[anon:chromium]", region.path);
-      break;
+  auto is_region_named = [](uintptr_t start_address) {
+    std::string proc_maps;
+    EXPECT_TRUE(::base::debug::ReadProcMaps(&proc_maps));
+    std::vector<::base::debug::MappedMemoryRegion> regions;
+    EXPECT_TRUE(::base::debug::ParseProcMaps(proc_maps, &regions));
+
+    bool found = false;
+    for (const auto& region : regions) {
+      if (region.start == start_address) {
+        found = true;
+        return "[anon:chromium]" == region.path;
+      }
     }
-  }
+    EXPECT_TRUE(found);
+    return false;
+  };
 
-  FreePages(buffer, PageAllocationGranularity());
-  EXPECT_TRUE(found);
+  bool before = is_region_named(buffer);
+  DecommitAndZeroSystemPages(buffer, size);
+  bool after = is_region_named(buffer);
+
+#if BUILDFLAG(IS_ANDROID)
+  EXPECT_TRUE(before) << "VMA tagging should always work on Android";
+#endif
+  // When not running on Android, the prctl() command may be defined in the
+  // headers, but not be implemented by the host kernel.
+  EXPECT_EQ(before, after);
+
+  FreePages(buffer, size);
 }
-#endif  // BUILDFLAG(IS_ANDROID)
+#endif  // defined(LINUX_NAME_REGION)
 
 TEST(PartitionAllocPageAllocatorTest, DecommitErasesMemory) {
   if (!DecommittedMemoryIsAlwaysZeroed()) {
diff --git a/partition_address_space.h b/partition_address_space.h
index 8b20e5f..c5598d2 100644
--- a/partition_address_space.h
+++ b/partition_address_space.h
@@ -5,10 +5,8 @@
 #ifndef BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_ADDRESS_SPACE_H_
 #define BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_ADDRESS_SPACE_H_
 
-#include <algorithm>
-#include <array>
 #include <cstddef>
-#include <limits>
+#include <utility>
 
 #include "base/allocator/partition_allocator/address_pool_manager_types.h"
 #include "base/allocator/partition_allocator/page_allocator_constants.h"
@@ -23,9 +21,12 @@
 #include "base/allocator/partition_allocator/partition_alloc_notreached.h"
 #include "base/allocator/partition_allocator/tagging.h"
 #include "base/allocator/partition_allocator/thread_isolation/alignment.h"
-#include "base/allocator/partition_allocator/thread_isolation/thread_isolation.h"
 #include "build/build_config.h"
 
+#if BUILDFLAG(ENABLE_THREAD_ISOLATION)
+#include "base/allocator/partition_allocator/thread_isolation/thread_isolation.h"
+#endif
+
 // The feature is not applicable to 32-bit address space.
 #if BUILDFLAG(HAS_64_BIT_POINTERS)
 
@@ -37,6 +38,12 @@ namespace internal {
 // See `glossary.md`.
 class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAddressSpace {
  public:
+  // Represents pool-specific information about a given address.
+  struct PoolInfo {
+    pool_handle handle;
+    uintptr_t offset;
+  };
+
 #if PA_CONFIG(DYNAMICALLY_SELECT_POOL_SIZE)
   PA_ALWAYS_INLINE static uintptr_t RegularPoolBaseMask() {
     return setup_.regular_pool_base_mask_;
@@ -47,8 +54,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAddressSpace {
   }
 #endif
 
-  PA_ALWAYS_INLINE static std::pair<pool_handle, uintptr_t> GetPoolAndOffset(
-      uintptr_t address) {
+  PA_ALWAYS_INLINE static PoolInfo GetPoolAndOffset(uintptr_t address) {
     // When USE_BACKUP_REF_PTR is off, BRP pool isn't used.
 #if !BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
     PA_DCHECK(!IsInBRPPool(address));
@@ -75,7 +81,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAddressSpace {
     } else {
       PA_NOTREACHED();
     }
-    return std::make_pair(pool, address - base);
+    return PoolInfo{.handle = pool, .offset = address - base};
   }
   PA_ALWAYS_INLINE static constexpr size_t ConfigurablePoolMaxSize() {
     return kConfigurablePoolMaxSize;
@@ -367,13 +373,13 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAddressSpace {
 #endif
 };
 
-PA_ALWAYS_INLINE std::pair<pool_handle, uintptr_t> GetPoolAndOffset(
+PA_ALWAYS_INLINE PartitionAddressSpace::PoolInfo GetPoolAndOffset(
     uintptr_t address) {
   return PartitionAddressSpace::GetPoolAndOffset(address);
 }
 
 PA_ALWAYS_INLINE pool_handle GetPool(uintptr_t address) {
-  return std::get<0>(GetPoolAndOffset(address));
+  return GetPoolAndOffset(address).handle;
 }
 
 PA_ALWAYS_INLINE uintptr_t OffsetInBRPPool(uintptr_t address) {
@@ -390,6 +396,7 @@ PA_ALWAYS_INLINE std::ptrdiff_t ShadowPoolOffset(pool_handle pool) {
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAlloc(uintptr_t address) {
+  address = internal::UntagAddr(address);
   // When ENABLE_BACKUP_REF_PTR_SUPPORT is off, BRP pool isn't used.
 #if !BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
   PA_DCHECK(!internal::PartitionAddressSpace::IsInBRPPool(address));
@@ -406,11 +413,13 @@ PA_ALWAYS_INLINE bool IsManagedByPartitionAlloc(uintptr_t address) {
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocRegularPool(uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::PartitionAddressSpace::IsInRegularPool(address);
 }
 
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocBRPPool(uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::PartitionAddressSpace::IsInBRPPool(address);
 }
 
@@ -418,6 +427,7 @@ PA_ALWAYS_INLINE bool IsManagedByPartitionAllocBRPPool(uintptr_t address) {
 // Checks whether the address belongs to either regular or BRP pool.
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocCorePools(uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::PartitionAddressSpace::IsInCorePools(address);
 }
 #endif  // BUILDFLAG(GLUE_CORE_POOLS)
@@ -425,6 +435,7 @@ PA_ALWAYS_INLINE bool IsManagedByPartitionAllocCorePools(uintptr_t address) {
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocConfigurablePool(
     uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::PartitionAddressSpace::IsInConfigurablePool(address);
 }
 
@@ -432,6 +443,7 @@ PA_ALWAYS_INLINE bool IsManagedByPartitionAllocConfigurablePool(
 // Returns false for nullptr.
 PA_ALWAYS_INLINE bool IsManagedByPartitionAllocThreadIsolatedPool(
     uintptr_t address) {
+  address = internal::UntagAddr(address);
   return internal::PartitionAddressSpace::IsInThreadIsolatedPool(address);
 }
 #endif
diff --git a/partition_alloc-inl.h b/partition_alloc-inl.h
index 4dfa4d2..4cbfcb6 100644
--- a/partition_alloc-inl.h
+++ b/partition_alloc-inl.h
@@ -90,13 +90,9 @@ PA_ALWAYS_INLINE uintptr_t ObjectPtr2Addr(const void* object) {
   // TODO(bartekn): Check that |object| is indeed an object start.
   return ObjectInnerPtr2Addr(object);
 }
-PA_ALWAYS_INLINE void* SlotStartAddr2Ptr(uintptr_t slot_start) {
+PA_ALWAYS_INLINE uintptr_t SlotStartPtr2Addr(const TaggedSlot slot_start) {
   // TODO(bartekn): Check that |slot_start| is indeed a slot start.
-  return TagAddr(slot_start);
-}
-PA_ALWAYS_INLINE uintptr_t SlotStartPtr2Addr(const void* slot_start) {
-  // TODO(bartekn): Check that |slot_start| is indeed a slot start.
-  return UntagPtr(slot_start);
+  return UntagSlot(slot_start).value();
 }
 
 }  // namespace partition_alloc::internal
diff --git a/partition_alloc.cc b/partition_alloc.cc
index ff1771f..5c44e10 100644
--- a/partition_alloc.cc
+++ b/partition_alloc.cc
@@ -106,7 +106,7 @@ void PartitionAllocGlobalUninitForTesting() {
   internal::g_oom_handling_function = nullptr;
 }
 
-namespace internal {
+PartitionAllocator::PartitionAllocator() = default;
 
 PartitionAllocator::~PartitionAllocator() {
   MemoryReclaimer::Instance()->UnregisterPartition(&partition_root_);
@@ -121,6 +121,4 @@ void PartitionAllocator::init(PartitionOptions opts) {
   MemoryReclaimer::Instance()->RegisterPartition(&partition_root_);
 }
 
-}  // namespace internal
-
 }  // namespace partition_alloc
diff --git a/partition_alloc.gni b/partition_alloc.gni
index 7334337..915fb68 100644
--- a/partition_alloc.gni
+++ b/partition_alloc.gni
@@ -52,14 +52,26 @@ declare_args() {
 }
 
 declare_args() {
+  # Turns on compiler optimizations in PartitionAlloc in Debug build.
+  # If enabling PartitionAlloc-Everywhere in Debug build for tests in Debug
+  # build, since all memory allocations and deallocations are executed by
+  # non-optimized PartitionAlloc, chrome (including tests) will be much
+  # slower. This will cause debug trybots' timeouts. If we want to debug
+  # PartitionAlloc itself, use partition_alloc_optimized_debug=false.
+  # Otherwise, use partition_alloc_optimized_debug=true to enable optimized
+  # PartitionAlloc.
+  partition_alloc_optimized_debug = true
+
   # PartitionAlloc-Everywhere (PA-E).
-  use_partition_alloc_as_malloc =
-      use_partition_alloc && use_partition_alloc_as_malloc_default
+  # use_partition_alloc_as_malloc =
+  #     use_partition_alloc && use_partition_alloc_as_malloc_default
+  use_partition_alloc_as_malloc = true
 }
 
 declare_args() {
   # Causes all the allocations to be routed via allocator_shim.cc.
-  use_allocator_shim = use_allocator_shim_default
+  # use_allocator_shim = use_allocator_shim_default
+  use_allocator_shim = true
 }
 
 assert(!use_allocator_shim || (is_android || is_apple || is_chromeos ||
@@ -108,6 +120,8 @@ declare_args() {
   # partition_alloc::PartitionOptions::BackupRefPtr::kEnabled.
   enable_backup_ref_ptr_support =
       use_partition_alloc && enable_backup_ref_ptr_support_default
+      && false
+
 
   # RAW_PTR_EXCLUSION macro is disabled on official builds because it increased
   # binary size. This flag can be used to enable it for official builds too.
@@ -118,15 +132,12 @@ assert(!enable_pointer_compression_support || glue_core_pools,
        "Pointer compression relies on core pools being contiguous.")
 
 declare_args() {
-  # The supported platforms are supposed to match `_is_brp_supported`, but we
-  # enable the feature on Linux early because it's most widely used for security
-  # research.
-  #
   # The implementation of ASan BRP is purpose-built to inspect Chromium
   # internals and is entangled with `//base` s.t. it cannot be used
   # outside of Chromium.
   use_asan_backup_ref_ptr =
-      build_with_chromium && is_asan && (is_win || is_android || is_linux)
+      build_with_chromium && is_asan &&
+      (is_win || is_android || is_linux || is_mac || is_chromeos)
 
   # Use probe-on-destruct unowned ptr detection with ASAN.
   use_asan_unowned_ptr = false
@@ -201,8 +212,9 @@ declare_args() {
 use_starscan = build_with_chromium && has_64_bit_pointers
 
 pcscan_stack_supported =
-    use_starscan && (current_cpu == "x64" || current_cpu == "x86" ||
-                     current_cpu == "arm" || current_cpu == "arm64")
+    use_starscan &&
+    (current_cpu == "x64" || current_cpu == "x86" || current_cpu == "arm" ||
+     current_cpu == "arm64" || current_cpu == "riscv64")
 
 # We want to provide assertions that guard against inconsistent build
 # args, but there is no point in having them fire if we're not building
@@ -317,7 +329,107 @@ declare_args() {
   # pkeys support is explicitly disabled in all Cronet builds, as some test
   # dependencies that use partition_allocator are compiled in AOSP against a
   # version of glibc that does not include pkeys syscall numbers.
-  enable_pkeys = is_linux && target_cpu == "x64" && !is_cronet_build
+  # enable_pkeys = is_linux && target_cpu == "x64" && !is_cronet_build
+  enable_pkeys = false
 }
 assert(!enable_pkeys || (is_linux && target_cpu == "x64"),
        "Pkeys are only supported on x64 linux")
+
+
+declare_args() {
+  # tmemk_enable = is_linux && target_cpu == "x64" && !is_cronet_build
+  tmemk_enable = true
+}
+declare_args() {
+  tmemk_enable_debug = false
+}
+declare_args() {
+  tmemk_override_debug = false
+}
+declare_args() {
+  tmemk_override_debug_expensive_checks = false
+}
+declare_args() {
+  # TMEMK align at shim (in addition to internal minimum granule size being 64B)
+  tmemk_align_at_shim = false
+}
+declare_args() {
+  # each slot, at first allocation/reserve, gets one of two known tags
+  # that are chosen per SuperPage. values are 1 and 2
+  tmemk_starting_tags = true
+}
+declare_args() {
+  # each slot, at first allocation/reserve, gets one of two known tags
+  # that are chosen per SuperPage.
+  # Values are chosen randomly
+  tmemk_starting_tags_random = true
+}
+declare_args() {
+  # every slot in a superpage gets the same starting tag. non intra-superpage overflow detection!
+  tmemk_starting_tags_same = false
+}
+declare_args() {
+  tmemk_integrity = true
+}
+declare_args() {
+  tmemk_encryption = true
+}
+declare_args() {
+  tmemk_quarantining = false
+
+}
+declare_args() {
+  tmemk_returnnull = true
+}
+declare_args() {
+  tmemk_noalias = false
+}
+declare_args() {
+  tmemk_notagging = false
+}
+declare_args() {
+  tmemk_padding = true
+}
+assert(!tmemk_integrity || !tmemk_encryption || !tmemk_enable || tmemk_integrity && tmemk_padding, "integrity requires padding")
+assert(!tmemk_encryption || !tmemk_enable || tmemk_encryption && tmemk_padding, "encryption requires (some) padding")
+# assert(!tmemk_notagging || (tmemk_noalias), "tagging requires aliasing")
+declare_args() {
+  tmemk_keybits = 3
+}
+declare_args() {
+  tmemk_maxtagsize = 1024
+}
+
+declare_args() {
+  tmemk_thread_isolation = false
+}
+declare_args() {
+  tmemk_trace = false
+}
+declare_args() {
+  tmemk_tripwires = false
+}
+declare_args() {
+  tmemk_never_increment = false
+}
+declare_args() {
+  tmemk_flush_old_after_free = false
+}
+declare_args() {
+  tmemk_skip_memset_when_never_increment = false
+}
+declare_args() {
+  tmemk_tripwire_max = -1
+}
+declare_args() {
+
+  # backuprefptr may require setting this to false
+  # tmemk_memzero_instead_of_memcpy_at_free = false
+  tmemk_memzero_instead_of_memcpy_at_free = true
+}
+declare_args() {
+  # mostly for debugging and/or if the host OS is not enlightened
+  # set to false if OS de-initializes all returned pages itself
+  tmemk_memzero_before_decommit = false
+
+}
diff --git a/partition_alloc.h b/partition_alloc.h
index 0ef2f9b..46a83e4 100644
--- a/partition_alloc.h
+++ b/partition_alloc.h
@@ -48,8 +48,8 @@ void PartitionAllocGlobalInit(OomFunction on_out_of_memory);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 void PartitionAllocGlobalUninitForTesting();
 
-namespace internal {
 struct PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAllocator {
+  PartitionAllocator();
   ~PartitionAllocator();
 
   void init(PartitionOptions);
@@ -63,10 +63,6 @@ struct PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAllocator {
   PartitionRoot partition_root_;
 };
 
-}  // namespace internal
-
-using PartitionAllocator = internal::PartitionAllocator;
-
 }  // namespace partition_alloc
 
 #endif  // BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_ALLOC_H_
diff --git a/partition_alloc_base/bits.h b/partition_alloc_base/bits.h
index 5a3fc1d..624b58b 100644
--- a/partition_alloc_base/bits.h
+++ b/partition_alloc_base/bits.h
@@ -7,7 +7,6 @@
 #ifndef BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_ALLOC_BASE_BITS_H_
 #define BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_ALLOC_BASE_BITS_H_
 
-#include <climits>
 #include <cstddef>
 #include <cstdint>
 #include <type_traits>
@@ -149,7 +148,7 @@ constexpr T LeftmostBit() {
   static_assert(std::is_integral<T>::value,
                 "This function can only be used with integral types.");
   T one(1u);
-  return one << ((CHAR_BIT * sizeof(T) - 1));
+  return one << (8 * sizeof(T) - 1);
 }
 
 }  // namespace partition_alloc::internal::base::bits
diff --git a/partition_alloc_base/logging.h b/partition_alloc_base/logging.h
index 270d770..23e310d 100644
--- a/partition_alloc_base/logging.h
+++ b/partition_alloc_base/logging.h
@@ -18,6 +18,9 @@
 #include "base/allocator/partition_allocator/partition_alloc_base/scoped_clear_last_error.h"
 #include "build/build_config.h"
 
+#include <stdio.h>
+#include "base/allocator/partition_allocator/partition_alloc_base/strings/stringprintf.h"
+
 // TODO(1151236): Need to update the description, because logging for PA
 // standalone library was minimized.
 //
diff --git a/partition_alloc_base/strings/stringprintf.h b/partition_alloc_base/strings/stringprintf.h
index 5551fb6..e9294b6 100644
--- a/partition_alloc_base/strings/stringprintf.h
+++ b/partition_alloc_base/strings/stringprintf.h
@@ -13,6 +13,101 @@
 #include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "build/build_config.h"
 
+
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE
+#endif
+#include <unistd.h>
+#include <sys/syscall.h>
+
+[[maybe_unused]] PA_COMPONENT_EXPORT(PARTITION_ALLOC) static void __debug_never(const char* format, ...)  {
+}
+
+[[maybe_unused]] PA_COMPONENT_EXPORT(PARTITION_ALLOC) static void __debug_always(const char* format, ...)  {
+//   base::ScopedClearLastError last_error;
+  [[maybe_unused]] static const char * COLOR_RED     = "\x1B[31m";
+  [[maybe_unused]] static const char * COLOR_GREEN   = "\x1B[32m";
+  [[maybe_unused]] static const char * COLOR_YELLOW  = "\x1B[33m";
+  [[maybe_unused]] static const char * COLOR_BLUE    = "\x1B[34m";
+  [[maybe_unused]] static const char * COLOR_MAGENTA = "\x1B[35m";
+  [[maybe_unused]] static const char * COLOR_CYAN    = "\x1B[36m";
+  [[maybe_unused]] static const char * COLOR_WHITE   = "\x1B[37m";
+  [[maybe_unused]] static const char * COLOR_GRAY    = "\x1B[90m";
+  [[maybe_unused]] static const char * COLOR_RESET   = "\x1B[0m";
+  const char * color = (strncmp(format, "TODO", 4) == 0) ? COLOR_YELLOW : ( (strncmp(format, "ERROR", 5) == 0) ? COLOR_RED : COLOR_GRAY);
+  size_t kMaxLengthOfTruncatingStringPrintfResult = 255U;
+  char stack_buf[kMaxLengthOfTruncatingStringPrintfResult + 1];
+  size_t max_strlen = kMaxLengthOfTruncatingStringPrintfResult - strlen(COLOR_RESET) - strlen(color) - 1;
+  char * stack_buf_ptr = stack_buf;
+  strcpy(stack_buf, color);
+  stack_buf_ptr += strlen(color);
+
+  va_list arguments;
+  va_start(arguments, format);
+  int result = vsnprintf(stack_buf_ptr, max_strlen, format, arguments);
+  va_end(arguments);
+  if (result >= 0){
+    stack_buf_ptr += result;
+    result = snprintf(stack_buf_ptr, (size_t)(strlen(COLOR_RESET) + 1), "\r\n%s", COLOR_RESET);
+    stack_buf_ptr += result;
+    // puts(stack_buf);
+    // int fd = open("/dev/tty", O_WRONLY);
+    [[maybe_unused]] int sysret = syscall(__NR_write, 2/*stderr*/, stack_buf, stack_buf_ptr-stack_buf);
+    sysret = syscall(__NR_write, 2/*stderr*/, COLOR_RESET, strlen(COLOR_RESET));
+    sysret = syscall(__NR_fsync, 2/*stderr*/ );
+  }
+}
+#define __debug __debug_never
+#if defined(BUILDFLAG_INTERNAL_TMEMK_ENABLE_DEBUG)
+#if BUILDFLAG(TMEMK_ENABLE_DEBUG)
+#undef __debug
+#define __debug __debug_always
+#endif
+#endif
+
+#if defined(BUILDFLAG_INTERNAL_TMEMK_TRACE)
+#if BUILDFLAG(TMEMK_TRACE)
+#define __tmemk_trace __debug_always
+#else
+#define __tmemk_trace(...)
+#endif  // BUILDFLAG(TMEMK_TRACE)
+#endif
+
+
+// [[maybe_unused]] PA_COMPONENT_EXPORT(PARTITION_ALLOC) static void __debug(const char* format, ...)  {
+// #if defined(BUILDFLAG_INTERNAL_TMEMK_ENABLE_DEBUG)
+// #if BUILDFLAG(TMEMK_ENABLE_DEBUG)
+// //   base::ScopedClearLastError last_error;
+//   [[maybe_unused]] static const char * COLOR_RED     = "\x1B[31m";
+//   [[maybe_unused]] static const char * COLOR_GREEN   = "\x1B[32m";
+//   [[maybe_unused]] static const char * COLOR_YELLOW  = "\x1B[33m";
+//   [[maybe_unused]] static const char * COLOR_BLUE    = "\x1B[34m";
+//   [[maybe_unused]] static const char * COLOR_MAGENTA = "\x1B[35m";
+//   [[maybe_unused]] static const char * COLOR_CYAN    = "\x1B[36m";
+//   [[maybe_unused]] static const char * COLOR_WHITE   = "\x1B[37m";
+//   [[maybe_unused]] static const char * COLOR_GRAY    = "\x1B[90m";
+//   [[maybe_unused]] static const char * COLOR_RESET   = "\x1B[0m";
+//   const char * color = (strncmp(format, "TODO", 4) == 0) ? COLOR_YELLOW : ( (strncmp(format, "ERROR", 5) == 0) ? COLOR_RED : COLOR_GRAY);
+//   size_t kMaxLengthOfTruncatingStringPrintfResult = 255U;
+//   char stack_buf[kMaxLengthOfTruncatingStringPrintfResult + 1];
+//   size_t max_strlen = kMaxLengthOfTruncatingStringPrintfResult - strlen(COLOR_RESET) - strlen(color) - 1;
+//   char * stack_buf_ptr = stack_buf;
+//   strcpy(stack_buf, color);
+//   stack_buf_ptr += strlen(color);
+
+//   va_list arguments;
+//   va_start(arguments, format);
+//   int result = vsnprintf(stack_buf_ptr, max_strlen, format, arguments);
+//   va_end(arguments);
+//   if (result >= 0){
+//     stack_buf_ptr += result;
+//     result = snprintf(stack_buf_ptr, (size_t)(strlen(COLOR_RESET) + 1), "%s", COLOR_RESET);
+//     puts(stack_buf);
+//   }
+// #endif /* BUILDFLAG(TMEMK_ENABLE_DEBUG) */
+// #endif /* defined(BUILDFLAG_INTERNAL_TMEMK_ENABLE_DEBUG) */
+// }
+
 namespace partition_alloc::internal::base {
 
 // Since Only SystemErrorCodeToString and partition_alloc_perftests use
diff --git a/partition_alloc_check.h b/partition_alloc_check.h
index 0cc803e..75d2fcf 100644
--- a/partition_alloc_check.h
+++ b/partition_alloc_check.h
@@ -118,21 +118,23 @@
 
 namespace partition_alloc::internal {
 
+static constexpr size_t kDebugKeyMaxLength = 8ull;
+
 // Used for PA_DEBUG_DATA_ON_STACK, below.
 struct PA_DEBUGKV_ALIGN DebugKv {
   // 16 bytes object aligned on 16 bytes, to make it easier to see in crash
   // reports.
-  char k[8] = {};  // Not necessarily 0-terminated.
+  char k[kDebugKeyMaxLength] = {};  // Not necessarily 0-terminated.
   uint64_t v = 0;
 
   DebugKv(const char* key, uint64_t value) : v(value) {
     // Fill with ' ', so that the stack dump is nicer to read.  Not using
     // memset() on purpose, this header is included from *many* places.
-    for (int index = 0; index < 8; index++) {
+    for (size_t index = 0; index < sizeof k; index++) {
       k[index] = ' ';
     }
 
-    for (int index = 0; index < 8; index++) {
+    for (size_t index = 0; index < sizeof k; index++) {
       k[index] = key[index];
       if (key[index] == '\0') {
         break;
@@ -166,6 +168,8 @@ struct PA_DEBUGKV_ALIGN DebugKv {
 // x/8g <STACK_POINTER>
 // to see the data. With lldb, "x <STACK_POINTER> <FRAME_POJNTER>" can be used.
 #define PA_DEBUG_DATA_ON_STACK(name, value)                               \
+  static_assert(sizeof name <=                                            \
+                ::partition_alloc::internal::kDebugKeyMaxLength + 1);     \
   ::partition_alloc::internal::DebugKv PA_DEBUG_UNIQUE_NAME{name, value}; \
   ::partition_alloc::internal::base::debug::Alias(&PA_DEBUG_UNIQUE_NAME);
 
diff --git a/partition_alloc_config.h b/partition_alloc_config.h
index 7b69265..fa6d9c3 100644
--- a/partition_alloc_config.h
+++ b/partition_alloc_config.h
@@ -149,9 +149,14 @@ static_assert(sizeof(void*) != 8, "");
 #define PA_CONFIG_HAS_FREELIST_SHADOW_ENTRY()    \
   (!BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) && \
    defined(ARCH_CPU_LITTLE_ENDIAN))
+
+
+#if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
+XXX not implemented
+#endif
 
 #define PA_CONFIG_HAS_MEMORY_TAGGING()              \
-  (defined(ARCH_CPU_ARM64) && defined(__clang__) && \
+  (((defined(ARCH_CPU_ARM64) && defined(__clang__)) || BUILDFLAG(TMEMK_ENABLE)) && \
    (BUILDFLAG(IS_LINUX) || BUILDFLAG(IS_ANDROID)))
 
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
@@ -171,7 +176,7 @@ static_assert(sizeof(void*) == 8);
 #endif
 
 // Specifies whether allocation extras need to be added.
-#if BUILDFLAG(PA_DCHECK_IS_ON) || BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+#if BUILDFLAG(PA_DCHECK_IS_ON) || BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT) || BUILDFLAG(TMEMK_TRIPWIRES)
 #define PA_CONFIG_EXTRAS_REQUIRED() 1
 #else
 #define PA_CONFIG_EXTRAS_REQUIRED() 0
@@ -254,12 +259,14 @@ constexpr bool kUseLazyCommit = false;
     BUILDFLAG(ENABLE_BACKUP_REF_PTR_SLOW_CHECKS)))
 
 // Use available space in the reference count to store the initially requested
-// size from the application. This is used for debugging. On mac, it is used to
-// workaround a bug. (crbug.com/1378822)
-#if BUILDFLAG(IS_MAC) && !PA_CONFIG(REF_COUNT_CHECK_COOKIE) && \
+// size from the application. This is used for debugging.
+#if !PA_CONFIG(REF_COUNT_CHECK_COOKIE) && \
     !BUILDFLAG(ENABLE_DANGLING_RAW_PTR_CHECKS)
-#define PA_CONFIG_REF_COUNT_STORE_REQUESTED_SIZE() 1
+// Set to 1 when needed.
+#define PA_CONFIG_REF_COUNT_STORE_REQUESTED_SIZE() 0
 #else
+// You probably want it at 0, outside of local testing, or else
+// PartitionRefCount will grow past 8B.
 #define PA_CONFIG_REF_COUNT_STORE_REQUESTED_SIZE() 0
 #endif
 
diff --git a/partition_alloc_constants.h b/partition_alloc_constants.h
index 7d2060e..0d195f9 100644
--- a/partition_alloc_constants.h
+++ b/partition_alloc_constants.h
@@ -19,6 +19,9 @@
 #include "base/allocator/partition_allocator/tagging.h"
 #include "build/build_config.h"
 
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
+#include "base/allocator/partition_allocator/tagging.h"
+
 #if BUILDFLAG(IS_APPLE) && defined(ARCH_CPU_64_BITS)
 #include <mach/vm_page_size.h>
 #endif
@@ -97,7 +100,7 @@ PA_ALWAYS_INLINE PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR size_t
 PartitionPageShift() {
   return PageAllocationGranularityShift() + 2;
 }
-#else
+#else /* default */
 PA_ALWAYS_INLINE PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR size_t
 PartitionPageShift() {
   return 14;  // 16 KiB
@@ -244,7 +247,7 @@ constexpr size_t kHighThresholdForAlternateDistribution =
 //     +----------------------------------+
 //     | SuperPageExtentEntry (32 B)      |
 //     | PartitionPage (32 B)             |
-//     | PartitionBucket (40 B)           |
+//     | PartitionBucket (40 B)           | TODO update size
 //     | PartitionDirectMapExtent (32 B)  |
 //     +----------------------------------+
 //
@@ -314,11 +317,15 @@ static_assert(kThreadIsolatedPoolHandle == kNumPools,
 // for allocations larger than this constant should not be backed with PROT_MTE
 // (which saves shadow tag memory). We also save CPU cycles by skipping tagging
 // of large areas which are less likely to benefit from MTE protection.
-constexpr size_t kMaxMemoryTaggingSize = 1024;
+// constexpr size_t kMaxMemoryTaggingSize = 1024;
+static_assert(BUILDFLAG(TMEMK_MAXTAGSIZE) >= 1024);
+constexpr size_t kMaxMemoryTaggingSize = BUILDFLAG(TMEMK_MAXTAGSIZE);
+
 
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
 // Returns whether the tag of |object| overflowed, meaning the containing slot
 // needs to be moved to quarantine.
+#if ! BUILDFLAG(TMEMK_ENABLE)
 PA_ALWAYS_INLINE bool HasOverflowTag(void* object) {
   // The tag with which the slot is put to quarantine.
   constexpr uintptr_t kOverflowTag = 0x0f00000000000000uLL;
@@ -326,6 +333,7 @@ PA_ALWAYS_INLINE bool HasOverflowTag(void* object) {
                 "Overflow tag must be in tag bits");
   return (reinterpret_cast<uintptr_t>(object) & kPtrTagMask) == kOverflowTag;
 }
+#endif  // ! BUILDFLAG(TMEMK_ENABLE)
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
 
 PA_ALWAYS_INLINE PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR size_t
@@ -385,8 +393,9 @@ DirectMapAllocationGranularityOffsetMask() {
 // bytes on 64 bit ones.
 //
 // Keep in sync with //tools/memory/partition_allocator/objects_per_size_py.
-constexpr size_t kMinBucketedOrder =
-    kAlignment == 16 ? 5 : 4;  // 2^(order - 1), that is 16 or 8.
+constexpr size_t kMinBucketedOrder =
+    kAlignment == 64 ? 7 : (kAlignment == 16 ? 5 : 4);  // 2^(order - 1)
+static_assert(kAlignment == 64 || kAlignment == 16 || kAlignment == 8, "");
 // The largest bucketed order is 1 << (20 - 1), storing [512 KiB, 1 MiB):
 constexpr size_t kMaxBucketedOrder = 20;
 constexpr size_t kNumBucketedOrders =
@@ -463,7 +472,8 @@ constexpr size_t kReasonableSizeOfUnusedPages = 1024 * 1024 * 1024;  // 1 GiB
 
 // These byte values match tcmalloc.
 constexpr unsigned char kUninitializedByte = 0xAB;
-constexpr unsigned char kFreedByte = 0xCD;
+// constexpr unsigned char kFreedByte = 0xCD;
+constexpr unsigned char kFreedByte = 0x00;
 
 constexpr unsigned char kQuarantinedByte = 0xEF;
 
diff --git a/partition_alloc_forward.h b/partition_alloc_forward.h
index a5b8a67..5404389 100644
--- a/partition_alloc_forward.h
+++ b/partition_alloc_forward.h
@@ -28,14 +28,16 @@ namespace internal {
 // the second one 16. We could technically return something different for
 // malloc() and operator new(), but this would complicate things, and most of
 // our allocations are presumably coming from operator new() anyway.
+#if BUILDFLAG(TMEMK_ENABLE) || BUILDFLAG(TMEMK_PADDING)
+constexpr size_t kAlignment = 64;
+#else
 constexpr size_t kAlignment =
     std::max(alignof(max_align_t),
              static_cast<size_t>(__STDCPP_DEFAULT_NEW_ALIGNMENT__));
 static_assert(kAlignment <= 16,
               "PartitionAlloc doesn't support a fundamental alignment larger "
               "than 16 bytes.");
-
-constexpr bool ThreadSafe = true;
+#endif
 
 struct SlotSpanMetadata;
 class PA_LOCKABLE Lock;
@@ -54,6 +56,9 @@ class PartitionStatsDumper;
 
 struct PartitionRoot;
 
+// TODO(787153): third_party/pdfium depends on ThreadSafePartitionRoot.
+//  After replacing the ThreadSafePartitionRoot with PartitionRoot, remove
+// ThreadSafePartitionRoot from partition_alloc_forward.h.
 using ThreadSafePartitionRoot = PartitionRoot;
 
 namespace internal {
diff --git a/partition_alloc_perftest.cc b/partition_alloc_perftest.cc
index c2b634a..e5d93a1 100644
--- a/partition_alloc_perftest.cc
+++ b/partition_alloc_perftest.cc
@@ -98,10 +98,10 @@ class PartitionAllocator : public Allocator {
   void* Alloc(size_t size) override {
     return alloc_.AllocWithFlagsNoHooks(0, size, PartitionPageSize());
   }
-  void Free(void* data) override { ThreadSafePartitionRoot::FreeNoHooks(data); }
+  void Free(void* data) override { PartitionRoot::FreeNoHooks(data); }
 
  private:
-  ThreadSafePartitionRoot alloc_{PartitionOptions{}};
+  PartitionRoot alloc_{PartitionOptions{}};
 };
 
 class PartitionAllocatorWithThreadCache : public Allocator {
@@ -154,11 +154,11 @@ class PartitionAllocatorWithAllocationStackTraceRecorder : public Allocator {
     return alloc_.AllocWithFlags(0, size, nullptr);
   }
 
-  void Free(void* data) override { ThreadSafePartitionRoot::Free(data); }
+  void Free(void* data) override { PartitionRoot::Free(data); }
 
  private:
   bool const register_hooks_;
-  ThreadSafePartitionRoot alloc_{PartitionOptions{}};
+  PartitionRoot alloc_{PartitionOptions{}};
   ::base::allocator::dispatcher::Dispatcher& dispatcher_ =
       ::base::allocator::dispatcher::Dispatcher::GetInstance();
   ::base::debug::tracer::AllocationTraceRecorder recorder_;
diff --git a/partition_alloc_unittest.cc b/partition_alloc_unittest.cc
index 3cf14e1..824fbef 100644
--- a/partition_alloc_unittest.cc
+++ b/partition_alloc_unittest.cc
@@ -222,7 +222,7 @@ class CountDanglingRawPtr {
 // For ease of reading, the tests are placed into the latter namespace.
 namespace partition_alloc::internal {
 
-using BucketDistribution = ThreadSafePartitionRoot::BucketDistribution;
+using BucketDistribution = PartitionRoot::BucketDistribution;
 using SlotSpan = SlotSpanMetadata;
 
 const size_t kTestAllocSize = 16;
@@ -237,7 +237,7 @@ const size_t kExtraAllocSizeWithoutRefCount = kCookieSize;
 
 const char* type_name = nullptr;
 
-void SetDistributionForPartitionRoot(ThreadSafePartitionRoot* root,
+void SetDistributionForPartitionRoot(PartitionRoot* root,
                                      BucketDistribution distribution) {
   switch (distribution) {
     case BucketDistribution::kDefault:
@@ -256,25 +256,17 @@ struct PartitionAllocTestParam {
 };
 
 const std::vector<PartitionAllocTestParam> GetPartitionAllocTestParams() {
-  std::vector<size_t> ref_count_sizes = {16};
-
-  bool only_supports_16b_ref_count = false;
-#if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
-  only_supports_16b_ref_count =
-      partition_alloc::internal::base::CPU::GetInstanceNoAllocation().has_mte();
-#endif
-
-  if (!only_supports_16b_ref_count) {
-    ref_count_sizes.push_back(0);
-    ref_count_sizes.push_back(8);
-    // sizeof(PartitionRefCount) == 8 under some configurations, so we can't
-    // force the size down to 4.
+  std::vector<size_t> ref_count_sizes = {0, 8, 16};
+  // sizeof(PartitionRefCount) == 8 under some configurations, so we can't force
+  // the size down to 4.
 #if !PA_CONFIG(REF_COUNT_STORE_REQUESTED_SIZE) && \
     !PA_CONFIG(REF_COUNT_CHECK_COOKIE) &&         \
     !BUILDFLAG(ENABLE_DANGLING_RAW_PTR_CHECKS)
-    ref_count_sizes.push_back(4);
+  ref_count_sizes.push_back(4);
 #endif
-  }
+  // Using MTE or Mac13 workaroud increases extras size without increasing
+  // sizeof(PartitionRefCount), so we don't have to exclude it here, as long as
+  // ExtraAllocSize() accounts for it.
 
   std::vector<PartitionAllocTestParam> params;
   for (size_t ref_count_size : ref_count_sizes) {
@@ -465,12 +457,22 @@ class PartitionAllocTest
   }
 
   static size_t ExtraAllocSize(const PartitionAllocator& allocator) {
-    size_t ref_count_size = GetParam().ref_count_size;
-    if (!ref_count_size) {
-      ref_count_size = kInSlotRefCountBufferSize;
+    size_t ref_count_size = 0;
+    // Duplicate the logic from PartitionRoot::Init().
+    if (allocator.root()->brp_enabled()) {
+      ref_count_size = GetParam().ref_count_size;
+      if (!ref_count_size) {
+        ref_count_size = kPartitionRefCountSizeAdjustment;
+      }
+      ref_count_size = AlignUpRefCountSizeForMac(ref_count_size);
+#if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
+      if (allocator.root()->IsMemoryTaggingEnabled()) {
+        ref_count_size = partition_alloc::internal::base::bits::AlignUp(
+            ref_count_size, kMemTagGranuleSize);
+      }
+#endif  // PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
     }
-    return kExtraAllocSizeWithoutRefCount +
-           (allocator.root()->brp_enabled() ? ref_count_size : 0);
+    return kExtraAllocSizeWithoutRefCount + ref_count_size;
   }
 
   size_t GetNumPagesPerSlotSpan(size_t size) {
diff --git a/partition_bucket.cc b/partition_bucket.cc
index 29efa29..752cd77 100644
--- a/partition_bucket.cc
+++ b/partition_bucket.cc
@@ -375,6 +375,12 @@ SlotSpanMetadata* PartitionDirectMap(PartitionRoot* root,
               base::bits::AlignDown(reinterpret_cast<uintptr_t>(metadata) +
                                         sizeof(PartitionDirectMapMetadata) - 1,
                                     SystemPageSize()));
+
+    PA_CHECK(page == &metadata->page);
+    // PA_CHECK((uint64_t)page->slot_span_metadata.freelist_head < (1ULL << 47));
+    PA_CHECK((uint64_t)page->slot_span_metadata.next_slot_span < (1ULL << 47));
+    PA_CHECK((uint64_t)page->slot_span_metadata.bucket < (1ULL << 47));
+
     PA_DCHECK(page == &metadata->page);
     page->is_valid = true;
     PA_DCHECK(!page->has_valid_span_after_this);
@@ -569,19 +575,6 @@ uint8_t ComputeSystemPagesPerSlotSpanInternal(size_t slot_size) {
   PA_CHECK(best_pages <= MaxSystemPagesPerRegularSlotSpan());
   return static_cast<uint8_t>(best_pages);
 }
-
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
-// Returns size that should be tagged. Avoiding the previous slot ref count if
-// it exists to avoid a race (crbug.com/1445816).
-PA_ALWAYS_INLINE size_t TagSizeForSlot(PartitionRoot* root, size_t slot_size) {
-#if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
-  return slot_size - root->settings.ref_count_size;
-#else
-  return slot_size;
-#endif
-}
-#endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
-
 }  // namespace
 
 uint8_t ComputeSystemPagesPerSlotSpan(size_t slot_size,
@@ -668,7 +661,7 @@ PA_ALWAYS_INLINE SlotSpanMetadata* PartitionBucket::AllocNewSlotSpan(
   auto* slot_span = &gap_end_page->slot_span_metadata;
   InitializeSlotSpan(slot_span);
   // Now that slot span is initialized, it's safe to call FromSlotStart.
-  PA_DCHECK(slot_span == SlotSpanMetadata::FromSlotStart(slot_span_start));
+  PA_DCHECK(slot_span == SlotSpanMetadata::FromSlotStart((UntaggedSlot)slot_span_start));
 
   // System pages in the super page come in a decommited state. Commit them
   // before vending them back.
@@ -679,6 +672,7 @@ PA_ALWAYS_INLINE SlotSpanMetadata* PartitionBucket::AllocNewSlotSpan(
     PA_DEBUG_DATA_ON_STACK("spansize", slot_span_reservation_size);
     PA_DEBUG_DATA_ON_STACK("spancmt", slot_span_committed_size);
 
+
     root->RecommitSystemPagesForData(
         slot_span_start, slot_span_committed_size,
         PageAccessibilityDisposition::kRequireUpdate,
@@ -753,7 +747,10 @@ PartitionBucket::InitializeSuperPage(PartitionRoot* root,
   uintptr_t state_bitmap =
       super_page + PartitionPageSize() +
       (is_direct_mapped() ? 0 : ReservedFreeSlotBitmapSize());
-#if BUILDFLAG(USE_STARSCAN)
+
+
+
+#if BUILDFLAG(USE_STARSCAN) //|| BUILDFLAG(TMEMK_QUARANTINING)
   PA_DCHECK(SuperPageStateBitmapAddr(super_page) == state_bitmap);
   const size_t state_bitmap_reservation_size =
       root->IsQuarantineAllowed() ? ReservedStateBitmapSize() : 0;
@@ -905,7 +902,7 @@ PA_ALWAYS_INLINE void PartitionBucket::InitializeSlotSpan(
   }
 }
 
-PA_ALWAYS_INLINE uintptr_t
+PA_ALWAYS_INLINE TaggedSlot
 PartitionBucket::ProvisionMoreSlotsAndAllocOne(PartitionRoot* root,
                                                SlotSpanMetadata* slot_span) {
   PA_DCHECK(slot_span != SlotSpanMetadata::get_sentinel_slot_span());
@@ -958,12 +955,19 @@ PartitionBucket::ProvisionMoreSlotsAndAllocOne(PartitionRoot* root,
         slot_size <= kMaxMemoryTaggingSize);
   }
 
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
+  TaggedSlot return_slot_tagged = (TaggedSlot)return_slot;
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_THREAD_ISOLATION)
+  //Note: if TMEMK_THREAD_ISOLATION is enabled, we tag the slots much later
   const bool use_tagging =
       root->IsMemoryTaggingEnabled() && slot_size <= kMaxMemoryTaggingSize;
   if (PA_LIKELY(use_tagging)) {
     // Ensure the MTE-tag of the memory pointed by |return_slot| is unguessable.
-    TagMemoryRangeRandomly(return_slot, TagSizeForSlot(root, slot_size));
+    PA_DCHECK(root->settings.extras_offset == 0);
+    PA_DCHECK(root->TaggedSlotStartToObject(return_slot_tagged) == (void*)return_slot);
+    return_slot_tagged = (TaggedSlot)TagMemoryRangeRandomly(return_slot, root->TagSizeForSlot(slot_size));
+    #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+    XXX not implemented
+    #endif
   }
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
   // Add all slots that fit within so far committed pages to the free list.
@@ -971,22 +975,23 @@ PartitionBucket::ProvisionMoreSlotsAndAllocOne(PartitionRoot* root,
   uintptr_t next_slot_end = next_slot + slot_size;
   size_t free_list_entries_added = 0;
   while (next_slot_end <= commit_end) {
-    void* next_slot_ptr;
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
+    TaggedSlot next_slot_tagged = (TaggedSlot)next_slot;
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_THREAD_ISOLATION)
     if (PA_LIKELY(use_tagging)) {
       // Ensure the MTE-tag of the memory pointed by other provisioned slot is
       // unguessable. They will be returned to the app as is, and the MTE-tag
       // will only change upon calling Free().
-      next_slot_ptr =
-          TagMemoryRangeRandomly(next_slot, TagSizeForSlot(root, slot_size));
+      PA_DCHECK(root->settings.extras_offset == 0);
+      PA_DCHECK(root->TaggedSlotStartToObject((TaggedSlot)next_slot_tagged) == (void*)next_slot);
+      next_slot_tagged = (TaggedSlot)TagMemoryRangeRandomly(next_slot, root->TagSizeForSlot(slot_size));
+      #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+      XXX not implemented
+      #endif
     } else {
       // No MTE-tagging for larger slots, just cast.
-      next_slot_ptr = reinterpret_cast<void*>(next_slot);
     }
-#else  // PA_CONFIG(HAS_MEMORY_TAGGING)
-    next_slot_ptr = reinterpret_cast<void*>(next_slot);
 #endif
-    auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(next_slot_ptr);
+    auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(next_slot_tagged);
     if (!slot_span->get_freelist_head()) {
       PA_DCHECK(!prev_entry);
       PA_DCHECK(!free_list_entries_added);
@@ -1025,7 +1030,9 @@ PartitionBucket::ProvisionMoreSlotsAndAllocOne(PartitionRoot* root,
   // We had no free slots, and created some (potentially 0) in sorted order.
   slot_span->set_freelist_sorted();
 
-  return return_slot;
+
+
+  return return_slot_tagged;
 }
 
 bool PartitionBucket::SetNewActiveSlotSpan() {
@@ -1290,7 +1297,7 @@ void PartitionBucket::SortActiveSlotSpans() {
   }
 }
 
-uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
+TaggedSlot PartitionBucket::SlowPathAlloc(PartitionRoot* root,
                                          unsigned int flags,
                                          size_t raw_size,
                                          size_t slot_span_alignment,
@@ -1330,7 +1337,7 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
 
     // No fast path for direct-mapped allocations.
     if (flags & AllocFlags::kFastPathOrReturnNull) {
-      return 0;
+      return (TaggedSlot)NULL;
     }
 
     new_slot_span =
@@ -1339,7 +1346,10 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
       new_bucket = new_slot_span->bucket;
     }
     // Memory from PageAllocator is always zeroed.
-    *is_already_zeroed = true;
+    // *is_already_zeroed = true;
+    *is_already_zeroed = false;
+
+
   } else if (PA_LIKELY(!allocate_aligned_slot_span && SetNewActiveSlotSpan())) {
     // First, did we find an active slot span in the active list?
     new_slot_span = active_slot_spans_head;
@@ -1376,7 +1386,7 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
         PA_LIKELY(decommitted_slot_spans_head != nullptr)) {
       // Commit can be expensive, don't do it.
       if (flags & AllocFlags::kFastPathOrReturnNull) {
-        return 0;
+        return (TaggedSlot)NULL;
       }
 
       new_slot_span = decommitted_slot_spans_head;
@@ -1394,6 +1404,7 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
         // PageAccessibilityDisposition::kAllowKeepForPerf, so use the
         // same option as an optimization.
         // TODO(lizeb): Handle commit failure.
+
         root->RecommitSystemPagesForData(
             slot_span_start, new_slot_span->bucket->get_bytes_per_span(),
             PageAccessibilityDisposition::kAllowKeepForPerf,
@@ -1401,13 +1412,15 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
       }
 
       new_slot_span->Reset();
-      *is_already_zeroed = DecommittedMemoryIsAlwaysZeroed();
+      // *is_already_zeroed = DecommittedMemoryIsAlwaysZeroed();
+
+      *is_already_zeroed = false;
     }
     PA_DCHECK(new_slot_span);
   } else {
     // Getting a new slot span is expensive, don't do it.
     if (flags & AllocFlags::kFastPathOrReturnNull) {
-      return 0;
+      return (TaggedSlot)NULL;
     }
 
     // Third. If we get here, we need a brand new slot span.
@@ -1415,7 +1428,9 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
     // as slot_span_committed_size.
     new_slot_span = AllocNewSlotSpan(root, flags, slot_span_alignment);
     // New memory from PageAllocator is always zeroed.
-    *is_already_zeroed = true;
+    // *is_already_zeroed = true;
+
+    *is_already_zeroed = false;
   }
 
   // Bail if we had a memory allocation failure.
@@ -1423,7 +1438,7 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
     PA_DCHECK(active_slot_spans_head ==
               SlotSpanMetadata::get_sentinel_slot_span());
     if (flags & AllocFlags::kReturnNull) {
-      return 0;
+      return (TaggedSlot)NULL;
     }
     // See comment in PartitionDirectMap() for unlocking.
     ScopedUnlockGuard unlock{PartitionRootLock(root)};
@@ -1446,13 +1461,31 @@ uintptr_t PartitionBucket::SlowPathAlloc(PartitionRoot* root,
     // We may have set *is_already_zeroed to true above, make sure that the
     // freelist entry doesn't contain data. Either way, it wouldn't be a good
     // idea to let users see our internal data.
-    uintptr_t slot_start = entry->ClearForAllocation();
+    TaggedSlot slot_start = entry->ClearForAllocation();
+    //ClearForAllocation no longer strips pointer.
+    PA_DCHECK(slot_start.value() == entry);
+
+// #if PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(PA_DCHECK_IS_ON)
+//     xxx maybe wrong because slot size vs raw size (incl extras)
+//     const bool use_tagging = root->IsMemoryTaggingEnabled() && new_bucket->slot_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING);
+//     if(use_tagging){
+//       if((void*)UntagPtr(slot_start.value()) == slot_start.value()){
+//         __debug("ERROR we should probably return something tagged here %s:%d", __FUNCTION__, __LINE__);
+//       }
+//       PA_DCHECK((void*)UntagPtr(slot_start.value()) != slot_start.value());
+//     }
+// #endif
+
     return slot_start;
   }
 
   // Otherwise, we need to provision more slots by committing more pages. Build
   // the free list for the newly provisioned slots.
   PA_DCHECK(new_slot_span->num_unprovisioned_slots);
+
+  // *is_already_zeroed = true;
+  *is_already_zeroed = false;
+
   return ProvisionMoreSlotsAndAllocOne(root, new_slot_span);
 }
 
diff --git a/partition_bucket.h b/partition_bucket.h
index 7dcccba..8a5f8f2 100644
--- a/partition_bucket.h
+++ b/partition_bucket.h
@@ -16,6 +16,8 @@
 #include "base/allocator/partition_allocator/partition_alloc_forward.h"
 #include "base/allocator/partition_allocator/partition_page_constants.h"
 
+#include "base/allocator/partition_allocator/tagging.h"
+
 namespace partition_alloc::internal {
 
 constexpr inline int kPartitionNumSystemPagesPerSlotSpanBits = 8;
@@ -64,6 +66,7 @@ struct PartitionBucket {
   // Public API.
   PA_COMPONENT_EXPORT(PARTITION_ALLOC) void Init(uint32_t new_slot_size);
 
+
   // Sets |is_already_zeroed| to true if the allocation was satisfied by
   // requesting (a) new page(s) from the operating system, or false otherwise.
   // This enables an optimization for when callers use
@@ -72,7 +75,9 @@ struct PartitionBucket {
   // |PartitionRoot::AllocFromBucket|.)
   //
   // Note the matching Free() functions are in SlotSpanMetadata.
-  PA_NOINLINE PA_COMPONENT_EXPORT(PARTITION_ALLOC) uintptr_t
+
+
+  PA_NOINLINE PA_COMPONENT_EXPORT(PARTITION_ALLOC) TaggedSlot
       SlowPathAlloc(PartitionRoot* root,
                     unsigned int flags,
                     size_t raw_size,
@@ -89,7 +94,6 @@ struct PartitionBucket {
     if (PA_LIKELY(slot_size <= MaxRegularSlotSpanSize())) {
       return false;
     }
-
     PA_DCHECK((slot_size % SystemPageSize()) == 0);
     PA_DCHECK(is_direct_mapped() || get_slots_per_span() == 1);
 
@@ -215,7 +219,7 @@ struct PartitionBucket {
   //
   // If |slot_span| was freshly allocated, it must have been passed through
   // InitializeSlotSpan() first.
-  PA_ALWAYS_INLINE uintptr_t
+  PA_ALWAYS_INLINE TaggedSlot
   ProvisionMoreSlotsAndAllocOne(PartitionRoot* root,
                                 SlotSpanMetadata* slot_span)
       PA_EXCLUSIVE_LOCKS_REQUIRED(PartitionRootLock(root));
diff --git a/partition_cookie.h b/partition_cookie.h
index 7c6b4a2..e39fe2d 100644
--- a/partition_cookie.h
+++ b/partition_cookie.h
@@ -11,10 +11,33 @@
 
 namespace partition_alloc::internal {
 
-static constexpr size_t kCookieSize = 16;
+static constexpr size_t kCookieSize = 64;
 
 // Cookie is enabled for debug builds.
-#if BUILDFLAG(PA_DCHECK_IS_ON)
+#if BUILDFLAG(TMEMK_TRIPWIRES)
+
+constexpr size_t kPartitionCookieSizeAdjustment = kCookieSize;
+
+PA_ALWAYS_INLINE void PartitionCookieCheckValue(unsigned char* cookie_ptr) {
+  PA_DCHECK( ((uintptr_t)(cookie_ptr)) % kMemTagGranuleSize == 0);
+  // __debug("PartitionCookieCheckValue(0x%014lx)", cookie_ptr);
+  cookie_ptr = (unsigned char*)UntagPtr((void*)cookie_ptr);
+  [[maybe_unused]] volatile uint64_t tmp = *(volatile uint64_t*)cookie_ptr;
+  // __debug("PartitionCookieCheckValue(0x%014lx) *cookie_ptr = %lx", cookie_ptr, tmp);
+  // keyID0 does not generate an exception if the MAC is wrong
+  // thus we require explicit check for the value
+  // we could use the overflow key, which is also unused for normal allocations
+  // but there's no real benefit and we might want to use the overflow key later
+  PA_CHECK(tmp == 0);
+}
+
+PA_ALWAYS_INLINE void PartitionCookieWriteValue(unsigned char* cookie_ptr) {
+  PA_DCHECK( ((uintptr_t)(cookie_ptr)) % kMemTagGranuleSize == 0);
+  cookie_ptr = (unsigned char*)UntagPtr((void*)cookie_ptr);
+  MemZeroWithMovdir64B_q((uintptr_t)cookie_ptr, 64);
+}
+
+#elif BUILDFLAG(PA_DCHECK_IS_ON)
 
 inline constexpr unsigned char kCookieValue[kCookieSize] = {
     0xDE, 0xAD, 0xBE, 0xEF, 0xCA, 0xFE, 0xD0, 0x0D,
@@ -23,12 +46,11 @@ inline constexpr unsigned char kCookieValue[kCookieSize] = {
 constexpr size_t kPartitionCookieSizeAdjustment = kCookieSize;
 
 PA_ALWAYS_INLINE void PartitionCookieCheckValue(unsigned char* cookie_ptr) {
-  for (size_t i = 0; i < kCookieSize; ++i, ++cookie_ptr) {
-    PA_DCHECK(*cookie_ptr == kCookieValue[i]);
-  }
+  PA_DCHECK( ((uintptr_t)(cookie_ptr)) % kMemTagGranuleSize == 0);
 }
 
 PA_ALWAYS_INLINE void PartitionCookieWriteValue(unsigned char* cookie_ptr) {
+  PA_DCHECK( ((uintptr_t)(cookie_ptr)) % kMemTagGranuleSize == 0);
   for (size_t i = 0; i < kCookieSize; ++i, ++cookie_ptr) {
     *cookie_ptr = kCookieValue[i];
   }
diff --git a/partition_dcheck_helper.h b/partition_dcheck_helper.h
index 2b1cc20..846dbe7 100644
--- a/partition_dcheck_helper.h
+++ b/partition_dcheck_helper.h
@@ -63,10 +63,6 @@ PA_EXPORT_IF_DCHECK_IS_ON()
 void DCheckRootLockIsAcquired(PartitionRoot* root)
     PA_EMPTY_BODY_IF_DCHECK_IS_OFF();
 
-PA_EXPORT_IF_DCHECK_IS_ON()
-void DCheckRootLockOfSlotSpanIsAcquired(internal::SlotSpanMetadata* slot_span)
-    PA_EMPTY_BODY_IF_DCHECK_IS_OFF();
-
 }  // namespace partition_alloc::internal
 
 #endif  // BASE_ALLOCATOR_PARTITION_ALLOCATOR_PARTITION_DCHECK_HELPER_H_
diff --git a/partition_freelist_entry.h b/partition_freelist_entry.h
index e319339..7414552 100644
--- a/partition_freelist_entry.h
+++ b/partition_freelist_entry.h
@@ -29,9 +29,9 @@ namespace partition_alloc::internal {
 
 namespace {
 
-[[noreturn]] PA_NOINLINE void FreelistCorruptionDetected(size_t extra) {
+[[noreturn]] PA_NOINLINE void FreelistCorruptionDetected(size_t slot_size) {
   // Make it visible in minidumps.
-  PA_DEBUG_DATA_ON_STACK("extra", extra);
+  PA_DEBUG_DATA_ON_STACK("slotsize", slot_size);
   PA_IMMEDIATE_CRASH();
 }
 
@@ -72,6 +72,11 @@ class EncodedPartitionFreelistEntryPtr {
     //    corrupt a freelist pointer, partial pointer overwrite attacks are
     //    thwarted.
     // For big endian, similar guarantees are arrived at with a negation.
+
+// #if PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(TMEMK_ENABLE) && BUILDFLAG(TMEMK_ENCRYPTION)
+//     // no need to encode, since free slot has unknown key
+//     return address;
+// #endif
 #if defined(ARCH_CPU_BIG_ENDIAN)
     uintptr_t transformed = ~address;
 #else
@@ -122,14 +127,14 @@ class PartitionFreelistEntry {
   // Emplaces the freelist entry at the beginning of the given slot span, and
   // initializes it as null-terminated.
   PA_ALWAYS_INLINE static PartitionFreelistEntry* EmplaceAndInitNull(
-      void* slot_start_tagged) {
+      TaggedSlot slot_start_tagged) {
     // |slot_start_tagged| is MTE-tagged.
-    auto* entry = new (slot_start_tagged) PartitionFreelistEntry(nullptr);
+    auto* entry = new (slot_start_tagged.value()) PartitionFreelistEntry(nullptr);
     return entry;
   }
   PA_ALWAYS_INLINE static PartitionFreelistEntry* EmplaceAndInitNull(
       uintptr_t slot_start) {
-    return EmplaceAndInitNull(SlotStartAddr2Ptr(slot_start));
+    return EmplaceAndInitNull((TaggedSlot)slot_start);
   }
 
   // Emplaces the freelist entry at the beginning of the given slot span, and
@@ -139,10 +144,9 @@ class PartitionFreelistEntry {
   // can't perform a check that this and the next pointer belong to the same
   // super page, as thread-cache spans may chain slots across super pages.
   PA_ALWAYS_INLINE static PartitionFreelistEntry* EmplaceAndInitForThreadCache(
-      uintptr_t slot_start,
+      TaggedSlot slot_start,
       PartitionFreelistEntry* next) {
-    auto* entry =
-        new (SlotStartAddr2Ptr(slot_start)) PartitionFreelistEntry(next);
+    auto* entry = new (slot_start.value()) PartitionFreelistEntry(next);
     return entry;
   }
 
@@ -151,10 +155,10 @@ class PartitionFreelistEntry {
   //
   // This is for testing purposes only! |make_shadow_match| allows you to choose
   // if the shadow matches the next pointer properly or is trash.
-  PA_ALWAYS_INLINE static void EmplaceAndInitForTest(uintptr_t slot_start,
+  PA_ALWAYS_INLINE static void EmplaceAndInitForTest(TaggedSlot slot_start,
                                                      void* next,
                                                      bool make_shadow_match) {
-    new (SlotStartAddr2Ptr(slot_start))
+    new (slot_start.value())
         PartitionFreelistEntry(next, make_shadow_match);
   }
 
@@ -163,22 +167,22 @@ class PartitionFreelistEntry {
     encoded_next_.Override(EncodedPartitionFreelistEntryPtr::Transform(v));
   }
 
-  // Puts |extra| on the stack before crashing in case of memory
+  // Puts `slot_size` on the stack before crashing in case of memory
   // corruption. Meant to be used to report the failed allocation size.
   template <bool crash_on_corruption>
   PA_ALWAYS_INLINE PartitionFreelistEntry* GetNextForThreadCache(
-      size_t extra) const;
-  PA_ALWAYS_INLINE PartitionFreelistEntry* GetNext(size_t extra) const;
+      size_t slot_size) const;
+  PA_ALWAYS_INLINE PartitionFreelistEntry* GetNext(size_t slot_size) const;
 
-  PA_NOINLINE void CheckFreeList(size_t extra) const {
-    for (auto* entry = this; entry; entry = entry->GetNext(extra)) {
+  PA_NOINLINE void CheckFreeList(size_t slot_size) const {
+    for (auto* entry = this; entry; entry = entry->GetNext(slot_size)) {
       // |GetNext()| checks freelist integrity.
     }
   }
 
-  PA_NOINLINE void CheckFreeListForThreadCache(size_t extra) const {
+  PA_NOINLINE void CheckFreeListForThreadCache(size_t slot_size) const {
     for (auto* entry = this; entry;
-         entry = entry->GetNextForThreadCache<true>(extra)) {
+         entry = entry->GetNextForThreadCache<true>(slot_size)) {
       // |GetNextForThreadCache()| checks freelist integrity.
     }
   }
@@ -192,8 +196,8 @@ class PartitionFreelistEntry {
     //
     // This is most likely a PartitionAlloc bug if this triggers.
     if (PA_UNLIKELY(entry &&
-                    (SlotStartPtr2Addr(this) & kSuperPageBaseMask) !=
-                        (SlotStartPtr2Addr(entry) & kSuperPageBaseMask))) {
+                    (SlotStartPtr2Addr((TaggedSlot)this) & kSuperPageBaseMask) !=
+                        (SlotStartPtr2Addr((TaggedSlot)entry) & kSuperPageBaseMask))) {
       FreelistCorruptionDetected(0);
     }
 #endif  // BUILDFLAG(PA_DCHECK_IS_ON)
@@ -207,12 +211,13 @@ class PartitionFreelistEntry {
   // Zeroes out |this| before returning the slot. The pointer to this memory
   // will be returned to the user (caller of Alloc()), thus can't have internal
   // data.
-  PA_ALWAYS_INLINE uintptr_t ClearForAllocation() {
+  PA_ALWAYS_INLINE TaggedSlot ClearForAllocation() {
     encoded_next_.Override(0);
 #if PA_CONFIG(HAS_FREELIST_SHADOW_ENTRY)
     shadow_ = 0;
 #endif
-    return SlotStartPtr2Addr(this);
+    // return SlotStartPtr2Addr(this);
+    return (TaggedSlot)this;
   }
 
   PA_ALWAYS_INLINE constexpr bool IsEncodedNextPtrZero() const {
@@ -222,7 +227,7 @@ class PartitionFreelistEntry {
  private:
   template <bool crash_on_corruption>
   PA_ALWAYS_INLINE PartitionFreelistEntry* GetNextInternal(
-      size_t extra,
+      size_t slot_size,
       bool for_thread_cache) const;
 
   PA_ALWAYS_INLINE static bool IsSane(const PartitionFreelistEntry* here,
@@ -236,8 +241,8 @@ class PartitionFreelistEntry {
     //
     // Also, the lightweight UaF detection (pointer shadow) is checked.
 
-    uintptr_t here_address = SlotStartPtr2Addr(here);
-    uintptr_t next_address = SlotStartPtr2Addr(next);
+    uintptr_t here_address = SlotStartPtr2Addr((TaggedSlot)here);
+    uintptr_t next_address = SlotStartPtr2Addr((TaggedSlot)next);
 
 #if PA_CONFIG(HAS_FREELIST_SHADOW_ENTRY)
     bool shadow_ptr_ok = here->encoded_next_.Inverted() == here->shadow_;
@@ -294,7 +299,7 @@ static_assert(kSmallestUsedBucket >=
 
 template <bool crash_on_corruption>
 PA_ALWAYS_INLINE PartitionFreelistEntry*
-PartitionFreelistEntry::GetNextInternal(size_t extra,
+PartitionFreelistEntry::GetNextInternal(size_t slot_size,
                                         bool for_thread_cache) const {
   // GetNext() can be called on discarded memory, in which case |encoded_next_|
   // is 0, and none of the checks apply. Don't prefetch nullptr either.
@@ -314,7 +319,7 @@ PartitionFreelistEntry::GetNextInternal(size_t extra,
 #if PA_CONFIG(HAS_FREELIST_SHADOW_ENTRY)
       PA_DEBUG_DATA_ON_STACK("second", static_cast<size_t>(shadow_));
 #endif
-      FreelistCorruptionDetected(extra);
+      FreelistCorruptionDetected(slot_size);
     } else {
       return nullptr;
     }
@@ -333,13 +338,13 @@ PartitionFreelistEntry::GetNextInternal(size_t extra,
 
 template <bool crash_on_corruption>
 PA_ALWAYS_INLINE PartitionFreelistEntry*
-PartitionFreelistEntry::GetNextForThreadCache(size_t extra) const {
-  return GetNextInternal<crash_on_corruption>(extra, true);
+PartitionFreelistEntry::GetNextForThreadCache(size_t slot_size) const {
+  return GetNextInternal<crash_on_corruption>(slot_size, true);
 }
 
 PA_ALWAYS_INLINE PartitionFreelistEntry* PartitionFreelistEntry::GetNext(
-    size_t extra) const {
-  return GetNextInternal<true>(extra, false);
+    size_t slot_size) const {
+  return GetNextInternal<true>(slot_size, false);
 }
 
 }  // namespace partition_alloc::internal
diff --git a/partition_page.cc b/partition_page.cc
index 6cd7e0d..5154148 100644
--- a/partition_page.cc
+++ b/partition_page.cc
@@ -149,7 +149,7 @@ SlotSpanMetadata::SlotSpanMetadata(PartitionBucket* bucket)
     : bucket(bucket), can_store_raw_size_(bucket->CanStoreRawSize()) {}
 
 void SlotSpanMetadata::FreeSlowPath(size_t number_of_freed) {
-  DCheckRootLockOfSlotSpanIsAcquired(this);
+  DCheckRootLockIsAcquired(PartitionRoot::FromSlotSpan(this));
   PA_DCHECK(this != get_sentinel_slot_span());
 
   // The caller has already modified |num_allocated_slots|. It is a
@@ -252,6 +252,17 @@ void SlotSpanMetadata::DecommitIfPossible(PartitionRoot* root) {
 
 void SlotSpanMetadata::SortFreelist() {
   std::bitset<kMaxSlotsPerSlotSpan> free_slots;
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+  PA_DCHECK(TMEMK_KEY_BITS <= 8);
+  using tag_type = uint8_t;
+  tag_type free_slots_tags[kMaxSlotsPerSlotSpan];
+  memset(free_slots_tags, 0x00, sizeof(free_slots_tags));
+  //kMaxSlotsPerSlotSpanBits is 13. thus at most we have 2^13 entries
+  //but kMaxSlotsPerSlotSpan = PartitionPageSize() / kSmallestBucket
+  //on linux and with our 64B tags we get 16KiB/64B = 256 entries
+  //256*1B = 256B. neglible.
+#endif
+
   uintptr_t slot_span_start = ToSlotSpanStart(this);
 
   size_t num_provisioned_slots =
@@ -263,10 +274,14 @@ void SlotSpanMetadata::SortFreelist() {
   for (PartitionFreelistEntry* head = freelist_head; head;
        head = head->GetNext(slot_size)) {
     ++num_free_slots;
-    size_t offset_in_slot_span = SlotStartPtr2Addr(head) - slot_span_start;
+    size_t offset_in_slot_span = SlotStartPtr2Addr((TaggedSlot)head) - slot_span_start;
     size_t slot_number = bucket->GetSlotNumber(offset_in_slot_span);
     PA_DCHECK(slot_number < num_provisioned_slots);
     free_slots[slot_number] = true;
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+    uintptr_t slot_start_tagged = (uintptr_t)head;
+    free_slots_tags[slot_number] = (tag_type)internal::tmemk_get_tag_from_ptr((void*)slot_start_tagged);
+#endif
   }
   PA_DCHECK(num_free_slots == GetFreelistLength());
 
@@ -279,7 +294,15 @@ void SlotSpanMetadata::SortFreelist() {
          slot_number++) {
       if (free_slots[slot_number]) {
         uintptr_t slot_start = slot_span_start + (slot_size * slot_number);
+
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+        //re-add tag from before
+        auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(
+          TaggedSlot(tmemk_add_tag_to_ptr((void*)slot_start, free_slots_tags[slot_number]))
+        );
+#else
         auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(slot_start);
+#endif /* PA_CONFIG(HAS_MEMORY_TAGGING) */
 
         if (!head) {
           head = entry;
diff --git a/partition_page.h b/partition_page.h
index d379e57..3e093d7 100644
--- a/partition_page.h
+++ b/partition_page.h
@@ -28,6 +28,7 @@
 #include "base/allocator/partition_allocator/reservation_offset_table.h"
 #include "base/allocator/partition_allocator/tagging.h"
 #include "build/build_config.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
 
 #if BUILDFLAG(USE_STARSCAN)
 #include "base/allocator/partition_allocator/starscan/state_bitmap.h"
@@ -101,14 +102,21 @@ struct SlotSpanMetadata {
  private:
   const uint32_t can_store_raw_size_ : 1;
   uint32_t freelist_is_sorted_ : 1;
-  uint32_t unused1_ : (32 - 1 - 2 * kMaxSlotsPerSlotSpanBits - 1 - 1);
+  static constexpr size_t unused1_size = 32 - 1 - 2 * kMaxSlotsPerSlotSpanBits - 1 - 1;
+  uint32_t unused1_ : unused1_size; // 3bit
   // If |in_empty_cache_|==1, |empty_cache_index| is undefined and mustn't be
   // used.
   uint16_t in_empty_cache_ : 1;
   uint16_t empty_cache_index_ : kEmptyCacheIndexBits;  // < kMaxFreeableSpans.
-  uint16_t unused2_ : (16 - 1 - kEmptyCacheIndexBits);
+  static constexpr size_t notunused2_size = 16 - 1 - kEmptyCacheIndexBits;
+  uint16_t notunused2_ : notunused2_size; // 16-1-7 = 8 bit
   // Can use only 48 bits (6B) in this bitfield, as this structure is embedded
   // in PartitionPage which has 2B worth of fields and must fit in 32B.
+  // NOTE we removed 1 byte from PartitionPage, now we have an additional one here
+  // change this in the future and just use freelist or normal quarantine bitmap
+  static constexpr size_t notunused3_size = 8;
+  uint8_t notunused3_ : notunused3_size;
+
 
  public:
   PA_COMPONENT_EXPORT(PARTITION_ALLOC)
@@ -121,7 +129,7 @@ struct SlotSpanMetadata {
   PA_NOINLINE PA_COMPONENT_EXPORT(PARTITION_ALLOC) void FreeSlowPath(
       size_t number_of_freed);
   PA_ALWAYS_INLINE PartitionFreelistEntry* PopForAlloc(size_t size);
-  PA_ALWAYS_INLINE void Free(uintptr_t ptr, PartitionRoot* root);
+  PA_ALWAYS_INLINE void Free(TaggedSlot ptr, PartitionRoot* root);
   // Appends the passed freelist to the slot-span's freelist. Please note that
   // the function doesn't increment the tags of the passed freelist entries,
   // since FreeNoHooks() did it already.
@@ -146,7 +154,7 @@ struct SlotSpanMetadata {
   PA_ALWAYS_INLINE static uintptr_t ToSlotSpanStart(
       const SlotSpanMetadata* slot_span);
   PA_ALWAYS_INLINE static SlotSpanMetadata* FromAddr(uintptr_t address);
-  PA_ALWAYS_INLINE static SlotSpanMetadata* FromSlotStart(uintptr_t slot_start);
+  PA_ALWAYS_INLINE static SlotSpanMetadata* FromSlotStart(UntaggedSlot slot_start);
   PA_ALWAYS_INLINE static SlotSpanMetadata* FromObject(void* object);
   PA_ALWAYS_INLINE static SlotSpanMetadata* FromObjectInnerAddr(
       uintptr_t address);
@@ -228,6 +236,27 @@ struct SlotSpanMetadata {
   }
   PA_ALWAYS_INLINE void set_freelist_sorted() { freelist_is_sorted_ = true; }
 
+  PA_ALWAYS_INLINE size_t GetNumQuarantinedSlots() {
+    size_t num_quarantined_slots = 0;
+    num_quarantined_slots += notunused2_;
+    num_quarantined_slots += notunused3_ << notunused2_size;
+    return num_quarantined_slots;
+  }
+  PA_ALWAYS_INLINE void SetNumQuarantinedSlots(size_t num_quarantined_slots) {
+    notunused2_ = num_quarantined_slots;
+    notunused3_ = num_quarantined_slots >> notunused2_size;
+    PA_CHECK(GetNumQuarantinedSlots() == num_quarantined_slots);
+
+  }
+
+  static_assert((1ULL) << (notunused3_size + notunused2_size) >= ((1ULL << kPartitionNumSystemPagesPerSlotSpanBits) << SystemPageShift()) / kSmallestBucket);
+
+  // constexpr inline int kPartitionNumSystemPagesPerSlotSpanBits = 8;
+  // uint32_t num_system_pages_per_slot_span : kPartitionNumSystemPagesPerSlotSpanBits;
+  // return static_cast<size_t>(num_system_pages_per_slot_span) << SystemPageShift();
+
+
+
  private:
   // sentinel_slot_span_ is used as a sentinel to indicate that there is no slot
   // span in the active list. We could use nullptr, but in that case we need to
@@ -240,6 +269,8 @@ struct SlotSpanMetadata {
   inline constexpr SlotSpanMetadata() noexcept;
 };
 #pragma pack(pop)
+static_assert(sizeof(SlotSpanMetadata) <= kPageMetadataSize - 1); // SlotSpanMetadata must fit in partitionpage, but that one also has 2 additional bytes
+// static_assert(sizeof(SlotSpanMetadata) <= kPageMetadataSize - 2); // SlotSpanMetadata must fit in partitionpage, but that one also has 2 additional bytes
 static_assert(sizeof(SlotSpanMetadata) <= kPageMetadataSize,
               "SlotSpanMetadata must fit into a Page Metadata slot.");
 
@@ -252,9 +283,11 @@ inline constexpr SlotSpanMetadata::SlotSpanMetadata() noexcept
       unused1_(0),
       in_empty_cache_(0),
       empty_cache_index_(0),
-      unused2_(0) {
+      notunused2_(0),
+      notunused3_(0) {
   (void)unused1_;
-  (void)unused2_;
+  (void)notunused2_;
+  (void)notunused3_;
 }
 
 inline SlotSpanMetadata::SlotSpanMetadata(const SlotSpanMetadata&) = default;
@@ -292,7 +325,8 @@ struct PartitionPage {
     // - below kPageMetadataSize
     //
     // This makes sure that this is respected no matter the architecture.
-    char optional_padding[kPageMetadataSize - sizeof(uint8_t) - sizeof(bool)];
+    // char optional_padding[kPageMetadataSize - sizeof(uint8_t) - sizeof(bool)];
+    char optional_padding[kPageMetadataSize - sizeof(uint8_t)];
   };
 
   // The first PartitionPage of the slot span holds its metadata. This offset
@@ -316,7 +350,7 @@ struct PartitionPage {
   //   |!slot_span_metadata_offset && slot_span_metadata->bucket|.
   bool is_valid : 1;
   bool has_valid_span_after_this : 1;
-  uint8_t unused;
+  // uint8_t unused;
 
   PA_ALWAYS_INLINE static PartitionPage* FromAddr(uintptr_t address);
 };
@@ -459,6 +493,7 @@ PA_ALWAYS_INLINE bool IsWithinSuperPagePayload(uintptr_t address,
 // care has to be taken with direct maps that span multiple super pages. This
 // function's behavior is undefined if |ptr| lies in a subsequent super page.
 PA_ALWAYS_INLINE PartitionPage* PartitionPage::FromAddr(uintptr_t address) {
+  PA_DCHECK(UntagAddr(address) == address);
   uintptr_t super_page = address & kSuperPageBaseMask;
 
 #if BUILDFLAG(PA_DCHECK_IS_ON)
@@ -511,6 +546,7 @@ SlotSpanMetadata::ToSlotSpanStart(const SlotSpanMetadata* slot_span) {
 // partition page.
 PA_ALWAYS_INLINE SlotSpanMetadata* SlotSpanMetadata::FromAddr(
     uintptr_t address) {
+  PA_DCHECK(address == internal::UntagAddr(address)); //cannot have tagged pointers here.
   auto* page = PartitionPage::FromAddr(address);
   PA_DCHECK(page->is_valid);
   // Partition pages in the same slot span share the same SlotSpanMetadata
@@ -534,12 +570,13 @@ PA_ALWAYS_INLINE SlotSpanMetadata* SlotSpanMetadata::FromAddr(
 //
 // This works on direct maps too.
 PA_ALWAYS_INLINE SlotSpanMetadata* SlotSpanMetadata::FromSlotStart(
-    uintptr_t slot_start) {
-  auto* slot_span = FromAddr(slot_start);
+    UntaggedSlot slot_start) {
+  PA_DCHECK(slot_start.value() == internal::UntagAddr(slot_start.value()));
+  auto* slot_span = FromAddr(slot_start.value());
 #if BUILDFLAG(PA_DCHECK_IS_ON)
   // Checks that the pointer is a multiple of slot size.
   uintptr_t slot_span_start = ToSlotSpanStart(slot_span);
-  PA_DCHECK(!((slot_start - slot_span_start) % slot_span->bucket->slot_size));
+  PA_DCHECK(!((slot_start.value() - slot_span_start) % slot_span->bucket->slot_size));
 #endif  // BUILDFLAG(PA_DCHECK_IS_ON)
   return slot_span;
 }
@@ -621,14 +658,14 @@ PA_ALWAYS_INLINE PartitionFreelistEntry* SlotSpanMetadata::PopForAlloc(
   return result;
 }
 
-PA_ALWAYS_INLINE void SlotSpanMetadata::Free(uintptr_t slot_start,
+PA_ALWAYS_INLINE void SlotSpanMetadata::Free(TaggedSlot slot_start,
                                              PartitionRoot* root)
     // PartitionRootLock() is not defined inside partition_page.h, but
     // static analysis doesn't require the implementation.
     PA_EXCLUSIVE_LOCKS_REQUIRED(PartitionRootLock(root)) {
-  DCheckRootLockOfSlotSpanIsAcquired(this);
-  auto* entry = static_cast<internal::PartitionFreelistEntry*>(
-      SlotStartAddr2Ptr(slot_start));
+
+  DCheckRootLockIsAcquired(root);
+  auto* entry = static_cast<internal::PartitionFreelistEntry*>(slot_start.value());
   // Catches an immediate double free.
   PA_CHECK(entry != freelist_head);
   // Look for double free one level deeper in debug.
@@ -656,7 +693,7 @@ PA_ALWAYS_INLINE void SlotSpanMetadata::AppendFreeList(
     size_t number_of_freed,
     PartitionRoot* root) PA_EXCLUSIVE_LOCKS_REQUIRED(PartitionRootLock(root)) {
 #if BUILDFLAG(PA_DCHECK_IS_ON)
-  DCheckRootLockOfSlotSpanIsAcquired(this);
+  DCheckRootLockIsAcquired(root);
   PA_DCHECK(!tail->GetNext(bucket->slot_size));
   PA_DCHECK(number_of_freed);
   PA_DCHECK(num_allocated_slots);
diff --git a/partition_ref_count.h b/partition_ref_count.h
index 6a58431..28a7f2c 100644
--- a/partition_ref_count.h
+++ b/partition_ref_count.h
@@ -21,8 +21,30 @@
 #include "base/allocator/partition_allocator/tagging.h"
 #include "build/build_config.h"
 
+#if BUILDFLAG(IS_MAC)
+#include "base/allocator/partition_allocator/partition_alloc_base/bits.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/mac/mac_util.h"
+#endif  // BUILDFLAG(IS_MAC)
+
 namespace partition_alloc::internal {
 
+// Aligns up (on 8B boundary) and returns `ref_count_size` if needed.
+// *  Known to be needed on MacOS 13: https://crbug.com/1378822.
+// *  Thought to be needed on MacOS 14: https://crbug.com/1457756.
+// *  No-op everywhere else.
+//
+// Placed outside `BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)`
+// intentionally to accommodate usage in contexts also outside
+// this gating.
+PA_ALWAYS_INLINE size_t AlignUpRefCountSizeForMac(size_t ref_count_size) {
+#if BUILDFLAG(IS_MAC)
+  if (internal::base::mac::IsOS13() || internal::base::mac::IsOS14()) {
+    return internal::base::bits::AlignUp(ref_count_size, 8);
+  }
+#endif  // BUILDFLAG(IS_MAC)
+  return ref_count_size;
+}
+
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
 
 // Special-purpose atomic reference count class used by RawPtrBackupRefImpl.
@@ -349,6 +371,20 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRefCount {
 #if PA_CONFIG(REF_COUNT_STORE_REQUESTED_SIZE)
   uint32_t requested_size_;
 #endif
+
+#if BUILDFLAG(TMEMK_ENABLE)
+
+  static constexpr pad_bytes = 64 - sizeof(std::atomic<CountType>)
+#if PA_CONFIG(REF_COUNT_CHECK_COOKIE)
+  - sizeof(uint32_t)
+#endif
+#if PA_CONFIG(REF_COUNT_STORE_REQUESTED_SIZE)
+  - sizeof(uint32_t)
+#endif
+  ;
+  char _tmemk_padding[pad_bytes];
+#endif
+
 };
 
 PA_ALWAYS_INLINE PartitionRefCount::PartitionRefCount(
@@ -366,6 +402,7 @@ PA_ALWAYS_INLINE PartitionRefCount::PartitionRefCount(
 
 static_assert(kAlignment % alignof(PartitionRefCount) == 0,
               "kAlignment must be multiples of alignof(PartitionRefCount).");
+// or the other way around if kAlignment >= sizeof(PartitionRefCount)
 
 // Allocate extra space for the reference count to satisfy the alignment
 // requirement.
@@ -414,6 +451,8 @@ GetPartitionRefCountIndexMultiplierShift() {
 
 PA_ALWAYS_INLINE PartitionRefCount* PartitionRefCountPointer(
     uintptr_t slot_start) {
+
+
   if (PA_LIKELY(slot_start & SystemPageOffsetMask())) {
     uintptr_t refcount_address = slot_start - sizeof(PartitionRefCount);
 #if BUILDFLAG(PA_DCHECK_IS_ON) || BUILDFLAG(ENABLE_BACKUP_REF_PTR_SLOW_CHECKS)
@@ -450,6 +489,9 @@ PA_ALWAYS_INLINE PartitionRefCount* PartitionRefCountPointer(
     uintptr_t slot_start) {
   // Have to MTE-tag, because the address is untagged, but lies within a slot
   // area, which is protected by MTE.
+
+
+
   return static_cast<PartitionRefCount*>(TagAddr(slot_start));
 }
 
diff --git a/partition_root.cc b/partition_root.cc
index 3fc0b11..b9c61ba 100644
--- a/partition_root.cc
+++ b/partition_root.cc
@@ -29,7 +29,7 @@
 #include "base/allocator/partition_allocator/thread_isolation/thread_isolation.h"
 #include "build/build_config.h"
 
-#if PA_CONFIG(ENABLE_MAC11_MALLOC_SIZE_HACK)
+#if BUILDFLAG(IS_MAC)
 #include "base/allocator/partition_allocator/partition_alloc_base/mac/mac_util.h"
 #endif
 
@@ -85,7 +85,7 @@ PtrPosWithinAlloc IsPtrWithinSameAlloc(uintptr_t orig_address,
   // Double check that ref-count is indeed present.
   PA_DCHECK(root->brp_enabled());
 
-  uintptr_t object_addr = root->SlotStartToObjectAddr(slot_start);
+  uintptr_t object_addr = root->TaggedSlotStartToObjectAddr((TaggedSlot)slot_start);
   uintptr_t object_end = object_addr + root->GetSlotUsableSize(slot_span);
   if (test_address < object_addr || object_end < test_address) {
     return PtrPosWithinAlloc::kFarOOB;
@@ -119,8 +119,7 @@ namespace internal {
 
 class PartitionRootEnumerator {
  public:
-  using EnumerateCallback = void (*)(ThreadSafePartitionRoot* root,
-                                     bool in_child);
+  using EnumerateCallback = void (*)(PartitionRoot* root, bool in_child);
   enum EnumerateOrder {
     kNormal,
     kReverse,
@@ -135,14 +134,14 @@ class PartitionRootEnumerator {
                  bool in_child,
                  EnumerateOrder order) PA_NO_THREAD_SAFETY_ANALYSIS {
     if (order == kNormal) {
-      ThreadSafePartitionRoot* root;
+      PartitionRoot* root;
       for (root = Head(partition_roots_); root != nullptr;
            root = root->next_root) {
         callback(root, in_child);
       }
     } else {
       PA_DCHECK(order == kReverse);
-      ThreadSafePartitionRoot* root;
+      PartitionRoot* root;
       for (root = Tail(partition_roots_); root != nullptr;
            root = root->prev_root) {
         callback(root, in_child);
@@ -150,8 +149,8 @@ class PartitionRootEnumerator {
     }
   }
 
-  void Register(ThreadSafePartitionRoot* root) {
-    internal::ScopedGuard guard(ThreadSafePartitionRoot::GetEnumeratorLock());
+  void Register(PartitionRoot* root) {
+    internal::ScopedGuard guard(PartitionRoot::GetEnumeratorLock());
     root->next_root = partition_roots_;
     root->prev_root = nullptr;
     if (partition_roots_) {
@@ -160,10 +159,10 @@ class PartitionRootEnumerator {
     partition_roots_ = root;
   }
 
-  void Unregister(ThreadSafePartitionRoot* root) {
-    internal::ScopedGuard guard(ThreadSafePartitionRoot::GetEnumeratorLock());
-    ThreadSafePartitionRoot* prev = root->prev_root;
-    ThreadSafePartitionRoot* next = root->next_root;
+  void Unregister(PartitionRoot* root) {
+    internal::ScopedGuard guard(PartitionRoot::GetEnumeratorLock());
+    PartitionRoot* prev = root->prev_root;
+    PartitionRoot* next = root->next_root;
     if (prev) {
       PA_DCHECK(prev->next_root == root);
       prev->next_root = next;
@@ -182,23 +181,20 @@ class PartitionRootEnumerator {
  private:
   constexpr PartitionRootEnumerator() = default;
 
-  ThreadSafePartitionRoot* Head(ThreadSafePartitionRoot* roots) {
-    return roots;
-  }
+  PartitionRoot* Head(PartitionRoot* roots) { return roots; }
 
-  ThreadSafePartitionRoot* Tail(ThreadSafePartitionRoot* roots)
-      PA_NO_THREAD_SAFETY_ANALYSIS {
+  PartitionRoot* Tail(PartitionRoot* roots) PA_NO_THREAD_SAFETY_ANALYSIS {
     if (!roots) {
       return nullptr;
     }
-    ThreadSafePartitionRoot* node = roots;
+    PartitionRoot* node = roots;
     for (; node->next_root != nullptr; node = node->next_root)
       ;
     return node;
   }
 
-  ThreadSafePartitionRoot* partition_roots_
-      PA_GUARDED_BY(ThreadSafePartitionRoot::GetEnumeratorLock()) = nullptr;
+  PartitionRoot* partition_roots_
+      PA_GUARDED_BY(PartitionRoot::GetEnumeratorLock()) = nullptr;
 };
 
 }  // namespace internal
@@ -219,7 +215,7 @@ void LockRoot(PartitionRoot* root, bool) PA_NO_THREAD_SAFETY_ANALYSIS {
 // PA_NO_THREAD_SAFETY_ANALYSIS: acquires the lock and doesn't release it, by
 // design.
 void BeforeForkInParent() PA_NO_THREAD_SAFETY_ANALYSIS {
-  // ThreadSafePartitionRoot::GetLock() is private. So use
+  // PartitionRoot::GetLock() is private. So use
   // g_root_enumerator_lock here.
   g_root_enumerator_lock.Acquire();
   internal::PartitionRootEnumerator::Instance().Enumerate(
@@ -252,7 +248,7 @@ void ReleaseLocks(bool in_child) PA_NO_THREAD_SAFETY_ANALYSIS {
       UnlockOrReinitRoot, in_child,
       internal::PartitionRootEnumerator::EnumerateOrder::kReverse);
 
-  // ThreadSafePartitionRoot::GetLock() is private. So use
+  // PartitionRoot::GetLock() is private. So use
   // g_root_enumerator_lock here.
   UnlockOrReinit(g_root_enumerator_lock, in_child);
 }
@@ -393,6 +389,13 @@ static size_t PartitionPurgeSlotSpan(internal::SlotSpanMetadata* slot_span,
   PA_DCHECK(slot_span->num_unprovisioned_slots < bucket_num_slots);
   size_t num_slots = bucket_num_slots - slot_span->num_unprovisioned_slots;
   char slot_usage[kMaxSlotCount];
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+  PA_DCHECK(TMEMK_KEY_BITS <= 8); //TODO static assert
+  using tag_type = uint8_t;
+  tag_type slot_usage_tags[kMaxSlotCount];
+  memset(slot_usage_tags, 0x00, sizeof(slot_usage_tags));
+  //kMaxSlotCount = 4 * kMaxPurgeableSlotsPerSystemPage * kMaxPartitionPagesPerRegularSlotSpan = 4*64*4=1024 *1B = 1KB
+#endif /* PA_CONFIG(HAS_MEMORY_TAGGING) */
 #if !BUILDFLAG(IS_WIN)
   // The last freelist entry should not be discarded when using OS_WIN.
   // DiscardVirtualMemory makes the contents of discarded memory undefined.
@@ -405,9 +408,12 @@ static size_t PartitionPurgeSlotSpan(internal::SlotSpanMetadata* slot_span,
   for (PartitionFreelistEntry* entry = slot_span->get_freelist_head(); entry;
        /**/) {
     size_t slot_number =
-        bucket->GetSlotNumber(SlotStartPtr2Addr(entry) - slot_span_start);
+        bucket->GetSlotNumber(SlotStartPtr2Addr((TaggedSlot)entry) - slot_span_start);
     PA_DCHECK(slot_number < num_slots);
     slot_usage[slot_number] = 0;
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+    slot_usage_tags[slot_number] = (tag_type)internal::tmemk_get_tag_from_ptr(entry);
+#endif
 #if !BUILDFLAG(IS_WIN)
     // If we have a slot where the encoded next pointer is 0, we can actually
     // discard that entry because touching a discarded page is guaranteed to
@@ -472,9 +478,15 @@ static size_t PartitionPurgeSlotSpan(internal::SlotSpanMetadata* slot_span,
         if (slot_usage[slot_index]) {
           continue;
         }
-
-        auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(
-            slot_span_start + (slot_size * slot_index));
+        void* entry_addr = (void*)(slot_span_start + (slot_size * slot_index));
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+        TaggedSlot tagged = (TaggedSlot)internal::tmemk_add_tag_to_ptr(
+          (void*)entry_addr, slot_usage_tags[slot_index]
+        );
+        auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(tagged);
+#else
+        auto* entry = PartitionFreelistEntry::EmplaceAndInitNull((TaggedSlot)entry_addr);
+#endif
         if (!head) {
           head = entry;
           back = entry;
@@ -711,7 +723,7 @@ void PartitionAllocThreadIsolationInit(ThreadIsolationOption thread_isolation) {
   // stacks, and other allocators will also consume address space).
   const size_t kReasonableVirtualSize = (is_wow_64 ? 2800 : 1024) * 1024 * 1024;
   // Make it obvious whether we are running on 64-bit Windows.
-  PA_DEBUG_DATA_ON_STACK("is_wow_64", static_cast<size_t>(is_wow_64));
+  PA_DEBUG_DATA_ON_STACK("iswow64", static_cast<size_t>(is_wow_64));
 #else
   constexpr size_t kReasonableVirtualSize =
       // 1.5GiB elsewhere, since address space is typically 3GiB.
@@ -879,7 +891,15 @@ void PartitionRoot::Init(PartitionOptions opts) {
 
     settings.allow_aligned_alloc =
         opts.aligned_alloc == PartitionOptions::AlignedAlloc::kAllowed;
-    settings.allow_cookie = opts.cookie == PartitionOptions::Cookie::kAllowed;
+#if BUILDFLAG(TMEMK_TRIPWIRES)
+    settings.use_cookie = true;
+#elif BUILDFLAG(PA_DCHECK_IS_ON)
+    settings.use_cookie = opts.cookie == PartitionOptions::Cookie::kAllowed;
+#else
+#if ! BUILDFLAG(TMEMK_TRIPWIRES)
+    static_assert(!Settings::use_cookie);
+#endif  // BUILDFLAG(TMEMK_TRIPWIRES)
+#endif  // BUILDFLAG(PA_DCHECK_IS_ON)
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
     settings.brp_enabled_ =
         opts.backup_ref_ptr == PartitionOptions::BackupRefPtr::kEnabled;
@@ -930,8 +950,9 @@ void PartitionRoot::Init(PartitionOptions opts) {
     settings.extras_size = 0;
     settings.extras_offset = 0;
 
-    if (settings.allow_cookie) {
+    if (settings.use_cookie) {
       settings.extras_size += internal::kPartitionCookieSizeAdjustment;
+       PA_CHECK(internal::kPartitionCookieSizeAdjustment <= internal::kMemTagGranuleSize); //we can just assume that cookie fits inside granule.
     }
 
     if (brp_enabled()) {
@@ -942,6 +963,7 @@ void PartitionRoot::Init(PartitionOptions opts) {
       if (!ref_count_size) {
         ref_count_size = internal::kPartitionRefCountSizeAdjustment;
       }
+      ref_count_size = internal::AlignUpRefCountSizeForMac(ref_count_size);
 #if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
       if (IsMemoryTaggingEnabled()) {
         ref_count_size = internal::base::bits::AlignUp(
@@ -952,7 +974,16 @@ void PartitionRoot::Init(PartitionOptions opts) {
       PA_CHECK(internal::kPartitionRefCountSizeAdjustment <= ref_count_size);
       settings.extras_size += ref_count_size;
       settings.extras_offset += internal::kPartitionRefCountOffsetAdjustment;
+#if BUILDFLAG(TMEMK_ENABLE)
+      PA_CHECK(0);
+      // we didnt implement the necessary stuff for extras_offset yet
+      // because we may not use TaggedSlotStartToObject consistently
+#endif // BUILDFLAG(TMEMK_ENABLE)
     }
+
+    settings.extras_size = internal::base::bits::AlignUp(settings.extras_size, internal::kMemTagGranuleSize);
+    PA_CHECK(settings.extras_size % internal::kMemTagGranuleSize == 0);
+
 #endif  // PA_CONFIG(EXTRAS_REQUIRED)
 
     // Re-confirm the above PA_CHECKs, by making sure there are no
@@ -1183,13 +1214,12 @@ bool PartitionRoot::TryReallocInPlaceForDirectMap(
     thread_cache->RecordAllocation(GetSlotUsableSize(slot_span));
   }
 
-#if BUILDFLAG(PA_DCHECK_IS_ON)
   // Write a new trailing cookie.
-  if (settings.allow_cookie) {
-    auto* object = static_cast<unsigned char*>(SlotStartToObject(slot_start));
+  if (settings.use_cookie) {
+    auto* object = static_cast<unsigned char*>(TaggedSlotStartToObject((TaggedSlot)slot_start));
+    PA_DCHECK( ((uintptr_t)(object) + GetSlotUsableSize(slot_span)) % internal::kMemTagGranuleSize == 0);
     internal::PartitionCookieWriteValue(object + GetSlotUsableSize(slot_span));
   }
-#endif
 
   return true;
 }
@@ -1197,14 +1227,14 @@ bool PartitionRoot::TryReallocInPlaceForDirectMap(
 bool PartitionRoot::TryReallocInPlaceForNormalBuckets(void* object,
                                                       SlotSpan* slot_span,
                                                       size_t new_size) {
-  uintptr_t slot_start = ObjectToSlotStart(object);
-  PA_DCHECK(internal::IsManagedByNormalBuckets(slot_start));
+  UntaggedSlot slot_start_untagged = ObjectToUntaggedSlotStart(object);
+  PA_DCHECK(internal::IsManagedByNormalBuckets(slot_start_untagged.value()));
 
   // TODO: note that tcmalloc will "ignore" a downsizing realloc() unless the
   // new size is a significant percentage smaller. We could do the same if we
   // determine it is a win.
   if (AllocationCapacityFromRequestedSize(new_size) !=
-      AllocationCapacityFromSlotStart(slot_start)) {
+      AllocationCapacityFromSlotStart(slot_start_untagged)) {
     return false;
   }
   size_t current_usable_size = GetSlotUsableSize(slot_span);
@@ -1216,6 +1246,7 @@ bool PartitionRoot::TryReallocInPlaceForNormalBuckets(void* object,
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) && BUILDFLAG(PA_DCHECK_IS_ON)
     internal::PartitionRefCount* old_ref_count;
     if (brp_enabled()) {
+      XXX not implemented
       old_ref_count = internal::PartitionRefCountPointer(slot_start);
     }
 #endif  // BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) &&
@@ -1223,6 +1254,7 @@ bool PartitionRoot::TryReallocInPlaceForNormalBuckets(void* object,
     size_t new_raw_size = AdjustSizeForExtrasAdd(new_size);
     slot_span->SetRawSize(new_raw_size);
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) && BUILDFLAG(PA_DCHECK_IS_ON)
+    XXX not implemented
     if (brp_enabled()) {
       internal::PartitionRefCount* new_ref_count =
           internal::PartitionRefCountPointer(slot_start);
@@ -1230,14 +1262,12 @@ bool PartitionRoot::TryReallocInPlaceForNormalBuckets(void* object,
     }
 #endif  // BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) &&
         // BUILDFLAG(PA_DCHECK_IS_ON)
-#if BUILDFLAG(PA_DCHECK_IS_ON)
     // Write a new trailing cookie only when it is possible to keep track
     // raw size (otherwise we wouldn't know where to look for it later).
-    if (settings.allow_cookie) {
+    if (settings.use_cookie) {
       internal::PartitionCookieWriteValue(static_cast<unsigned char*>(object) +
                                           GetSlotUsableSize(slot_span));
     }
-#endif  // BUILDFLAG(PA_DCHECK_IS_ON)
   }
 
   // Always record a realloc() as a free() + malloc(), even if it's in
@@ -1283,6 +1313,10 @@ void* PartitionRoot::ReallocWithFlags(unsigned int flags,
     internal::PartitionExcessiveAllocationSize(new_size);
   }
 
+#if BUILDFLAG(TMEMK_ENABLE) && BUILDFLAG(TMEMK_INTEGRITY)
+  [[maybe_unused]] volatile uint64_t _access_for_violation = *(volatile uint64_t*)ptr;
+#endif
+
   const bool hooks_enabled = PartitionAllocHooks::AreHooksEnabled();
   bool overridden = false;
   size_t old_usable_size;
@@ -1339,8 +1373,30 @@ void* PartitionRoot::ReallocWithFlags(unsigned int flags,
     }
     internal::PartitionExcessiveAllocationSize(new_size);
   }
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+  new_size = internal::base::bits::AlignUp(new_size, internal::kMemTagGranuleSize);
+#endif // PA_CONFIG(HAS_MEMORY_TAGGING)
 
   memcpy(ret, ptr, std::min(old_usable_size, new_size));
+
+#if BUILDFLAG(PA_DCHECK_IS_ON) // || 1
+  size_t copy_size = std::min(old_usable_size, new_size);
+
+  static int _printed = 0;
+  if(!_printed){
+    // __debug_always("%s %d old_usable_size %zu new_size %zu ", __FILE__, __LINE__, old_usable_size, new_size);
+    __debug_always("%s %d expensive checks %zu %zu ", __FILE__, __LINE__, copy_size, (copy_size / sizeof(uint64_t)));
+    _printed = 1;
+  }
+  // __debug_always("%s %d remove this loop!", __FILE__, __LINE__);
+  for (uint32_t Idx = 0; Idx < (copy_size / sizeof(uint64_t)); Idx++) {
+    if( ((uint64_t*)ret)[Idx] != ((uint64_t*)ptr)[Idx] ){
+      __debug_always("%p[%zu],%p[%zu] = 0x%016llx 0x%016llx", ret, Idx, ptr, Idx, ((uint64_t*)ret)[Idx], ((uint64_t*)ptr)[Idx]);
+    }
+  }
+  PA_CHECK( 0 == memcmp(ptr, ret, copy_size));
+#endif // BUILDFLAG(PA_DCHECK_IS_ON)
+
   Free(ptr);  // Implicitly protects the old ptr on MTE systems.
   return ret;
 #endif
diff --git a/partition_root.h b/partition_root.h
index de66fee..369c5bc 100644
--- a/partition_root.h
+++ b/partition_root.h
@@ -69,6 +69,7 @@
 #include "base/allocator/partition_allocator/thread_cache.h"
 #include "base/allocator/partition_allocator/thread_isolation/thread_isolation.h"
 #include "build/build_config.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
 
 #if BUILDFLAG(USE_STARSCAN)
 #include "base/allocator/partition_allocator/starscan/pcscan.h"
@@ -166,8 +167,8 @@ struct PartitionOptions {
     kAllowed,
   };
 
+  // TODO(bartekn): Remove.
   enum class Cookie : uint8_t {
-    kDisallowed,
     kAllowed,
   };
 
@@ -192,8 +193,12 @@ struct PartitionOptions {
   Cookie cookie = Cookie::kAllowed;
   BackupRefPtr backup_ref_ptr = BackupRefPtr::kDisabled;
   UseConfigurablePool use_configurable_pool = UseConfigurablePool::kNo;
-  size_t ref_count_size;
+  size_t ref_count_size = 0;
+#if BUILDFLAG(TMEMK_ENABLE)
+  MemoryTagging memory_tagging = MemoryTagging::kEnabled;
+#else  // BUILDFLAG(TMEMK_ENABLE)
   MemoryTagging memory_tagging = MemoryTagging::kDisabled;
+#endif  // BUILDFLAG(TMEMK_ENABLE)
 #if BUILDFLAG(ENABLE_THREAD_ISOLATION)
   ThreadIsolationOption thread_isolation;
 #endif
@@ -228,9 +233,8 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
   // Root settings accessed on fast paths.
   //
   // Careful! PartitionAlloc's performance is sensitive to its layout.  Please
-  // put the fast-path objects in the struct below, and the other ones after
-  // the union..
-  struct Settings {
+  // put the fast-path objects in the struct below.
+  struct alignas(internal::kPartitionCachelineSize) Settings {
     // Chromium-style: Complex constructor needs an explicit out-of-line
     // constructor.
     Settings();
@@ -250,19 +254,30 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
     bool with_thread_cache = false;
 
     bool allow_aligned_alloc = false;
-    bool allow_cookie = false;
+
+#if BUILDFLAG(TMEMK_TRIPWIRES)
+    bool use_cookie = true;
+#elif BUILDFLAG(PA_DCHECK_IS_ON)
+    bool use_cookie = false;
+#else
+    static constexpr bool use_cookie = false;
+#endif  // BUILDFLAG(PA_DCHECK_IS_ON)
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
     bool brp_enabled_ = false;
 #if PA_CONFIG(ENABLE_MAC11_MALLOC_SIZE_HACK)
     bool mac11_malloc_size_hack_enabled_ = false;
-    size_t mac11_malloc_size_hack_usable_size_;
+    size_t mac11_malloc_size_hack_usable_size_ = 0;
 #endif  // PA_CONFIG(ENABLE_MAC11_MALLOC_SIZE_HACK)
 #endif  // BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
     bool use_configurable_pool = false;
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if BUILDFLAG(TMEMK_ENABLE)
+    bool memory_tagging_enabled_ = true;
+#else  // BUILDFLAG(TMEMK_ENABLE)
     bool memory_tagging_enabled_ = false;
+#endif  // BUILDFLAG(TMEMK_ENABLE)
 #if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
-    size_t ref_count_size;
+    size_t ref_count_size = 0;
 #endif  // PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
 #if BUILDFLAG(ENABLE_THREAD_ISOLATION)
@@ -280,19 +295,11 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
 #endif  // PA_CONFIG(EXTRAS_REQUIRED)
   };
 
-  // Read-mostly settings.
-  union {
-    Settings settings;
-
-    // The flags above are accessed for all (de)allocations, and are mostly
-    // read-only. They should not share a cacheline with the data below, which
-    // is only touched when the lock is taken.
-    uint8_t one_cacheline[internal::kPartitionCachelineSize];
-  };
+  Settings settings;
 
   // Not used on the fastest path (thread cache allocations), but on the fast
   // path of the central allocator.
-  ::partition_alloc::internal::Lock lock_;
+  alignas(internal::kPartitionCachelineSize) internal::Lock lock_;
 
   Bucket buckets[internal::kNumBuckets] = {};
   Bucket sentinel_bucket{};
@@ -524,12 +531,25 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
   // beginning of the slot that contains |object|.
   PA_ALWAYS_INLINE void FreeNoHooksImmediate(void* object,
                                              SlotSpan* slot_span,
-                                             uintptr_t slot_start);
+                                             TaggedSlot slot_start_tagged);
 
   PA_ALWAYS_INLINE size_t GetSlotUsableSize(SlotSpan* slot_span) {
     return AdjustSizeForExtrasSubtract(slot_span->GetUtilizedSlotSize());
   }
 
+
+#if PA_CONFIG(HAS_MEMORY_TAGGING)
+// Returns size that should be tagged. Avoiding the previous slot ref count if
+// it exists to avoid a race (crbug.com/1445816).
+
+PA_ALWAYS_INLINE size_t TagSizeForSlot(size_t slot_size) {
+  return AdjustSizeForExtrasSubtract(slot_size);
+}
+
+#endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
+
+
+
   PA_ALWAYS_INLINE static size_t GetUsableSize(void* ptr);
 
   // Same as GetUsableSize() except it adjusts the return value for macOS 11
@@ -544,7 +564,7 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
           PageAccessibilityConfiguration::Permissions) const;
 
   PA_ALWAYS_INLINE size_t
-  AllocationCapacityFromSlotStart(uintptr_t slot_start) const;
+  AllocationCapacityFromSlotStart(UntaggedSlot slot_start) const;
   PA_ALWAYS_INLINE size_t
   AllocationCapacityFromRequestedSize(size_t size) const;
 
@@ -582,13 +602,13 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
   static uint16_t SizeToBucketIndex(size_t size,
                                     BucketDistribution bucket_distribution);
 
-  PA_ALWAYS_INLINE void FreeInSlotSpan(uintptr_t slot_start,
+  PA_ALWAYS_INLINE void FreeInSlotSpan(TaggedSlot slot_start,
                                        SlotSpan* slot_span)
       PA_EXCLUSIVE_LOCKS_REQUIRED(internal::PartitionRootLock(this));
 
   // Frees memory, with |slot_start| as returned by |RawAlloc()|.
-  PA_ALWAYS_INLINE void RawFree(uintptr_t slot_start);
-  PA_ALWAYS_INLINE void RawFree(uintptr_t slot_start, SlotSpan* slot_span)
+  PA_ALWAYS_INLINE void RawFree(TaggedSlot slot_start);
+  PA_ALWAYS_INLINE void RawFree(TaggedSlot slot_start, SlotSpan* slot_span)
       PA_LOCKS_EXCLUDED(internal::PartitionRootLock(this));
 
   PA_ALWAYS_INLINE void RawFreeBatch(FreeListEntry* head,
@@ -597,7 +617,7 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
                                      SlotSpan* slot_span)
       PA_LOCKS_EXCLUDED(internal::PartitionRootLock(this));
 
-  PA_ALWAYS_INLINE void RawFreeWithThreadCache(uintptr_t slot_start,
+  PA_ALWAYS_INLINE void RawFreeWithThreadCache(TaggedSlot slot_start,
                                                SlotSpan* slot_span);
 
   // This is safe to do because we are switching to a bucket distribution with
@@ -658,30 +678,44 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
   }
 
   PA_ALWAYS_INLINE bool IsQuarantineAllowed() const {
+#if ! BUILDFLAG(TMEMK_QUARANTINING)
     return settings.quarantine_mode != QuarantineMode::kAlwaysDisabled;
+#else
+    return true;
+#endif
   }
 
   PA_ALWAYS_INLINE bool IsQuarantineEnabled() const {
+#if ! BUILDFLAG(TMEMK_QUARANTINING)
     return settings.quarantine_mode == QuarantineMode::kEnabled;
+#else
+    return true;
+#endif
   }
 
   PA_ALWAYS_INLINE bool ShouldQuarantine(void* object) const {
-    if (PA_UNLIKELY(settings.quarantine_mode != QuarantineMode::kEnabled)) {
+    // __debug("ShouldQuarantine %p", object);
+    if (PA_UNLIKELY(!IsQuarantineEnabled())) {
       return false;
     }
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
     if (PA_UNLIKELY(quarantine_always_for_testing)) {
+      // __debug("ShouldQuarantine %p %d", object, 1);
       return true;
     }
     // If quarantine is enabled and the tag overflows, move the containing slot
     // to quarantine, to prevent the attacker from exploiting a pointer that has
     // an old tag.
     if (PA_LIKELY(IsMemoryTaggingEnabled())) {
-      return internal::HasOverflowTag(object);
+      bool ret = internal::HasOverflowTag(object);
+      // __debug("ShouldQuarantine %p %d", object, ret);
+      return ret;
     }
     // Default behaviour if MTE is not enabled for this PartitionRoot.
+    // __debug("ShouldQuarantine %p %d", object, 1);
     return true;
 #else
+    // __debug("ShouldQuarantine %p %d", object, 1);
     return true;
 #endif
   }
@@ -762,8 +796,14 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
 
   // Adjusts the size by adding extras. Also include the 0->1 adjustment if
   // needed.
+
   PA_ALWAYS_INLINE size_t AdjustSizeForExtrasAdd(size_t size) const {
     size = AdjustSize0IfNeeded(size);
+// #if BUILDFLAG(TMEMK_TRIPWIRES)
+    if(settings.extras_size){
+      size = internal::base::bits::AlignUp(size, internal::kMemTagGranuleSize);
+    }
+// #endif // BUILDFLAG(TMEMK_TRIPWIRES)
     PA_DCHECK(size + settings.extras_size >= size);
     return size + settings.extras_size;
   }
@@ -776,25 +816,18 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
     return size - settings.extras_size;
   }
 
-  PA_ALWAYS_INLINE uintptr_t SlotStartToObjectAddr(uintptr_t slot_start) const {
-    // TODO(bartekn): Check that |slot_start| is indeed a slot start.
-    return slot_start + settings.extras_offset;
-  }
-
-  PA_ALWAYS_INLINE void* SlotStartToObject(uintptr_t slot_start) const {
+  PA_ALWAYS_INLINE void* TaggedSlotStartToObject(TaggedSlot slot_start) const {
     // TODO(bartekn): Check that |slot_start| is indeed a slot start.
-    return internal::TagAddr(SlotStartToObjectAddr(slot_start));
+    return (void*)((uintptr_t)slot_start.value() + settings.extras_offset);
   }
 
-  PA_ALWAYS_INLINE void* TaggedSlotStartToObject(
-      void* tagged_slot_start) const {
-    // TODO(bartekn): Check that |slot_start| is indeed a slot start.
-    return reinterpret_cast<void*>(
-        SlotStartToObjectAddr(reinterpret_cast<uintptr_t>(tagged_slot_start)));
+  PA_ALWAYS_INLINE UntaggedSlot ObjectToUntaggedSlotStart(void* object) const {
+    return (UntaggedSlot)(UntagPtr(object) - settings.extras_offset);
+    // TODO(bartekn): Check that the result is indeed a slot start.
   }
 
-  PA_ALWAYS_INLINE uintptr_t ObjectToSlotStart(void* object) const {
-    return UntagPtr(object) - settings.extras_offset;
+  PA_ALWAYS_INLINE TaggedSlot ObjectToTaggedSlotStart(void* object) const {
+    return (TaggedSlot)((uintptr_t)object - settings.extras_offset);
     // TODO(bartekn): Check that the result is indeed a slot start.
   }
 
@@ -867,13 +900,13 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
   //   |requested_size|.
   // - |usable_size| and |is_already_zeroed| are output only. |usable_size| is
   //   guaranteed to be larger or equal to AllocWithFlags()'s |requested_size|.
-  PA_ALWAYS_INLINE uintptr_t RawAlloc(Bucket* bucket,
+  PA_ALWAYS_INLINE TaggedSlot RawAlloc(Bucket* bucket,
                                       unsigned int flags,
                                       size_t raw_size,
                                       size_t slot_span_alignment,
                                       size_t* usable_size,
                                       bool* is_already_zeroed);
-  PA_ALWAYS_INLINE uintptr_t AllocFromBucket(Bucket* bucket,
+  PA_ALWAYS_INLINE TaggedSlot AllocFromBucket(Bucket* bucket,
                                              unsigned int flags,
                                              size_t raw_size,
                                              size_t slot_span_alignment,
@@ -889,7 +922,7 @@ struct PA_ALIGNAS(64) PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionRoot {
       PA_EXCLUSIVE_LOCKS_REQUIRED(internal::PartitionRootLock(this));
   void DecommitEmptySlotSpans()
       PA_EXCLUSIVE_LOCKS_REQUIRED(internal::PartitionRootLock(this));
-  PA_ALWAYS_INLINE void RawFreeLocked(uintptr_t slot_start)
+  PA_ALWAYS_INLINE void RawFreeLocked(TaggedSlot slot_start)
       PA_EXCLUSIVE_LOCKS_REQUIRED(internal::PartitionRootLock(this));
   ThreadCache* MaybeInitThreadCache();
 
@@ -1052,10 +1085,11 @@ PtrPosWithinAlloc IsPtrWithinSameAlloc(uintptr_t orig_address,
                                        uintptr_t test_address,
                                        size_t type_size);
 
-PA_ALWAYS_INLINE void PartitionAllocFreeForRefCounting(uintptr_t slot_start) {
-  PA_DCHECK(!PartitionRefCountPointer(slot_start)->IsAlive());
+PA_ALWAYS_INLINE void PartitionAllocFreeForRefCounting(TaggedSlot slot_start) {
+  UntaggedSlot slot_start_untagged = UntagSlot(slot_start);
+  PA_DCHECK(!PartitionRefCountPointer(slot_start_untagged.value())->IsAlive());
 
-  auto* slot_span = SlotSpanMetadata::FromSlotStart(slot_start);
+  auto* slot_span = SlotSpanMetadata::FromSlotStart(slot_start_untagged);
   auto* root = PartitionRoot::FromSlotSpan(slot_span);
   // PartitionRefCount is required to be allocated inside a `PartitionRoot` that
   // supports reference counts.
@@ -1068,12 +1102,13 @@ PA_ALWAYS_INLINE void PartitionAllocFreeForRefCounting(uintptr_t slot_start) {
   // with |kQuarantinedByte|.
   if (PA_LIKELY(!hook)) {
     unsigned char* object =
-        static_cast<unsigned char*>(root->SlotStartToObject(slot_start));
+        static_cast<unsigned char*>(root->TaggedSlotStartToObject(slot_start));
     for (size_t i = 0; i < root->GetSlotUsableSize(slot_span); ++i) {
       PA_DCHECK(object[i] == kQuarantinedByte);
     }
   }
-  DebugMemset(SlotStartAddr2Ptr(slot_start), kFreedByte,
+  // DebugMemset(SlotStartAddr2Ptr(slot_start_untagged), kFreedByte,
+  DebugMemset(slot_start.value(), kFreedByte,
               slot_span->GetUtilizedSlotSize()
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
                   - sizeof(PartitionRefCount)
@@ -1092,7 +1127,7 @@ PA_ALWAYS_INLINE void PartitionAllocFreeForRefCounting(uintptr_t slot_start) {
 
 }  // namespace internal
 
-PA_ALWAYS_INLINE uintptr_t
+PA_ALWAYS_INLINE TaggedSlot
 PartitionRoot::AllocFromBucket(Bucket* bucket,
                                unsigned int flags,
                                size_t raw_size,
@@ -1108,8 +1143,9 @@ PartitionRoot::AllocFromBucket(Bucket* bucket,
   // removed from the active list.
   PA_DCHECK(!slot_span->marked_full);
 
-  uintptr_t slot_start =
-      internal::SlotStartPtr2Addr(slot_span->get_freelist_head());
+  TaggedSlot slot_start_tagged = (TaggedSlot)slot_span->get_freelist_head(); //converts from PartitionFreelistEntry* to TaggedSlot
+  uintptr_t slot_start = internal::SlotStartPtr2Addr(slot_start_tagged);
+  // uintptr_t slot_start = internal::SlotStartPtr2Addr(slot_start_tagged);
   // Use the fast path when a slot is readily available on the free list of the
   // first active slot span. However, fall back to the slow path if a
   // higher-order alignment is requested, because an inner slot of an existing
@@ -1130,18 +1166,19 @@ PartitionRoot::AllocFromBucket(Bucket* bucket,
     // the size metadata.
     PA_DCHECK(!slot_span->CanStoreRawSize());
     PA_DCHECK(!slot_span->bucket->is_direct_mapped());
-    void* entry = slot_span->PopForAlloc(bucket->slot_size);
+    TaggedSlot entry = (TaggedSlot)slot_span->PopForAlloc(bucket->slot_size); //type conversion
     PA_DCHECK(internal::SlotStartPtr2Addr(entry) == slot_start);
 
     PA_DCHECK(slot_span->bucket == bucket);
   } else {
-    slot_start = bucket->SlowPathAlloc(this, flags, raw_size,
+    slot_start_tagged = bucket->SlowPathAlloc(this, flags, raw_size,
                                        slot_span_alignment, is_already_zeroed);
+    slot_start = UntagSlot(slot_start_tagged).value();
     if (PA_UNLIKELY(!slot_start)) {
-      return 0;
+      return (TaggedSlot)NULL;
     }
 
-    slot_span = SlotSpan::FromSlotStart(slot_start);
+    slot_span = SlotSpan::FromSlotStart(UntagSlot(slot_start_tagged));
     // TODO(crbug.com/1257655): See if we can afford to make this a CHECK.
     DCheckIsValidSlotSpan(slot_span);
     // For direct mapped allocations, |bucket| is the sentinel.
@@ -1161,7 +1198,14 @@ PartitionRoot::AllocFromBucket(Bucket* bucket,
   }
 #endif
 
-  return slot_start;
+  return slot_start_tagged;
+
+
+
+
+
+
+
 }
 
 // static
@@ -1236,24 +1280,25 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooks(void* object) {
   SlotSpan* slot_span = SlotSpan::FromObject(object);
   PA_DCHECK(PartitionRoot::FromSlotSpan(slot_span) == root);
 
-  uintptr_t slot_start = root->ObjectToSlotStart(object);
-  PA_DCHECK(slot_span == SlotSpan::FromSlotStart(slot_start));
+  TaggedSlot slot_start_tagged = root->ObjectToTaggedSlotStart(object);
 
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
   if (PA_LIKELY(root->IsMemoryTaggingEnabled())) {
     const size_t slot_size = slot_span->bucket->slot_size;
     if (PA_LIKELY(slot_size <= internal::kMaxMemoryTaggingSize)) {
-      // slot_span is untagged at this point, so we have to recover its tag
-      // again to increment and provide use-after-free mitigations.
-      size_t tag_size = slot_size;
-#if PA_CONFIG(INCREASE_REF_COUNT_SIZE_FOR_MTE)
-      tag_size -= root->settings.ref_count_size;
-#endif
-      void* retagged_slot_start = internal::TagMemoryRangeIncrement(
-          internal::TagAddr(slot_start), tag_size);
+      size_t tag_size = root->TagSizeForSlot(slot_size);
+      PA_DCHECK(root->settings.extras_offset == 0);
+      PA_DCHECK((void*)root->ObjectToTaggedSlotStart(object).value() == object);
+      slot_start_tagged = (TaggedSlot)internal::TagMemoryRangeIncrement(
+          root->ObjectToTaggedSlotStart(object).value(),
+          tag_size);
+      #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+
+        XXX not implemented
+      #endif
       // Incrementing the MTE-tag in the memory range invalidates the |object|'s
       // tag, so it must be retagged.
-      object = root->TaggedSlotStartToObject(retagged_slot_start);
+      object = root->TaggedSlotStartToObject(slot_start_tagged);
     }
   }
 #else
@@ -1271,9 +1316,12 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooks(void* object) {
   PA_PREFETCH(slot_span);
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
 
+  PA_DCHECK(slot_span == SlotSpan::FromSlotStart(UntagSlot(slot_start_tagged)));
+
 #if BUILDFLAG(USE_STARSCAN)
   // TODO(bikineev): Change the condition to PA_LIKELY once PCScan is enabled by
   // default.
+  XXX slot_start vs slot_start_tagged
   if (PA_UNLIKELY(root->ShouldQuarantine(object))) {
     // PCScan safepoint. Call before potentially scheduling scanning task.
     PCScan::JoinScanIfNeeded();
@@ -1283,15 +1331,55 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooks(void* object) {
       return;
     }
   }
+#elif BUILDFLAG(TMEMK_QUARANTINING)
+  if ( !slot_span->bucket->is_direct_mapped() && PA_UNLIKELY(root->ShouldQuarantine(object))) {
+    // internal::PartitionSuperPageExtentEntry* superPageExtent = slot_span->ToSuperPageExtent();
+    // superPageExtent->tmemk_quarantined++;
+    // slot_span->num_quarantined_slots++;
+    slot_span->SetNumQuarantinedSlots(slot_span->GetNumQuarantinedSlots() + 1);
+    // __debug("kMaxBucketed = %zu", internal::kMaxBucketed);
+    __debug("quarantining object %p slot_size %zu usable_slot_size %zu slot_span %p slots %zu bytes_per_span %zu num_quarantined_slots %zu", object, slot_span->GetUtilizedSlotSize(), root->GetSlotUsableSize(slot_span), slot_span, slot_span->bucket->get_slots_per_span(), slot_span->bucket->get_bytes_per_span(), slot_span->GetNumQuarantinedSlots());
+    __tmemk_trace("TMEMK_TRACE quarantining object %p slot_size %zu usable_slot_size %zu slot_span %p slots %zu bytes_per_span %zu num_quarantined_slots %zu", object, slot_span->GetUtilizedSlotSize(), root->GetSlotUsableSize(slot_span), slot_span, slot_span->bucket->get_slots_per_span(), slot_span->bucket->get_bytes_per_span(), slot_span->GetNumQuarantinedSlots());
+
+
+// #if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+//     internal::MemZeroWithMovdir64B_q(root->ObjectToTaggedSlotStart(object).value(), root->TagSizeForSlot(slot_size));
+// #endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+
+    if(slot_span->GetNumQuarantinedSlots() >= slot_span->bucket->get_slots_per_span()){
+      __debug("decommitting slot_span %p", slot_span);
+      // slot_span->num_allocated_slots = 0;
+      // slot_span->freelist_head = nullptr;
+      // slot_span->Decommit(root);
+
+      uintptr_t slot_span_start = internal::SlotSpanMetadata::ToSlotSpanStart(slot_span);
+      size_t size_to_decommit = slot_span->bucket->get_bytes_per_span();
+      PA_DCHECK(size_to_decommit > 0);
+      //memset to re-initialize page to make sure that whoever gets the page next doesnt die due to wrong keys.
+      //we need to do this here, since we didnt do it when quarantining the individual slots. (but maybe we should)
+      __tmemk_trace("TMEMK_TRACE decommitting slot_span %p %zu", slot_span, size_to_decommit);
+#if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+      __debug("decommitting slot_span MemZeroWithMovdir64B_q and DecommitSystemPagesForData of (%p,%zu)", slot_span, size_to_decommit);
+      internal::MemZeroWithMovdir64B_q(slot_span_start, size_to_decommit);
+#endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+
+      ::partition_alloc::internal::ScopedGuard guard{internal::PartitionRootLock(root)};
+      // DCheckRootLockIsAcquired(PartitionRoot::FromSlotSpan(this));
+      // PartitionRootLock(root).AssertAcquired();
+      root->DecommitSystemPagesForData(slot_span_start, size_to_decommit, PageAccessibilityDisposition::kRequireUpdate);
+    }
+    return; // (without actually freeing the object)
+  }
 #endif  // BUILDFLAG(USE_STARSCAN)
 
-  root->FreeNoHooksImmediate(object, slot_span, slot_start);
+  root->FreeNoHooksImmediate(object, slot_span, slot_start_tagged);
 }
 
 PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
     void* object,
     SlotSpan* slot_span,
-    uintptr_t slot_start) {
+    TaggedSlot slot_start_tagged) {
+  UntaggedSlot slot_start = UntagSlot(slot_start_tagged);
   // The thread cache is added "in the middle" of the main allocator, that is:
   // - After all the cookie/ref-count management
   // - Before the "raw" allocator.
@@ -1304,7 +1392,7 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
   PA_DCHECK(object);
   PA_DCHECK(slot_span);
   DCheckIsValidSlotSpan(slot_span);
-  PA_DCHECK(slot_start);
+  PA_DCHECK(slot_start.value());
 
   // Layout inside the slot:
   //   |[refcnt]|...object...|[empty]|[cookie]|[unused]|
@@ -1326,31 +1414,33 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
   // For more context, see the other "Layout inside the slot" comment inside
   // AllocWithFlagsNoHooks().
 
-#if BUILDFLAG(PA_DCHECK_IS_ON)
-  if (settings.allow_cookie) {
+  if (settings.use_cookie) {
     // Verify the cookie after the allocated region.
     // If this assert fires, you probably corrupted memory.
+    PA_DCHECK( ((uintptr_t)(object) + GetSlotUsableSize(slot_span)) % internal::kMemTagGranuleSize == 0);
     internal::PartitionCookieCheckValue(static_cast<unsigned char*>(object) +
                                         GetSlotUsableSize(slot_span));
   }
-#endif
 
-#if BUILDFLAG(USE_STARSCAN)
+#if BUILDFLAG(USE_STARSCAN) // || BUILDFLAG(TMEMK_QUARANTINING)
+  XXX not implemented
+
   // TODO(bikineev): Change the condition to PA_LIKELY once PCScan is enabled by
   // default.
   if (PA_UNLIKELY(IsQuarantineEnabled())) {
-    if (PA_LIKELY(internal::IsManagedByNormalBuckets(slot_start))) {
+    if (PA_LIKELY(internal::IsManagedByNormalBuckets(slot_start.value()))) {
       // Mark the state in the state bitmap as freed.
-      internal::StateBitmapFromAddr(slot_start)->Free(slot_start);
+      internal::StateBitmapFromAddr(slot_start.value())->Free(slot_start.value());
     }
   }
 #endif  // BUILDFLAG(USE_STARSCAN)
 
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+  XXX not implemented
   // TODO(keishi): Add PA_LIKELY when brp is fully enabled as |brp_enabled| will
   // be false only for the aligned partition.
   if (brp_enabled()) {
-    auto* ref_count = internal::PartitionRefCountPointer(slot_start);
+    auto* ref_count = internal::PartitionRefCountPointer(slot_start.value());
     // If there are no more references to the allocation, it can be freed
     // immediately. Otherwise, defer the operation and zap the memory to turn
     // potential use-after-free issues into unexploitable crashes.
@@ -1380,7 +1470,7 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
 
   // memset() can be really expensive.
 #if BUILDFLAG(PA_EXPENSIVE_DCHECKS_ARE_ON)
-  internal::DebugMemset(internal::SlotStartAddr2Ptr(slot_start),
+  internal::DebugMemset(slot_start_tagged.value(),
                         internal::kFreedByte,
                         slot_span->GetUtilizedSlotSize()
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
@@ -1392,7 +1482,7 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
   // efficiency.
   if (PA_UNLIKELY(internal::RandomPeriod()) &&
       !IsDirectMappedBucket(slot_span->bucket)) {
-    internal::SecureMemset(internal::SlotStartAddr2Ptr(slot_start), 0,
+    internal::SecureMemset(slot_start_tagged.value(), 0,
                            slot_span->GetUtilizedSlotSize()
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
                                - sizeof(internal::PartitionRefCount)
@@ -1401,24 +1491,25 @@ PA_ALWAYS_INLINE void PartitionRoot::FreeNoHooksImmediate(
   }
 #endif  // PA_CONFIG(ZERO_RANDOMLY_ON_FREE)
 
-  RawFreeWithThreadCache(slot_start, slot_span);
+  RawFreeWithThreadCache(slot_start_tagged, slot_span);
 }
 
-PA_ALWAYS_INLINE void PartitionRoot::FreeInSlotSpan(uintptr_t slot_start,
+PA_ALWAYS_INLINE void PartitionRoot::FreeInSlotSpan(TaggedSlot slot_start,
                                                     SlotSpan* slot_span) {
-  DecreaseTotalSizeOfAllocatedBytes(slot_start,
+  uintptr_t slot_start_untagged = UntagPtr(slot_start.value());
+  DecreaseTotalSizeOfAllocatedBytes(slot_start_untagged,
                                     slot_span->GetSlotSizeForBookkeeping());
 #if BUILDFLAG(USE_FREESLOT_BITMAP)
   if (!slot_span->bucket->is_direct_mapped()) {
-    internal::FreeSlotBitmapMarkSlotAsFree(slot_start);
+    internal::FreeSlotBitmapMarkSlotAsFree(slot_start_untagged);
   }
 #endif
 
   return slot_span->Free(slot_start, this);
 }
 
-PA_ALWAYS_INLINE void PartitionRoot::RawFree(uintptr_t slot_start) {
-  SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start);
+PA_ALWAYS_INLINE void PartitionRoot::RawFree(TaggedSlot slot_start) {
+  SlotSpan* slot_span = SlotSpan::FromSlotStart(UntagSlot(slot_start));
   RawFree(slot_start, slot_span);
 }
 
@@ -1430,7 +1521,7 @@ PA_ALWAYS_INLINE void PartitionRoot::RawFree(uintptr_t slot_start) {
 // a no-op or similar. The documentation doesn't say.
 #pragma optimize("", off)
 #endif
-PA_ALWAYS_INLINE void PartitionRoot::RawFree(uintptr_t slot_start,
+PA_ALWAYS_INLINE void PartitionRoot::RawFree(TaggedSlot slot_start,
                                              SlotSpan* slot_span) {
   // At this point we are about to acquire the lock, so we try to minimize the
   // risk of blocking inside the locked section.
@@ -1456,15 +1547,17 @@ PA_ALWAYS_INLINE void PartitionRoot::RawFree(uintptr_t slot_start,
   // RawFreeLocked()). This is intentional, as the thread cache is purged often,
   // and the memory has a consequence the memory has already been touched
   // recently (to link the thread cache freelist).
-  *static_cast<volatile uintptr_t*>(internal::SlotStartAddr2Ptr(slot_start)) =
-      0;
+#if BUILDFLAG(TMEMK_ENABLE) && BUILDFLAG(TMEMK_INTEGRITY)
+  [[maybe_unused]] volatile uint64_t _access_for_violation = *(volatile uint64_t*)slot_start.value();
+#endif
+  *static_cast<volatile uintptr_t*>((void*)slot_start.value()) = 0;
   // Note: even though we write to slot_start + sizeof(void*) as well, due to
   // alignment constraints, the two locations are always going to be in the same
   // OS page. No need to write to the second one as well.
   //
   // Do not move the store above inside the locked section.
 #if !(PA_CONFIG(IS_NONCLANG_MSVC))
-  __asm__ __volatile__("" : : "r"(slot_start) : "memory");
+  __asm__ __volatile__("" : : "r"(slot_start.value()) : "memory");
 #endif
 
   ::partition_alloc::internal::ScopedGuard guard{
@@ -1498,7 +1591,7 @@ PA_ALWAYS_INLINE void PartitionRoot::RawFreeBatch(FreeListEntry* head,
 }
 
 PA_ALWAYS_INLINE void PartitionRoot::RawFreeWithThreadCache(
-    uintptr_t slot_start,
+    TaggedSlot slot_start,
     SlotSpan* slot_span) {
   // PA_LIKELY: performance-sensitive partitions have a thread cache,
   // direct-mapped allocations are uncommon.
@@ -1508,6 +1601,7 @@ PA_ALWAYS_INLINE void PartitionRoot::RawFreeWithThreadCache(
     size_t bucket_index =
         static_cast<size_t>(slot_span->bucket - this->buckets);
     size_t slot_size;
+    // PA_CHECK(internal::UntagAddr(slot_start) != slot_start); // slot_start must be tagged already (if it is taggable)
     if (PA_LIKELY(thread_cache->MaybePutInCache(slot_start, bucket_index,
                                                 &slot_size))) {
       // This is a fast path, avoid calling GetSlotUsableSize() in Release
@@ -1532,8 +1626,9 @@ PA_ALWAYS_INLINE void PartitionRoot::RawFreeWithThreadCache(
   RawFree(slot_start, slot_span);
 }
 
-PA_ALWAYS_INLINE void PartitionRoot::RawFreeLocked(uintptr_t slot_start) {
-  SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start);
+PA_ALWAYS_INLINE void PartitionRoot::RawFreeLocked(TaggedSlot slot_start) {
+  UntaggedSlot slot_start_untagged = UntagSlot(slot_start);
+  SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start_untagged);
   // Direct-mapped deallocation releases then re-acquires the lock. The caller
   // may not expect that, but we never call this function on direct-mapped
   // allocations.
@@ -1683,7 +1778,7 @@ PA_ALWAYS_INLINE size_t PartitionRoot::GetUsableSize(void* ptr) {
     return 0;
   }
   auto* slot_span = SlotSpan::FromObjectInnerPtr(ptr);
-  auto* root = PartitionRoot::FromSlotSpan(slot_span);
+  auto* root = FromSlotSpan(slot_span);
   return root->GetSlotUsableSize(slot_span);
 }
 
@@ -1694,7 +1789,7 @@ PartitionRoot::GetUsableSizeWithMac11MallocSizeHack(void* ptr) {
     return 0;
   }
   auto* slot_span = SlotSpan::FromObjectInnerPtr(ptr);
-  auto* root = PartitionRoot::FromSlotSpan(slot_span);
+  auto* root = FromSlotSpan(slot_span);
   size_t usable_size = root->GetSlotUsableSize(slot_span);
 #if PA_CONFIG(ENABLE_MAC11_MALLOC_SIZE_HACK)
   // Check |mac11_malloc_size_hack_enabled_| flag first as this doesn't
@@ -1725,10 +1820,15 @@ PartitionRoot::GetPageAccessibility(bool request_tagging) const {
   if (IsMemoryTaggingEnabled() && request_tagging) {
     permissions = PageAccessibilityConfiguration::kReadWriteTagged;
   }
+  // __debug("IsMemoryTaggingEnabled() = %d", IsMemoryTaggingEnabled());
+  // __debug("request_tagging = %d", request_tagging);
+  // __debug("GetPageAccessibility0 = %d", PageAccessibilityConfiguration(permissions));
 #endif
 #if BUILDFLAG(ENABLE_THREAD_ISOLATION)
+  // __debug("GetPageAccessibility1 = %d", PageAccessibilityConfiguration(permissions, settings.thread_isolation));
   return PageAccessibilityConfiguration(permissions, settings.thread_isolation);
 #else
+  // __debug("GetPageAccessibility2 = %d", PageAccessibilityConfiguration(permissions));
   return PageAccessibilityConfiguration(permissions);
 #endif
 }
@@ -1747,7 +1847,7 @@ PartitionRoot::PageAccessibilityWithThreadIsolationIfEnabled(
 // a new allocation (or realloc) happened with that returned value, it'd use
 // the same amount of underlying memory.
 PA_ALWAYS_INLINE size_t
-PartitionRoot::AllocationCapacityFromSlotStart(uintptr_t slot_start) const {
+PartitionRoot::AllocationCapacityFromSlotStart(UntaggedSlot slot_start) const {
   auto* slot_span = SlotSpan::FromSlotStart(slot_start);
   return AdjustSizeForExtrasSubtract(slot_span->bucket->slot_size);
 }
@@ -1815,6 +1915,7 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsInternal(
   return object;
 }
 
+static_assert(sizeof(internal::PartitionFreelistEntry) <= internal::kMemTagGranuleSize);
 PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
     unsigned int flags,
     size_t requested_size,
@@ -1845,9 +1946,13 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
       SizeToBucketIndex(raw_size, this->GetBucketDistribution());
   size_t usable_size;
   bool is_already_zeroed = false;
-  uintptr_t slot_start = 0;
-  size_t slot_size;
+  UntaggedSlot slot_start = (UntaggedSlot)NULL;
+  void* object = NULL;
+  size_t slot_size = 0;
 
+// #if BUILDFLAG(TMEMK_QUARANTINING)
+  // const bool is_quarantine_enabled = IsQuarantineEnabled();
+// #el
 #if BUILDFLAG(USE_STARSCAN)
   const bool is_quarantine_enabled = IsQuarantineEnabled();
   // PCScan safepoint. Call before trying to allocate from cache.
@@ -1869,10 +1974,12 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
     // Note: getting slot_size from the thread cache rather than by
     // `buckets[bucket_index].slot_size` to avoid touching `buckets` on the fast
     // path.
-    slot_start = thread_cache->GetFromCache(bucket_index, &slot_size);
+    TaggedSlot slot_start_tagged = thread_cache->GetFromCache(bucket_index, &slot_size);
+    object = TaggedSlotStartToObject(slot_start_tagged);
+    slot_start = UntagSlot(slot_start_tagged);
 
     // PA_LIKELY: median hit rate in the thread cache is 95%, from metrics.
-    if (PA_LIKELY(slot_start)) {
+    if (PA_LIKELY(slot_start.value())) {
       // This follows the logic of SlotSpanMetadata::GetUsableSize for small
       // buckets, which is too expensive to call here.
       // Keep it in sync!
@@ -1892,20 +1999,56 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
       PA_DCHECK(!slot_span->bucket->is_direct_mapped());
 #endif
     } else {
-      slot_start =
+      slot_start_tagged =
           RawAlloc(buckets + bucket_index, flags, raw_size, slot_span_alignment,
                    &usable_size, &is_already_zeroed);
+      object = TaggedSlotStartToObject(slot_start_tagged);
+      slot_start = UntagSlot(slot_start_tagged);
+      // __debug("%s:%d %p %p", __FILE__, __LINE__, (void*)object, (void*)slot_start);
+
     }
   } else {
-    slot_start =
+    TaggedSlot slot_start_tagged =
         RawAlloc(buckets + bucket_index, flags, raw_size, slot_span_alignment,
                  &usable_size, &is_already_zeroed);
+    object = TaggedSlotStartToObject(slot_start_tagged);
+    slot_start = UntagSlot(slot_start_tagged);
+    // __debug("%s:%d %p %p", __FILE__, __LINE__, (void*)object, (void*)slot_start);
   }
 
-  if (PA_UNLIKELY(!slot_start)) {
+  if (PA_UNLIKELY(!slot_start.value())) {
     return nullptr;
   }
 
+#if BUILDFLAG(TMEMK_THREAD_ISOLATION)
+  // tag slot
+  TaggedSlot slot_start_tagged = (TaggedSlot)slot_start.value();
+  {
+    SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start);
+    if(!slot_size){
+      slot_size = slot_span->bucket->slot_size;
+    }
+    auto* root = PartitionRoot::FromSlotSpan(slot_span);
+    //Note, at this point the slot_start_tagged is probably untagged because our PartitionFreeListEntry objects are untagged at free when using thread isolation
+    bool use_tagging = root->IsMemoryTaggingEnabled() && raw_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING);
+
+    if(use_tagging){
+      size_t tag_size = root->TagSizeForSlot(slot_size);
+      PA_DCHECK(root->settings.extras_offset == 0);
+      PA_DCHECK(TaggedSlotStartToObject(slot_start_tagged) == (void*)slot_start_tagged.value());
+      slot_start_tagged = internal::TagRegionForThread((uintptr_t)slot_start_tagged.value(), tag_size);
+    }
+  }
+  #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
+    XXX not implemented
+  #endif
+
+  object = TaggedSlotStartToObject(slot_start_tagged);
+  slot_start = UntagSlot(slot_start_tagged);
+#endif
+
+  __tmemk_trace("TMEMK_TRACE AllocWithFlagsNoHooks %zu %p", SlotSpan::FromSlotStart(slot_start)->bucket->slot_size, object);
+
   if (PA_LIKELY(ThreadCache::IsValid(thread_cache))) {
     thread_cache->RecordAllocation(usable_size);
   }
@@ -1918,12 +2061,15 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
   //   <---------(d)--------->   +   <--(d)--->
   //   <-----------------(e)------------------>
   //   <----------------------(f)---------------------->
+  //            <--------(g)------------------>
+  //
   //     (a) requested_size
   //     (b) usable_size
   //     (c) extras
   //     (d) raw_size
   //     (e) utilized_slot_size
   //     (f) slot_size
+  //     (g) usable_size + extras = tagged. refcnt is not tagged.
   // Notes:
   // - Ref-count may or may not exist in the slot, depending on brp_enabled().
   // - Cookie exists only in the BUILDFLAG(PA_DCHECK_IS_ON) case.
@@ -1948,22 +2094,49 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
   //   <----(d)----->   +   <--(d)--->   +    <--(d)--->
   //   <-------------(e)------------->   +    <--(e)--->
   //   <----------------------(f)---------------------->
+  //   <--------(g)------------------>
   // Notes:
   // If |slot_start| is not SystemPageSize()-aligned (possible only for small
   // allocations), ref-count of this slot is stored at the end of the previous
   // slot. Otherwise it is stored in ref-count table placed after the super page
   // metadata. For simplicity, the space for ref-count is still reserved at the
   // end of previous slot, even though redundant.
+  //
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
+  PA_CHECK(0);
+  XXX not implemented
+#endif
 
-  void* object = SlotStartToObject(slot_start);
+  const bool use_tagging = ((uintptr_t)object != UntagPtr(object)); // we are trusting that if we hav a tagged pointer here, then we do want to use tagging.
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(PA_DCHECK_IS_ON)
+  {
+    SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start);
+    auto* root = PartitionRoot::FromSlotSpan(slot_span);
+    // PA_DCHECK(use_tagging == root->IsMemoryTaggingEnabled() && raw_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING));
+    if(!(use_tagging == root->IsMemoryTaggingEnabled() && raw_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING))){
+      if(!slot_size){
+        slot_size = slot_span->bucket->slot_size;
+      }
+      // __debug("use_tagging = %d (?) slot_size = %zu raw_size = %zu internal::kMaxMemoryTaggingSize = %zu", use_tagging, slot_size, raw_size, internal::kMaxMemoryTaggingSize);
+    }
+    PA_DCHECK(UntagPtr(object) == slot_start.value());
+    // const bool use_tagging = root->IsMemoryTaggingEnabled() && raw_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING);
+    // if(use_tagging){
+      // __debug("object = %p. slot_start = %p. is same = %d", object, (void*)slot_start.value(), object == (void*)slot_start.value());
+      // PA_DCHECK(object != (void*)slot_start.value());
+      // PA_DCHECK(UntagPtr(object) == slot_start.value());
+      // PA_DCHECK(use_tagging == (UntagPtr(object) != (uintptr_t)object));
+      // // void* object = SlotStartToObject(slot_start);
+    // }
+  }
+#endif
 
-#if BUILDFLAG(PA_DCHECK_IS_ON)
   // Add the cookie after the allocation.
-  if (settings.allow_cookie) {
+  if (settings.use_cookie) {
+    PA_DCHECK( ((uintptr_t)(object) + usable_size) % internal::kMemTagGranuleSize == 0);
     internal::PartitionCookieWriteValue(static_cast<unsigned char*>(object) +
                                         usable_size);
   }
-#endif
 
   // Fill the region kUninitializedByte (on debug builds, if not requested to 0)
   // or 0 (if requested and not 0 already).
@@ -1971,11 +2144,45 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
   // PA_LIKELY: operator new() calls malloc(), not calloc().
   if (PA_LIKELY(!zero_fill)) {
     // memset() can be really expensive.
+    // __debug("later: optimize to never need additional memset since we can do this while retagging. but we probably do have to memset our freeslot object thingy");
 #if BUILDFLAG(PA_EXPENSIVE_DCHECKS_ARE_ON)
+
     internal::DebugMemset(object, internal::kUninitializedByte, usable_size);
 #endif
   } else if (!is_already_zeroed) {
-    memset(object, 0, usable_size);
+    // SlotSpan* slot_span = SlotSpan::FromSlotStart(slot_start);
+    // auto* root = PartitionRoot::FromSlotSpan(slot_span);
+    // const bool use_tagging = root->IsMemoryTaggingEnabled() && raw_size <= internal::kMaxMemoryTaggingSize && !BUILDFLAG(TMEMK_NOTAGGING);
+    if(!use_tagging){
+      memset(object, 0, usable_size);
+    }else{
+      // the slot itself, apart from the first tagging granule,
+      // which contained the PartitionFreeListEntry, is already zero-initialized
+      // since we do that at free. We only need to memset the first CL
+      // assuming sizeof(PartitionFreeListEntry) <= kMemTagGranuleSize
+#if BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+      // need to memset here instead because we didnt already do it at free()
+      memset(object, 0, usable_size);
+#else // BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+      #if BUILDFLAG(TMEMK_MEMZERO_INSTEAD_OF_MEMCPY_AT_FREE)
+        memset(object, 0, internal::kMemTagGranuleSize);
+      #else
+        memset(object, 0, usable_size);
+      #endif
+#endif // BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+      // __debug_always("zeroing: %p %zu %zu %d", object, usable_size, internal::kMemTagGranuleSize, use_tagging);
+    }
+#if BUILDFLAG(PA_DCHECK_IS_ON)
+    // checking if all memory is actually zero to verify that our assumptions are correct
+    for (uint32_t Idx = 0; Idx < (usable_size / sizeof(uint64_t)); Idx++) {
+      if(((uint64_t*)object)[Idx]){
+        __debug_always("%p[%zu] = 0x%llx", object, Idx, ((uint64_t*)object)[Idx]);
+      }
+    }
+    for (uint32_t Idx = 0; Idx < (usable_size / sizeof(uint64_t)); Idx++) {
+      PA_DCHECK( ((uint64_t*)object)[Idx] == 0 );
+    }
+#endif
   }
 
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
@@ -1992,7 +2199,8 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
       needs_mac11_malloc_size_hack = true;
     }
 #endif  // PA_CONFIG(ENABLE_MAC11_MALLOC_SIZE_HACK)
-    auto* ref_count = new (internal::PartitionRefCountPointer(slot_start))
+    XXX not implemented
+    auto* ref_count = new (internal::PartitionRefCountPointer(slot_start.value()))
         internal::PartitionRefCount(needs_mac11_malloc_size_hack);
 #if PA_CONFIG(REF_COUNT_STORE_REQUESTED_SIZE)
     ref_count->SetRequestedSize(requested_size);
@@ -2002,13 +2210,14 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
   }
 #endif  // BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
 
-#if BUILDFLAG(USE_STARSCAN)
+#if BUILDFLAG(USE_STARSCAN) //|| BUILDFLAG(TMEMK_QUARANTINING)
   // TODO(bikineev): Change the condition to PA_LIKELY once PCScan is enabled by
   // default.
+  XXX not implemented
   if (PA_UNLIKELY(is_quarantine_enabled)) {
-    if (PA_LIKELY(internal::IsManagedByNormalBuckets(slot_start))) {
+    if (PA_LIKELY(internal::IsManagedByNormalBuckets(slot_start.value()))) {
       // Mark the corresponding bits in the state bitmap as allocated.
-      internal::StateBitmapFromAddr(slot_start)->Allocate(slot_start);
+      internal::StateBitmapFromAddr(slot_start.value())->Allocate(slot_start.value());
     }
   }
 #endif  // BUILDFLAG(USE_STARSCAN)
@@ -2016,7 +2225,7 @@ PA_ALWAYS_INLINE void* PartitionRoot::AllocWithFlagsNoHooks(
   return object;
 }
 
-PA_ALWAYS_INLINE uintptr_t PartitionRoot::RawAlloc(Bucket* bucket,
+PA_ALWAYS_INLINE TaggedSlot PartitionRoot::RawAlloc(Bucket* bucket,
                                                    unsigned int flags,
                                                    size_t raw_size,
                                                    size_t slot_span_alignment,
@@ -2032,6 +2241,7 @@ PA_ALWAYS_INLINE void* PartitionRoot::AlignedAllocWithFlags(
     unsigned int flags,
     size_t alignment,
     size_t requested_size) {
+  // __debug("AlignedAllocWithFlags");
   // Aligned allocation support relies on the natural alignment guarantees of
   // PartitionAlloc. Specifically, it relies on the fact that slots within a
   // slot span are aligned to slot size, from the beginning of the span.
@@ -2160,12 +2370,6 @@ ThreadCache* PartitionRoot::GetThreadCache() {
   return PA_LIKELY(settings.with_thread_cache) ? ThreadCache::Get() : nullptr;
 }
 
-using ThreadSafePartitionRoot = PartitionRoot;
-
-static_assert(offsetof(ThreadSafePartitionRoot, lock_) ==
-                  internal::kPartitionCachelineSize,
-              "Padding is incorrect");
-
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
 // Usage in `raw_ptr.cc` is notable enough to merit a non-internal alias.
 using ::partition_alloc::internal::PartitionAllocGetSlotStartInBRPPool;
diff --git a/partition_stats.h b/partition_stats.h
index 82158ca..39398a4 100644
--- a/partition_stats.h
+++ b/partition_stats.h
@@ -118,6 +118,8 @@ struct PartitionBucketMemoryStats {
 // PartitionDumpStats for using the memory statistics.
 class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionStatsDumper {
  public:
+  virtual ~PartitionStatsDumper() = default;
+
   // Called to dump total memory used by partition, once per partition.
   virtual void PartitionDumpTotals(const char* partition_name,
                                    const PartitionMemoryStats*) = 0;
@@ -133,6 +135,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) SimplePartitionStatsDumper
     : public PartitionStatsDumper {
  public:
   SimplePartitionStatsDumper();
+  ~SimplePartitionStatsDumper() override = default;
 
   void PartitionDumpTotals(const char* partition_name,
                            const PartitionMemoryStats* memory_stats) override;
diff --git a/partition_superpage_extent_entry.h b/partition_superpage_extent_entry.h
index ee36a7a..7b87c4b 100644
--- a/partition_superpage_extent_entry.h
+++ b/partition_superpage_extent_entry.h
@@ -27,6 +27,11 @@ struct PartitionSuperPageExtentEntry {
   PartitionSuperPageExtentEntry* next;
   uint16_t number_of_consecutive_super_pages;
   uint16_t number_of_nonempty_slot_spans;
+  uint16_t tmemk_tag1;
+  uint16_t tmemk_tag2;
+  //NOTE: in theory we can just generate tmemk_tag2 based on tmemk_tag1 if we need more space here.
+  //NOTE: we can also reduce the tag field(s) to 8bit since we currently only have HW with 6 or 7 bits. (it does not need to be able to hold the overflow key, which might have more bits)
+  // uint64_t unused; //Note: kPageMetadataSize can be 32B in total
 
   PA_ALWAYS_INLINE void IncrementNumberOfNonemptySlotSpans() {
     DCheckNumberOfPartitionPagesInSuperPagePayload(
@@ -39,6 +44,7 @@ struct PartitionSuperPageExtentEntry {
     --number_of_nonempty_slot_spans;
   }
 };
+// static_assert(kSuperPageSize / internal::kSmallestBucket <= std::numeric_limits<decltype(PartitionSuperPageExtentEntry::tmemk_quarantined)>::max());
 
 static_assert(
     sizeof(PartitionSuperPageExtentEntry) <= kPageMetadataSize,
diff --git a/pointers/raw_ptr.h b/pointers/raw_ptr.h
index c356da1..bae773e 100644
--- a/pointers/raw_ptr.h
+++ b/pointers/raw_ptr.h
@@ -50,14 +50,12 @@
 
 #if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT)
 #include "base/allocator/partition_allocator/pointers/raw_ptr_backup_ref_impl.h"
-#endif
-
-#if BUILDFLAG(USE_ASAN_UNOWNED_PTR)
+#elif BUILDFLAG(USE_ASAN_UNOWNED_PTR)
 #include "base/allocator/partition_allocator/pointers/raw_ptr_asan_unowned_impl.h"
-#endif
-
-#if BUILDFLAG(USE_HOOKABLE_RAW_PTR)
+#elif BUILDFLAG(USE_HOOKABLE_RAW_PTR)
 #include "base/allocator/partition_allocator/pointers/raw_ptr_hookable_impl.h"
+#else
+#include "base/allocator/partition_allocator/pointers/raw_ptr_noop_impl.h"
 #endif
 
 namespace cc {
@@ -94,14 +92,10 @@ enum class RawPtrTraits : unsigned {
   // instead.
   kMayDangle = (1 << 0),
 
-#if BUILDFLAG(USE_ASAN_BACKUP_REF_PTR)
-  // Disables any hooks, by switching to NoOpImpl in that case.
+  // Disables any hooks, when building with BUILDFLAG(USE_HOOKABLE_RAW_PTR).
   //
   // Internal use only.
   kDisableHooks = (1 << 2),
-#else
-  kDisableHooks = kEmpty,
-#endif
 
   // Pointer arithmetic is discouraged and disabled by default.
   //
@@ -196,108 +190,6 @@ struct RawPtrGlobalSettings {
 
 namespace internal {
 
-struct RawPtrNoOpImpl {
-  static constexpr bool kMustZeroOnInit = false;
-  static constexpr bool kMustZeroOnMove = false;
-  static constexpr bool kMustZeroOnDestruct = false;
-
-  // Wraps a pointer.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* WrapRawPtr(T* ptr) {
-    return ptr;
-  }
-
-  // Notifies the allocator when a wrapped pointer is being removed or replaced.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr void ReleaseWrappedPtr(T*) {}
-
-  // Unwraps the pointer, while asserting that memory hasn't been freed. The
-  // function is allowed to crash on nullptr.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* SafelyUnwrapPtrForDereference(
-      T* wrapped_ptr) {
-    return wrapped_ptr;
-  }
-
-  // Unwraps the pointer, while asserting that memory hasn't been freed. The
-  // function must handle nullptr gracefully.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* SafelyUnwrapPtrForExtraction(
-      T* wrapped_ptr) {
-    return wrapped_ptr;
-  }
-
-  // Unwraps the pointer, without making an assertion on whether memory was
-  // freed or not.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* UnsafelyUnwrapPtrForComparison(
-      T* wrapped_ptr) {
-    return wrapped_ptr;
-  }
-
-  // Upcasts the wrapped pointer.
-  template <typename To, typename From>
-  PA_ALWAYS_INLINE static constexpr To* Upcast(From* wrapped_ptr) {
-    static_assert(std::is_convertible<From*, To*>::value,
-                  "From must be convertible to To.");
-    // Note, this cast may change the address if upcasting to base that lies in
-    // the middle of the derived object.
-    return wrapped_ptr;
-  }
-
-  // Advance the wrapped pointer by `delta_elems`.
-  template <
-      typename T,
-      typename Z,
-      typename =
-          std::enable_if_t<partition_alloc::internal::is_offset_type<Z>, void>>
-  PA_ALWAYS_INLINE static constexpr T* Advance(T* wrapped_ptr, Z delta_elems) {
-    return wrapped_ptr + delta_elems;
-  }
-
-  // Retreat the wrapped pointer by `delta_elems`.
-  template <
-      typename T,
-      typename Z,
-      typename =
-          std::enable_if_t<partition_alloc::internal::is_offset_type<Z>, void>>
-  PA_ALWAYS_INLINE static constexpr T* Retreat(T* wrapped_ptr, Z delta_elems) {
-    return wrapped_ptr - delta_elems;
-  }
-
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr ptrdiff_t GetDeltaElems(T* wrapped_ptr1,
-                                                            T* wrapped_ptr2) {
-    return wrapped_ptr1 - wrapped_ptr2;
-  }
-
-  // Returns a copy of a wrapped pointer, without making an assertion on whether
-  // memory was freed or not.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* Duplicate(T* wrapped_ptr) {
-    return wrapped_ptr;
-  }
-
-  // `WrapRawPtrForDuplication` and `UnsafelyUnwrapPtrForDuplication` are used
-  // to create a new raw_ptr<T> from another raw_ptr<T> of a different flavor.
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* WrapRawPtrForDuplication(T* ptr) {
-    return ptr;
-  }
-
-  template <typename T>
-  PA_ALWAYS_INLINE static constexpr T* UnsafelyUnwrapPtrForDuplication(
-      T* wrapped_ptr) {
-    return wrapped_ptr;
-  }
-
-  // This is for accounting only, used by unit tests.
-  PA_ALWAYS_INLINE constexpr static void IncrementSwapCountForTest() {}
-  PA_ALWAYS_INLINE constexpr static void IncrementLessCountForTest() {}
-  PA_ALWAYS_INLINE constexpr static void
-  IncrementPointerToMemberOperatorCountForTest() {}
-};
-
 // Wraps a raw_ptr/raw_ref implementation, with a class of the same interface
 // that provides accounting, for test purposes. raw_ptr/raw_ref that use it
 // perform extra bookkeeping, e.g. to track the number of times the raw_ptr is
@@ -491,18 +383,14 @@ struct TraitsToImpl {
       /*ExperimentalAsh=*/Contains(Traits, RawPtrTraits::kExperimentalAsh)>;
 
 #elif BUILDFLAG(USE_ASAN_UNOWNED_PTR)
-  using UnderlyingImpl = std::conditional_t<
-      Contains(Traits, RawPtrTraits::kMayDangle),
-      // No special bookkeeping required for this case,
-      // just treat these as ordinary pointers.
-      internal::RawPtrNoOpImpl,
-      internal::RawPtrAsanUnownedImpl<
-          Contains(Traits, RawPtrTraits::kAllowPtrArithmetic)>>;
+  using UnderlyingImpl = internal::RawPtrAsanUnownedImpl<
+      Contains(Traits, RawPtrTraits::kAllowPtrArithmetic),
+      Contains(Traits, RawPtrTraits::kMayDangle)>;
+
 #elif BUILDFLAG(USE_HOOKABLE_RAW_PTR)
-  using UnderlyingImpl =
-      std::conditional_t<Contains(Traits, RawPtrTraits::kDisableHooks),
-                         internal::RawPtrNoOpImpl,
-                         internal::RawPtrHookableImpl>;
+  using UnderlyingImpl = internal::RawPtrHookableImpl<
+      /*EnableHooks=*/!Contains(Traits, RawPtrTraits::kDisableHooks)>;
+
 #else
   using UnderlyingImpl = internal::RawPtrNoOpImpl;
 #endif
@@ -1172,8 +1060,16 @@ constexpr auto DanglingUntriaged = base::RawPtrTraits::kMayDangle;
 //
 // These were found from CQ runs and analysed in this dashboard:
 // https://docs.google.com/spreadsheets/d/1k12PQOG4y1-UEV9xDfP1F8FSk4cVFywafEYHmzFubJ8/
+//
+// This is not meant to be added manually. You can ignore this flag.
 constexpr auto FlakyDanglingUntriaged = base::RawPtrTraits::kMayDangle;
 
+// Dangling raw_ptr that is more likely to cause UAF: its memory was freed in
+// one task, and the raw_ptr was released in a different one.
+//
+// This is not meant to be added manually. You can ignore this flag.
+constexpr auto DanglingAcrossTasks = base::RawPtrTraits::kMayDangle;
+
 // The use of pointer arithmetic with raw_ptr is strongly discouraged and
 // disabled by default. Usually a container like span<> should be used
 // instead of the raw_ptr.
@@ -1193,6 +1089,8 @@ constexpr auto ExperimentalAsh = base::RawPtrTraits::kExperimentalAsh;
 // detected that those raw_ptr's were never released (either by calling
 // raw_ptr's destructor or by resetting its value), which can ultimately put
 // pressure on the BRP quarantine.
+//
+// This is not meant to be added manually. You can ignore this flag.
 constexpr auto LeakedDanglingUntriaged = base::RawPtrTraits::kMayDangle;
 
 namespace std {
diff --git a/pointers/raw_ptr_asan_unowned_impl.h b/pointers/raw_ptr_asan_unowned_impl.h
index dc9202e..4e73ea1 100644
--- a/pointers/raw_ptr_asan_unowned_impl.h
+++ b/pointers/raw_ptr_asan_unowned_impl.h
@@ -23,7 +23,7 @@ namespace base::internal {
 bool EndOfAliveAllocation(const volatile void* ptr, bool is_adjustable_ptr);
 bool LikelySmuggledScalar(const volatile void* ptr);
 
-template <bool IsAdjustablePtr>
+template <bool IsAdjustablePtr, bool MayDangle>
 struct RawPtrAsanUnownedImpl {
   // The first two are needed for correctness. The last one isn't technically a
   // must, but better to set it.
@@ -118,7 +118,7 @@ struct RawPtrAsanUnownedImpl {
 
   template <typename T>
   static void ProbeForLowSeverityLifetimeIssue(T* wrapped_ptr) {
-    if (wrapped_ptr) {
+    if (!MayDangle && wrapped_ptr) {
       const volatile void* probe_ptr =
           reinterpret_cast<const volatile void*>(wrapped_ptr);
       if (!LikelySmuggledScalar(probe_ptr) &&
diff --git a/pointers/raw_ptr_backup_ref_impl.cc b/pointers/raw_ptr_backup_ref_impl.cc
index 6deac74..d92ea26 100644
--- a/pointers/raw_ptr_backup_ref_impl.cc
+++ b/pointers/raw_ptr_backup_ref_impl.cc
@@ -40,15 +40,17 @@ void RawPtrBackupRefImpl<AllowDangling, ExperimentalAsh>::ReleaseInternal(
 #endif
   uintptr_t slot_start =
       partition_alloc::PartitionAllocGetSlotStartInBRPPool(address);
+  void* slot_start_tagged = ???;
+  XXX not yet supported because i think PartitionAllocFreeForRefCounting needs tagged pointers
   if constexpr (AllowDangling) {
     if (partition_alloc::internal::PartitionRefCountPointer(slot_start)
             ->ReleaseFromUnprotectedPtr()) {
-      partition_alloc::internal::PartitionAllocFreeForRefCounting(slot_start);
+      partition_alloc::internal::PartitionAllocFreeForRefCounting(slot_start_tagged);
     }
   } else {
     if (partition_alloc::internal::PartitionRefCountPointer(slot_start)
             ->Release()) {
-      partition_alloc::internal::PartitionAllocFreeForRefCounting(slot_start);
+      partition_alloc::internal::PartitionAllocFreeForRefCounting(slot_start_tagged);
     }
   }
 }
diff --git a/pointers/raw_ptr_hookable_impl.h b/pointers/raw_ptr_hookable_impl.h
index a47d0dd..75231d3 100644
--- a/pointers/raw_ptr_hookable_impl.h
+++ b/pointers/raw_ptr_hookable_impl.h
@@ -43,6 +43,7 @@ PA_COMPONENT_EXPORT(RAW_PTR) const RawPtrHooks* GetRawPtrHooks();
 PA_COMPONENT_EXPORT(RAW_PTR) void InstallRawPtrHooks(const RawPtrHooks*);
 PA_COMPONENT_EXPORT(RAW_PTR) void ResetRawPtrHooks();
 
+template <bool EnableHooks>
 struct RawPtrHookableImpl {
   // Since this Impl is used for BRP-ASan, match BRP as closely as possible.
   static constexpr bool kMustZeroOnInit = true;
@@ -53,7 +54,9 @@ struct RawPtrHookableImpl {
   template <typename T>
   PA_ALWAYS_INLINE static constexpr T* WrapRawPtr(T* ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->wrap_ptr(reinterpret_cast<uintptr_t>(ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->wrap_ptr(reinterpret_cast<uintptr_t>(ptr));
+      }
     }
     return ptr;
   }
@@ -62,7 +65,9 @@ struct RawPtrHookableImpl {
   template <typename T>
   PA_ALWAYS_INLINE static constexpr void ReleaseWrappedPtr(T* ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->release_wrapped_ptr(reinterpret_cast<uintptr_t>(ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->release_wrapped_ptr(reinterpret_cast<uintptr_t>(ptr));
+      }
     }
   }
 
@@ -72,8 +77,10 @@ struct RawPtrHookableImpl {
   PA_ALWAYS_INLINE static constexpr T* SafelyUnwrapPtrForDereference(
       T* wrapped_ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->safely_unwrap_for_dereference(
-          reinterpret_cast<uintptr_t>(wrapped_ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->safely_unwrap_for_dereference(
+            reinterpret_cast<uintptr_t>(wrapped_ptr));
+      }
     }
     return wrapped_ptr;
   }
@@ -84,8 +91,10 @@ struct RawPtrHookableImpl {
   PA_ALWAYS_INLINE static constexpr T* SafelyUnwrapPtrForExtraction(
       T* wrapped_ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->safely_unwrap_for_extraction(
-          reinterpret_cast<uintptr_t>(wrapped_ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->safely_unwrap_for_extraction(
+            reinterpret_cast<uintptr_t>(wrapped_ptr));
+      }
     }
     return wrapped_ptr;
   }
@@ -96,8 +105,10 @@ struct RawPtrHookableImpl {
   PA_ALWAYS_INLINE static constexpr T* UnsafelyUnwrapPtrForComparison(
       T* wrapped_ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->unsafely_unwrap_for_comparison(
-          reinterpret_cast<uintptr_t>(wrapped_ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->unsafely_unwrap_for_comparison(
+            reinterpret_cast<uintptr_t>(wrapped_ptr));
+      }
     }
     return wrapped_ptr;
   }
@@ -120,9 +131,11 @@ struct RawPtrHookableImpl {
           std::enable_if_t<partition_alloc::internal::is_offset_type<Z>, void>>
   PA_ALWAYS_INLINE static constexpr T* Advance(T* wrapped_ptr, Z delta_elems) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->advance(
-          reinterpret_cast<uintptr_t>(wrapped_ptr),
-          reinterpret_cast<uintptr_t>(wrapped_ptr + delta_elems));
+      if (EnableHooks) {
+        GetRawPtrHooks()->advance(
+            reinterpret_cast<uintptr_t>(wrapped_ptr),
+            reinterpret_cast<uintptr_t>(wrapped_ptr + delta_elems));
+      }
     }
     return wrapped_ptr + delta_elems;
   }
@@ -135,9 +148,11 @@ struct RawPtrHookableImpl {
           std::enable_if_t<partition_alloc::internal::is_offset_type<Z>, void>>
   PA_ALWAYS_INLINE static constexpr T* Retreat(T* wrapped_ptr, Z delta_elems) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->advance(
-          reinterpret_cast<uintptr_t>(wrapped_ptr),
-          reinterpret_cast<uintptr_t>(wrapped_ptr - delta_elems));
+      if (EnableHooks) {
+        GetRawPtrHooks()->advance(
+            reinterpret_cast<uintptr_t>(wrapped_ptr),
+            reinterpret_cast<uintptr_t>(wrapped_ptr - delta_elems));
+      }
     }
     return wrapped_ptr - delta_elems;
   }
@@ -153,7 +168,9 @@ struct RawPtrHookableImpl {
   template <typename T>
   PA_ALWAYS_INLINE static constexpr T* Duplicate(T* wrapped_ptr) {
     if (!partition_alloc::internal::base::is_constant_evaluated()) {
-      GetRawPtrHooks()->duplicate(reinterpret_cast<uintptr_t>(wrapped_ptr));
+      if (EnableHooks) {
+        GetRawPtrHooks()->duplicate(reinterpret_cast<uintptr_t>(wrapped_ptr));
+      }
     }
     return wrapped_ptr;
   }
diff --git a/shim/DEPS b/shim/DEPS
index c26088e..7119eb2 100644
--- a/shim/DEPS
+++ b/shim/DEPS
@@ -6,10 +6,6 @@
 # into partition_allocator/. This file will be removed away once the
 # migration gets done.
 
-include_rules = [
-    "+base/base_export.h",
-]
-
 specific_include_rules = {
   "allocator_shim_unittest\.cc$": [
     "+base/synchronization/waitable_event.h",
diff --git a/shim/allocator_interception_mac.h b/shim/allocator_interception_mac.h
index 58ce8ac..ac08b7c 100644
--- a/shim/allocator_interception_mac.h
+++ b/shim/allocator_interception_mac.h
@@ -7,8 +7,8 @@
 
 #include <stddef.h>
 
+#include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "base/allocator/partition_allocator/third_party/apple_apsl/malloc.h"
-#include "base/base_export.h"
 
 namespace allocator_shim {
 
@@ -29,23 +29,23 @@ void StoreFunctionsForAllZones();
 // |functions|.
 void ReplaceFunctionsForStoredZones(const MallocZoneFunctions* functions);
 
-BASE_EXPORT extern bool g_replaced_default_zone;
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) extern bool g_replaced_default_zone;
 
 // Calls the original implementation of malloc/calloc prior to interception.
-BASE_EXPORT bool UncheckedMallocMac(size_t size, void** result);
-BASE_EXPORT bool UncheckedCallocMac(size_t num_items,
-                                    size_t size,
-                                    void** result);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+bool UncheckedMallocMac(size_t size, void** result);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+bool UncheckedCallocMac(size_t num_items, size_t size, void** result);
 
 // Intercepts calls to default and purgeable malloc zones. Intercepts Core
 // Foundation and Objective-C allocations.
 // Has no effect on the default malloc zone if the allocator shim already
 // performs that interception.
-BASE_EXPORT void InterceptAllocationsMac();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void InterceptAllocationsMac();
 
 // Updates all malloc zones to use their original functions.
 // Also calls ClearAllMallocZonesForTesting.
-BASE_EXPORT void UninterceptMallocZonesForTesting();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void UninterceptMallocZonesForTesting();
 
 // Returns true if allocations are successfully being intercepted for all malloc
 // zones.
@@ -54,11 +54,12 @@ bool AreMallocZonesIntercepted();
 // heap_profiling::ProfilingClient needs to shim all malloc zones even ones
 // that are registered after the start-up time. ProfilingClient periodically
 // calls this API to make it sure that all malloc zones are shimmed.
-BASE_EXPORT void ShimNewMallocZones();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void ShimNewMallocZones();
 
 // Exposed for testing.
-BASE_EXPORT void ReplaceZoneFunctions(ChromeMallocZone* zone,
-                                      const MallocZoneFunctions* functions);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void ReplaceZoneFunctions(ChromeMallocZone* zone,
+                          const MallocZoneFunctions* functions);
 
 }  // namespace allocator_shim
 
diff --git a/shim/allocator_interception_mac.mm b/shim/allocator_interception_mac.mm
index 02c0740..ad46158 100644
--- a/shim/allocator_interception_mac.mm
+++ b/shim/allocator_interception_mac.mm
@@ -43,6 +43,12 @@
 #include "base/allocator/partition_allocator/partition_alloc_base/mac/mac_util.h"
 #endif
 
+// The patching of Objective-C runtime bits must be done without any
+// interference from the ARC machinery.
+#if defined(__has_feature) && __has_feature(objc_arc)
+#error "This file must not be compiled with ARC."
+#endif
+
 namespace allocator_shim {
 
 bool g_replaced_default_zone = false;
diff --git a/shim/allocator_shim.cc b/shim/allocator_shim.cc
index 5b52510..68ff237 100644
--- a/shim/allocator_shim.cc
+++ b/shim/allocator_shim.cc
@@ -64,6 +64,7 @@ bool CallNewHandler(size_t size) {
   if (!nh) {
     return false;
   }
+  __debug("calling new handler");
   (*nh)();
   // Assume the new_handler will abort if it fails. Exception are disabled and
   // we don't support the case of a new_handler throwing std::bad_balloc.
@@ -182,7 +183,45 @@ extern "C" {
 //       just suicide printing a message).
 //     - Assume it did succeed if it returns, in which case reattempt the alloc.
 
+#if BUILDFLAG(TMEMK_PADDING)
+#define __TMEMK_ALIGNMENT 64
+#else
+#define __TMEMK_ALIGNMENT 16
+#endif
+// #define __TMEMK_ALIGNMENT 4096
+
+PA_ALWAYS_INLINE void* ShimMemalign(size_t alignment,
+                                    size_t size,
+                                    void* context) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimMemalign %zu %zu", alignment, size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  alignment = partition_alloc::internal::base::bits::AlignUp(alignment, __TMEMK_ALIGNMENT);
+  // alignment = (alignment < __TMEMK_ALIGNMENT) ? __TMEMK_ALIGNMENT : alignment;
+  // PA_CHECK(alignment == __TMEMK_ALIGNMENT)
+  // if (((alignment % __TMEMK_ALIGNMENT) != 0) ||
+  //     !partition_alloc::internal::base::bits::IsPowerOfTwo(alignment)) {
+  //   return (void*)0;
+  // }
+#endif
+  const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
+  void* ptr;
+  do {
+    ptr = chain_head->alloc_aligned_function(chain_head, alignment, size,
+                                             context);
+  } while (!ptr && g_call_new_handler_on_malloc_failure &&
+           CallNewHandler(size));
+  // __debug("ShimMemalign returning %p (%zu %zu)", ptr, alignment, size);
+  return ptr;
+}
+
 PA_ALWAYS_INLINE void* ShimCppNew(size_t size) {
+
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimCppNew %zu", size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  return ShimMemalign(__TMEMK_ALIGNMENT, size, nullptr);
+#else
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   void* ptr;
   do {
@@ -193,18 +232,37 @@ PA_ALWAYS_INLINE void* ShimCppNew(size_t size) {
     ptr = chain_head->alloc_function(chain_head, size, context);
   } while (!ptr && CallNewHandler(size));
   return ptr;
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
 }
 
 PA_ALWAYS_INLINE void* ShimCppNewNoThrow(size_t size) {
+
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+
+  __debug("ShimCppNewNoThrow %zu", size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  return ShimMemalign(__TMEMK_ALIGNMENT, size, nullptr);
+#else
   void* context = nullptr;
 #if BUILDFLAG(IS_APPLE) && !BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
   context = malloc_default_zone();
 #endif
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   return chain_head->alloc_unchecked_function(chain_head, size, context);
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
 }
 
 PA_ALWAYS_INLINE void* ShimCppAlignedNew(size_t size, size_t alignment) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimCppAlignedNew %zu %zu", alignment, size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  alignment = partition_alloc::internal::base::bits::AlignUp(alignment, __TMEMK_ALIGNMENT);
+  // alignment = (alignment < __TMEMK_ALIGNMENT) ? __TMEMK_ALIGNMENT : alignment;
+  // if (((alignment % __TMEMK_ALIGNMENT) != 0) ||
+  //     !partition_alloc::internal::base::bits::IsPowerOfTwo(alignment)) {
+  //   return (void*)0;
+  // }
+#endif
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   void* ptr;
   do {
@@ -228,6 +286,13 @@ PA_ALWAYS_INLINE void ShimCppDelete(void* address) {
 }
 
 PA_ALWAYS_INLINE void* ShimMalloc(size_t size, void* context) {
+
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+
+  __debug("ShimMalloc %zu", size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  return ShimMemalign(__TMEMK_ALIGNMENT, size, context);
+#else
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   void* ptr;
   do {
@@ -235,9 +300,20 @@ PA_ALWAYS_INLINE void* ShimMalloc(size_t size, void* context) {
   } while (!ptr && g_call_new_handler_on_malloc_failure &&
            CallNewHandler(size));
   return ptr;
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
 }
 
 PA_ALWAYS_INLINE void* ShimCalloc(size_t n, size_t size, void* context) {
+
+
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+
+  __debug("ShimCalloc %zu", size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  void * ret = ShimMemalign(__TMEMK_ALIGNMENT, size, context);
+  memset(ret, 0x00, size);
+  return ret;
+#else
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   void* ptr;
   do {
@@ -246,36 +322,22 @@ PA_ALWAYS_INLINE void* ShimCalloc(size_t n, size_t size, void* context) {
   } while (!ptr && g_call_new_handler_on_malloc_failure &&
            CallNewHandler(size));
   return ptr;
-}
-
-PA_ALWAYS_INLINE void* ShimRealloc(void* address, size_t size, void* context) {
-  // realloc(size == 0) means free() and might return a nullptr. We should
-  // not call the std::new_handler in that case, though.
-  const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
-  void* ptr;
-  do {
-    ptr = chain_head->realloc_function(chain_head, address, size, context);
-  } while (!ptr && size && g_call_new_handler_on_malloc_failure &&
-           CallNewHandler(size));
-  return ptr;
-}
-
-PA_ALWAYS_INLINE void* ShimMemalign(size_t alignment,
-                                    size_t size,
-                                    void* context) {
-  const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
-  void* ptr;
-  do {
-    ptr = chain_head->alloc_aligned_function(chain_head, alignment, size,
-                                             context);
-  } while (!ptr && g_call_new_handler_on_malloc_failure &&
-           CallNewHandler(size));
-  return ptr;
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
 }
 
 PA_ALWAYS_INLINE int ShimPosixMemalign(void** res,
                                        size_t alignment,
                                        size_t size) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimPosixMemalign %zu %zu", alignment, size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  alignment = partition_alloc::internal::base::bits::AlignUp(alignment, __TMEMK_ALIGNMENT);
+  // alignment = (alignment < __TMEMK_ALIGNMENT) ? __TMEMK_ALIGNMENT : alignment;
+  // if (((alignment % __TMEMK_ALIGNMENT) != 0) ||
+  //     !partition_alloc::internal::base::bits::IsPowerOfTwo(alignment)) {
+  //   return EINVAL;
+  // }
+#endif
   // posix_memalign is supposed to check the arguments. See tc_posix_memalign()
   // in tc_malloc.cc.
   if (((alignment % sizeof(void*)) != 0) ||
@@ -288,6 +350,8 @@ PA_ALWAYS_INLINE int ShimPosixMemalign(void** res,
 }
 
 PA_ALWAYS_INLINE void* ShimValloc(size_t size, void* context) {
+  __debug("ShimValloc %zu", size);
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
   return ShimMemalign(GetCachedPageSize(), size, context);
 }
 
@@ -312,8 +376,9 @@ PA_ALWAYS_INLINE void ShimFree(void* address, void* context) {
 PA_ALWAYS_INLINE size_t ShimGetSizeEstimate(const void* address,
                                             void* context) {
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
-  return chain_head->get_size_estimate_function(
-      chain_head, const_cast<void*>(address), context);
+  size_t tmp = chain_head->get_size_estimate_function(
+      chain_head, const_cast<void*>(address), context);
+  return tmp;
 }
 
 PA_ALWAYS_INLINE bool ShimClaimedAddress(void* address, void* context) {
@@ -325,9 +390,14 @@ PA_ALWAYS_INLINE unsigned ShimBatchMalloc(size_t size,
                                           void** results,
                                           unsigned num_requested,
                                           void* context) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+PA_CHECK(0);
+return -1;
+#else
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   return chain_head->batch_malloc_function(chain_head, size, results,
                                            num_requested, context);
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
 }
 
 PA_ALWAYS_INLINE void ShimBatchFree(void** to_be_freed,
@@ -354,6 +424,15 @@ PA_ALWAYS_INLINE void ShimTryFreeDefault(void* ptr, void* context) {
 PA_ALWAYS_INLINE void* ShimAlignedMalloc(size_t size,
                                          size_t alignment,
                                          void* context) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  size = partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  alignment = partition_alloc::internal::base::bits::AlignUp(alignment, __TMEMK_ALIGNMENT);
+  // alignment = (alignment < __TMEMK_ALIGNMENT) ? __TMEMK_ALIGNMENT : alignment;
+  // if (((alignment % __TMEMK_ALIGNMENT) != 0) ||
+  //     !partition_alloc::internal::base::bits::IsPowerOfTwo(alignment)) {
+  //   return (void*)0;
+  // }
+#endif
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   void* ptr = nullptr;
   do {
@@ -368,6 +447,16 @@ PA_ALWAYS_INLINE void* ShimAlignedRealloc(void* address,
                                           size_t size,
                                           size_t alignment,
                                           void* context) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimAlignedRealloc %p %zu %zu", address, alignment, size);
+  size = size == 0 ? 0 : partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  alignment = partition_alloc::internal::base::bits::AlignUp(alignment, __TMEMK_ALIGNMENT);
+  // alignment = (alignment < __TMEMK_ALIGNMENT) ? __TMEMK_ALIGNMENT : alignment;
+  // if (((alignment % __TMEMK_ALIGNMENT) != 0) ||
+  //     !partition_alloc::internal::base::bits::IsPowerOfTwo(alignment)) {
+  //   return (void*)0;
+  // }
+#endif
   // _aligned_realloc(size == 0) means _aligned_free() and might return a
   // nullptr. We should not call the std::new_handler in that case, though.
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
@@ -380,7 +469,28 @@ PA_ALWAYS_INLINE void* ShimAlignedRealloc(void* address,
   return ptr;
 }
 
+PA_ALWAYS_INLINE void* ShimRealloc(void* address, size_t size, void* context) {
+#if BUILDFLAG(TMEMK_ALIGN_AT_SHIM)
+  __debug("ShimRealloc %p %zu", address, size);
+  size = size == 0 ? 0 : partition_alloc::internal::base::bits::AlignUp(size, __TMEMK_ALIGNMENT);
+  return ShimAlignedRealloc(address, size, __TMEMK_ALIGNMENT, context);
+#else
+  // realloc(size == 0) means free() and might return a nullptr. We should
+  // not call the std::new_handler in that case, though.
+  const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
+  void* ptr;
+  do {
+    ptr = chain_head->realloc_function(chain_head, address, size, context);
+  } while (!ptr && size && g_call_new_handler_on_malloc_failure &&
+           CallNewHandler(size));
+  return ptr;
+#endif /* BUILDFLAG(TMEMK_ALIGN_AT_SHIM) */
+}
+
 PA_ALWAYS_INLINE void ShimAlignedFree(void* address, void* context) {
+
+
+
   const allocator_shim::AllocatorDispatch* const chain_head = GetChainHead();
   return chain_head->aligned_free_function(chain_head, address, context);
 }
diff --git a/shim/allocator_shim.h b/shim/allocator_shim.h
index cf184b8..b841245 100644
--- a/shim/allocator_shim.h
+++ b/shim/allocator_shim.h
@@ -8,9 +8,9 @@
 #include <stddef.h>
 #include <stdint.h>
 
+#include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/types/strong_alias.h"
 #include "base/allocator/partition_allocator/partition_alloc_buildflags.h"
-#include "base/base_export.h"
 #include "build/build_config.h"
 
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC) && BUILDFLAG(USE_STARSCAN)
@@ -140,43 +140,48 @@ struct AllocatorDispatch {
 
 // When true makes malloc behave like new, w.r.t calling the new_handler if
 // the allocation fails (see set_new_mode() in Windows).
-BASE_EXPORT void SetCallNewHandlerOnMallocFailure(bool value);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void SetCallNewHandlerOnMallocFailure(bool value);
 
 // Allocates |size| bytes or returns nullptr. It does NOT call the new_handler,
 // regardless of SetCallNewHandlerOnMallocFailure().
-BASE_EXPORT void* UncheckedAlloc(size_t size);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void* UncheckedAlloc(size_t size);
 
 // Frees memory allocated with UncheckedAlloc().
-BASE_EXPORT void UncheckedFree(void* ptr);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void UncheckedFree(void* ptr);
 
 // Inserts |dispatch| in front of the allocator chain. This method is
 // thread-safe w.r.t concurrent invocations of InsertAllocatorDispatch().
 // The callers have responsibility for inserting a single dispatch no more
 // than once.
-BASE_EXPORT void InsertAllocatorDispatch(AllocatorDispatch* dispatch);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void InsertAllocatorDispatch(AllocatorDispatch* dispatch);
 
 // Test-only. Rationale: (1) lack of use cases; (2) dealing safely with a
 // removal of arbitrary elements from a singly linked list would require a lock
 // in malloc(), which we really don't want.
-BASE_EXPORT void RemoveAllocatorDispatchForTesting(AllocatorDispatch* dispatch);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void RemoveAllocatorDispatchForTesting(AllocatorDispatch* dispatch);
 
 #if BUILDFLAG(IS_APPLE)
 // The fallback function to be called when try_free_default_function receives a
 // pointer which doesn't belong to the allocator.
-BASE_EXPORT void TryFreeDefaultFallbackToFindZoneAndFree(void* ptr);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void TryFreeDefaultFallbackToFindZoneAndFree(void* ptr);
 #endif  // BUILDFLAG(IS_APPLE)
 
 #if BUILDFLAG(IS_APPLE)
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
-BASE_EXPORT void InitializeDefaultAllocatorPartitionRoot();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void InitializeDefaultAllocatorPartitionRoot();
 bool IsDefaultAllocatorPartitionRootInitialized();
 #endif  // BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
 // On macOS, the allocator shim needs to be turned on during runtime.
-BASE_EXPORT void InitializeAllocatorShim();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void InitializeAllocatorShim();
 #endif  // BUILDFLAG(IS_APPLE)
 
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
-BASE_EXPORT void EnablePartitionAllocMemoryReclaimer();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void EnablePartitionAllocMemoryReclaimer();
 
 using EnableBrp =
     partition_alloc::internal::base::StrongAlias<class EnableBrpTag, bool>;
@@ -195,7 +200,8 @@ enum class AlternateBucketDistribution : uint8_t { kDefault, kDenser };
 // If |thread_cache_on_non_quarantinable_partition| is specified, the
 // thread-cache will be enabled on the non-quarantinable partition. The
 // thread-cache on the main (malloc) partition will be disabled.
-BASE_EXPORT void ConfigurePartitions(
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void ConfigurePartitions(
     EnableBrp enable_brp,
     EnableBrpPartitionMemoryReclaimer enable_brp_memory_reclaimer,
     EnableMemoryTagging enable_memory_tagging,
@@ -204,10 +210,11 @@ BASE_EXPORT void ConfigurePartitions(
     size_t ref_count_size,
     AlternateBucketDistribution use_alternate_bucket_distribution);
 
-BASE_EXPORT uint32_t GetMainPartitionRootExtrasSize();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) uint32_t GetMainPartitionRootExtrasSize();
 
 #if BUILDFLAG(USE_STARSCAN)
-BASE_EXPORT void EnablePCScan(partition_alloc::internal::PCScan::InitConfig);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void EnablePCScan(partition_alloc::internal::PCScan::InitConfig);
 #endif
 #endif  // BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
 
diff --git a/shim/allocator_shim_default_dispatch_to_partition_alloc.cc b/shim/allocator_shim_default_dispatch_to_partition_alloc.cc
index 163e307..1c53eba 100644
--- a/shim/allocator_shim_default_dispatch_to_partition_alloc.cc
+++ b/shim/allocator_shim_default_dispatch_to_partition_alloc.cc
@@ -29,9 +29,31 @@
 #include "build/build_config.h"
 
 #if BUILDFLAG(IS_LINUX) || BUILDFLAG(IS_CHROMEOS)
+// #include <dlfcn.h>
 #include <malloc.h>
+
+#include <sys/mman.h>
+// For LIBC_SO which expands to proper libc name
+// #include <gnu/lib-names.h>
+
 #endif
 
+// extern "C" {
+// void* __libc_malloc(size_t size);
+// void* __libc_calloc(size_t n, size_t size);
+// void* __libc_realloc(void* address, size_t size);
+// void* __libc_memalign(size_t alignment, size_t size);
+// void __libc_free(void* ptr);
+// }
+// extern "C" {
+// void* __real_malloc(size_t);
+// void* __real_calloc(size_t, size_t);
+// void* __real_realloc(void*, size_t);
+// void* __real_memalign(size_t, size_t);
+// void __real_free(void*);
+// size_t __real_malloc_usable_size(void*);
+// }  // extern "C"
+
 using allocator_shim::AllocatorDispatch;
 
 namespace {
@@ -140,7 +162,7 @@ T* LeakySingleton<T, Constructor>::GetSlowPath() {
 
 class MainPartitionConstructor {
  public:
-  static partition_alloc::ThreadSafePartitionRoot* New(void* buffer) {
+  static partition_alloc::PartitionRoot* New(void* buffer) {
     constexpr partition_alloc::PartitionOptions::ThreadCache thread_cache =
 #if BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
         // Additional partitions may be created in ConfigurePartitions(). Since
@@ -154,8 +176,8 @@ class MainPartitionConstructor {
         // and only one is supported at a time.
         partition_alloc::PartitionOptions::ThreadCache::kDisabled;
 #endif  // BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
-    auto* new_root = new (buffer) partition_alloc::ThreadSafePartitionRoot(
-        partition_alloc::PartitionOptions{
+    auto* new_root = new (buffer)
+        partition_alloc::PartitionRoot(partition_alloc::PartitionOptions{
             .aligned_alloc =
                 partition_alloc::PartitionOptions::AlignedAlloc::kAllowed,
             .thread_cache = thread_cache,
@@ -163,42 +185,44 @@ class MainPartitionConstructor {
                 partition_alloc::PartitionOptions::Quarantine::kAllowed,
             .backup_ref_ptr =
                 partition_alloc::PartitionOptions::BackupRefPtr::kDisabled,
+// #if PA_CONFIG(HAS_MEMORY_TAGGING)
+//             .memory_tagging = partition_alloc::PartitionOptions::MemoryTagging::kEnabled,
+// #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
         });
 
     return new_root;
   }
 };
 
-LeakySingleton<partition_alloc::ThreadSafePartitionRoot,
-               MainPartitionConstructor>
-    g_root PA_CONSTINIT = {};
-partition_alloc::ThreadSafePartitionRoot* Allocator() {
+LeakySingleton<partition_alloc::PartitionRoot, MainPartitionConstructor> g_root
+    PA_CONSTINIT = {};
+partition_alloc::PartitionRoot* Allocator() {
   return g_root.Get();
 }
 
 // Original g_root_ if it was replaced by ConfigurePartitions().
-std::atomic<partition_alloc::ThreadSafePartitionRoot*> g_original_root(nullptr);
+std::atomic<partition_alloc::PartitionRoot*> g_original_root(nullptr);
 
 class AlignedPartitionConstructor {
  public:
-  static partition_alloc::ThreadSafePartitionRoot* New(void* buffer) {
+  static partition_alloc::PartitionRoot* New(void* buffer) {
     return g_root.Get();
   }
 };
 
-LeakySingleton<partition_alloc::ThreadSafePartitionRoot,
-               AlignedPartitionConstructor>
+LeakySingleton<partition_alloc::PartitionRoot, AlignedPartitionConstructor>
     g_aligned_root PA_CONSTINIT = {};
 
-partition_alloc::ThreadSafePartitionRoot* OriginalAllocator() {
+partition_alloc::PartitionRoot* OriginalAllocator() {
   return g_original_root.load(std::memory_order_relaxed);
 }
 
-partition_alloc::ThreadSafePartitionRoot* AlignedAllocator() {
+partition_alloc::PartitionRoot* AlignedAllocator() {
   return g_aligned_root.Get();
 }
 
 void* AllocateAlignedMemory(size_t alignment, size_t size) {
+  // __debug("AllocateAlignedMemory %lu %lu", alignment, size);
   // Memory returned by the regular allocator *always* respects |kAlignment|,
   // which is a power of two, and any valid alignment is also a power of two. So
   // we can directly fulfill these requests with the main allocator.
@@ -222,11 +246,26 @@ void* AllocateAlignedMemory(size_t alignment, size_t size) {
     // TODO(bartekn): See if the compiler optimizes branches down the stack on
     // Mac, where PartitionPageSize() isn't constexpr.
     return Allocator()->AllocWithFlagsNoHooks(
-        0, size, partition_alloc::PartitionPageSize());
+#if BUILDFLAG(TMEMK_RETURNNULL)
+        partition_alloc::AllocFlags::kReturnNull,
+#else
+        0,
+#endif
+        size, partition_alloc::PartitionPageSize());
   }
 
-  return AlignedAllocator()->AlignedAllocWithFlags(
+  void * ret = AlignedAllocator()->AlignedAllocWithFlags(
       partition_alloc::AllocFlags::kNoHooks, alignment, size);
+
+  // if(!ret){
+  //   //fallback to system allocator
+  //   // ret = calloc(1, size);
+  //   // ret = malloc(size);
+  //   ret = __libc_memalign(alignment, size);
+  //   __tmemk_trace("TMEMK_TRACE fallback aligned_alloc %zu %p", size, ret);
+  // }
+
+  return ret;
 }
 
 }  // namespace
@@ -236,9 +275,24 @@ namespace allocator_shim::internal {
 namespace {
 #if BUILDFLAG(IS_APPLE)
 unsigned int g_alloc_flags = 0;
+#elif BUILDFLAG(IS_LINUX)
+#if BUILDFLAG(TMEMK_RETURNNULL)
+constexpr unsigned int g_alloc_flags = partition_alloc::AllocFlags::kReturnNull;
+#else
+constexpr unsigned int g_alloc_flags = 0;
+#endif
 #else
 constexpr unsigned int g_alloc_flags = 0;
 #endif
+
+typedef struct {
+  void * addr;
+  size_t size;
+} mapping_info_t;
+
+#define NUM_MAPPINGS 16
+static mapping_info_t __mappings[NUM_MAPPINGS] = {{(void*)0,0},};
+
 }  // namespace
 
 void PartitionAllocSetCallNewHandlerOnMallocFailure(bool value) {
@@ -260,17 +314,70 @@ void PartitionAllocSetCallNewHandlerOnMallocFailure(bool value) {
 
 void* PartitionMalloc(const AllocatorDispatch*, size_t size, void* context) {
   partition_alloc::ScopedDisallowAllocations guard{};
-  return Allocator()->AllocWithFlagsNoHooks(
+  void * ret = Allocator()->AllocWithFlagsNoHooks(
       g_alloc_flags, size, partition_alloc::PartitionPageSize());
+  __tmemk_trace("TMEMK_TRACE malloc %zu %p", size, ret);
+  // PA_CHECK(ret != (void*)0);
+  // memset(ret, 0x00, size); __debug_always("%s %d remove this!", __FILE__, __LINE__);
+
+  if(!ret){
+    //fallback to system allocator
+    // if(!__test){
+    //   // void* h = dlopen("./malloc_wrapper.so", RTLD_NOW);
+    //   void * handle = dlopen(LIBC_SO, RTLD_LAZY);
+    //   if(handle){
+    //     void* (*mallocptr)(size_t);
+    //     *(void**)(&mallocptr) = dlsym(handle, "malloc");
+    //     if(mallocptr){
+    //       __test = mallocptr;
+    //       ret = (*mallocptr)(size);
+    //       __tmemk_trace("TMEMK_TRACE fallback malloc %zu %p", size, ret);
+    //     }
+    //   }
+    // }
+    // ret = calloc(1, size);
+    // ret = __real_malloc(size);
+    // __tmemk_trace("TMEMK_TRACE fallback malloc %zu %p", size, ret);
+    if(size > 4096){
+      //find free slot
+      for (size_t i = 0; i < NUM_MAPPINGS; i++){
+        if (__mappings[i].addr == NULL){
+          size = partition_alloc::internal::base::bits::AlignUp(size, 4096);
+          ret = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
+          if(ret != (void*)-1){
+            __mappings[i].addr = ret;
+            __mappings[i].size = size;
+          }
+          ret = ret == (void*)-1 ? 0 : ret;
+          __tmemk_trace("TMEMK_TRACE fallback malloc %zu %p", size, ret);
+          break;
+        }
+      }
+    }
+  }
+
+  return ret;
 }
 
 void* PartitionMallocUnchecked(const AllocatorDispatch*,
                                size_t size,
                                void* context) {
   partition_alloc::ScopedDisallowAllocations guard{};
-  return Allocator()->AllocWithFlagsNoHooks(
+  void * ret = Allocator()->AllocWithFlagsNoHooks(
       partition_alloc::AllocFlags::kReturnNull | g_alloc_flags, size,
       partition_alloc::PartitionPageSize());
+  __tmemk_trace("TMEMK_TRACE malloc %zu %p", size, ret);
+  // PA_CHECK(ret != (void*)0);
+  // memset(ret, 0x00, size); __debug_always("%s %d remove this!", __FILE__, __LINE__);
+
+  // if(!ret){
+  //   //fallback to system allocator
+  //   // ret = calloc(1, size);
+  //   ret = __libc_malloc(size);
+  //   __tmemk_trace("TMEMK_TRACE fallback malloc %zu %p", size, ret);
+  // }
+
+  return ret;
 }
 
 void* PartitionCalloc(const AllocatorDispatch*,
@@ -280,9 +387,40 @@ void* PartitionCalloc(const AllocatorDispatch*,
   partition_alloc::ScopedDisallowAllocations guard{};
   const size_t total =
       partition_alloc::internal::base::CheckMul(n, size).ValueOrDie();
-  return Allocator()->AllocWithFlagsNoHooks(
+  void * ret = Allocator()->AllocWithFlagsNoHooks(
       partition_alloc::AllocFlags::kZeroFill | g_alloc_flags, total,
       partition_alloc::PartitionPageSize());
+  __tmemk_trace("TMEMK_TRACE calloc %zu %p", total, ret);
+  // PA_CHECK(ret != (void*)0);
+  // memset(ret, 0x00, size); __debug_always("%s %d remove this!", __FILE__, __LINE__);
+#if BUILDFLAG(PA_DCHECK_IS_ON) // || 1
+  if(ret){
+    static int _printed = 0;
+    if(!_printed){
+      __debug_always("%s %d expensive checks", __FILE__, __LINE__);
+      _printed = 1;
+    }
+    // __debug_always("%s %d remove this loop!", __FILE__, __LINE__);
+    for (uint32_t Idx = 0; Idx < (total / sizeof(uint64_t)); Idx++) {
+      if(((uint64_t*)ret)[Idx]){
+        __debug_always("%p[%zu] = 0x%llx", ret, Idx, ((uint64_t*)ret)[Idx]);
+        PA_DCHECK(0);
+      }
+    }
+  }
+#endif // BUILDFLAG(PA_DCHECK_IS_ON)
+
+  // if(!ret){
+  //   //fallback to system allocator
+  //   // ret = calloc(1, size);
+  //   ret = __libc_malloc(size);
+  //   if(ret){
+  //     memset(ret, 0, size);
+  //   }
+  //   __tmemk_trace("TMEMK_TRACE fallback calloc %zu %p", size, ret);
+  // }
+
+  return ret;
 }
 
 void* PartitionMemalign(const AllocatorDispatch*,
@@ -290,7 +428,12 @@ void* PartitionMemalign(const AllocatorDispatch*,
                         size_t size,
                         void* context) {
   partition_alloc::ScopedDisallowAllocations guard{};
-  return AllocateAlignedMemory(alignment, size);
+  void * ret = AllocateAlignedMemory(alignment, size);
+  __tmemk_trace("TMEMK_TRACE aligned_alloc %zu %zu %p", size, alignment, ret);
+  // PA_CHECK(ret != (void*)0);
+  // memset(ret, 0x00, size); __debug_always("%s %d remove this!", __FILE__, __LINE__);
+
+  return ret;
 }
 
 void* PartitionAlignedAlloc(const AllocatorDispatch* dispatch,
@@ -298,7 +441,11 @@ void* PartitionAlignedAlloc(const AllocatorDispatch* dispatch,
                             size_t alignment,
                             void* context) {
   partition_alloc::ScopedDisallowAllocations guard{};
-  return AllocateAlignedMemory(alignment, size);
+  void * ret = AllocateAlignedMemory(alignment, size);
+  __tmemk_trace("TMEMK_TRACE aligned_alloc %zu %zu %p", size, alignment, ret);
+  // PA_CHECK(ret != (void*)0);
+  // memset(ret, 0x00, size); __debug_always("%s %d remove this!", __FILE__, __LINE__);
+  return ret;
 }
 
 // aligned_realloc documentation is
@@ -320,23 +467,45 @@ void* PartitionAlignedRealloc(const AllocatorDispatch* dispatch,
   } else {
     // size == 0 and address != null means just "free(address)".
     if (address) {
-      partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(address);
+      partition_alloc::PartitionRoot::FreeNoHooks(address);
     }
   }
   // The original memory block (specified by address) is unchanged if ENOMEM.
   if (!new_ptr) {
+    __tmemk_trace("TMEMK_TRACE aligned_realloc %p %zu %zu %p", address, size, alignment, nullptr);
+  // PA_CHECK(address != (void*)0);
+  // PA_CHECK(new_ptr != (void*)0);
     return nullptr;
   }
   // TODO(tasak): Need to compare the new alignment with the address' alignment.
   // If the two alignments are not the same, need to return nullptr with EINVAL.
   if (address) {
-    size_t usage =
-        partition_alloc::ThreadSafePartitionRoot::GetUsableSize(address);
+    size_t usage = partition_alloc::PartitionRoot::GetUsableSize(address);
     size_t copy_size = usage > size ? size : usage;
     memcpy(new_ptr, address, copy_size);
 
-    partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(address);
+    partition_alloc::PartitionRoot::FreeNoHooks(address);
+  }
+  __tmemk_trace("TMEMK_TRACE aligned_realloc %p %zu %zu %p", address, size, alignment, new_ptr);
+  // PA_CHECK(new_ptr != (void*)0);
+#if BUILDFLAG(PA_DCHECK_IS_ON) // || 1
+    static int _printed = 0;
+    if(!_printed){
+      __debug_always("%s %d expensive checks", __FILE__, __LINE__);
+      _printed = 1;
+    }
+  size_t usage = partition_alloc::PartitionRoot::GetUsableSize(address);
+  size_t copy_size = usage > size ? size : usage;
+  // __debug_always("%s %d remove this loop!", __FILE__, __LINE__);
+  for (uint32_t Idx = 0; Idx < (copy_size / sizeof(uint64_t)); Idx++) {
+    if( ((uint64_t*)new_ptr)[Idx] != ((uint64_t*)address)[Idx] ){
+      __debug_always("%p[%zu],%p[%zu] = 0x%016llx 0x%016llx",
+        new_ptr, Idx, address, Idx, ((uint64_t*)new_ptr)[Idx], ((uint64_t*)address)[Idx]);
+    }
   }
+  PA_CHECK( 0 == memcmp(address, new_ptr, copy_size)); // __debug_always("%s %d remove this!", __FILE__, __LINE__);
+#endif // BUILDFLAG(PA_DCHECK_IS_ON)
+
   return new_ptr;
 }
 
@@ -356,18 +525,45 @@ void* PartitionRealloc(const AllocatorDispatch*,
   }
 #endif  // BUILDFLAG(IS_APPLE)
 
-  return Allocator()->ReallocWithFlags(
+//TODO replace fallback
+  if (PA_UNLIKELY(!partition_alloc::IsManagedByPartitionAlloc(
+                      reinterpret_cast<uintptr_t>(address)) &&
+                  address)) {
+    //assume fallback
+    for (size_t i = 0; i < NUM_MAPPINGS; i++){
+      if (__mappings[i].addr == address){
+        void * ret = Allocator()->AllocWithFlagsNoHooks(
+          g_alloc_flags, size, partition_alloc::PartitionPageSize());
+        if(!ret){
+          return ret;
+        }
+        size_t copy_size = __mappings[i].size > size ? size : __mappings[i].size;
+        memcpy(ret, address, copy_size);
+        munmap(address, __mappings[i].size);
+        __mappings[i].addr = (void*)0;
+        __mappings[i].size = 0;
+      }
+    }
+  }
+
+  void * ret = Allocator()->ReallocWithFlags(
       partition_alloc::AllocFlags::kNoHooks | g_alloc_flags, address, size, "");
+  __tmemk_trace("TMEMK_TRACE realloc %p %zu %p", address, size, ret);
+  // PA_CHECK(ret != (void*)0);
+  return ret;
 }
 
 #if BUILDFLAG(PA_IS_CAST_ANDROID)
 extern "C" {
-void __real_free(void*);
+void __libc_free(void*);
 }  // extern "C"
 #endif  // BUILDFLAG(PA_IS_CAST_ANDROID)
 
 void PartitionFree(const AllocatorDispatch*, void* object, void* context) {
   partition_alloc::ScopedDisallowAllocations guard{};
+  if(object){
+    __tmemk_trace("TMEMK_TRACE free %p", object);
+  }
 #if BUILDFLAG(IS_APPLE)
   // TODO(bartekn): Add MTE unmasking here (and below).
   if (PA_UNLIKELY(!partition_alloc::IsManagedByPartitionAlloc(
@@ -384,18 +580,31 @@ void PartitionFree(const AllocatorDispatch*, void* object, void* context) {
   // malloc() pointer can be passed to PartitionAlloc's free(). If we don't own
   // the pointer, pass it along. This should not have a runtime cost vs regular
   // Android, since on Android we have a PA_CHECK() rather than the branch here.
-#if BUILDFLAG(PA_IS_CAST_ANDROID)
+#if BUILDFLAG(PA_IS_CAST_ANDROID) //|| 1
   if (PA_UNLIKELY(!partition_alloc::IsManagedByPartitionAlloc(
                       reinterpret_cast<uintptr_t>(object)) &&
                   object)) {
     // A memory region allocated by the system allocator is passed in this
-    // function.  Forward the request to `free()`, which is `__real_free()`
+    // function.  Forward the request to `free()`, which is `__libc_free()`
     // here.
-    return __real_free(object);
+    return __libc_free(object);
   }
 #endif  // BUILDFLAG(PA_IS_CAST_ANDROID)
 
-  partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(object);
+  if (PA_UNLIKELY(!partition_alloc::IsManagedByPartitionAlloc(
+                      reinterpret_cast<uintptr_t>(object)) &&
+                  object)) {
+    //assume fallback
+    for (size_t i = 0; i < NUM_MAPPINGS; i++){
+      if (__mappings[i].addr == object){
+        munmap(object, __mappings[i].size);
+        __mappings[i].addr = (void*)0;
+        __mappings[i].size = 0;
+      }
+    }
+  }
+
+  partition_alloc::PartitionRoot::FreeNoHooks(object);
 }
 
 #if BUILDFLAG(IS_APPLE)
@@ -412,7 +621,7 @@ void PartitionFreeDefiniteSize(const AllocatorDispatch*,
   partition_alloc::ScopedDisallowAllocations guard{};
   // TODO(lizeb): Optimize PartitionAlloc to use the size information. This is
   // still useful though, as we avoid double-checking that the address is owned.
-  partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(address);
+  partition_alloc::PartitionRoot::FreeNoHooks(address);
 }
 #endif  // BUILDFLAG(IS_APPLE)
 
@@ -425,7 +634,7 @@ size_t PartitionGetSizeEstimate(const AllocatorDispatch*,
     return 0;
   }
 
-#if BUILDFLAG(IS_APPLE)
+#if BUILDFLAG(IS_APPLE) || 1
   if (!partition_alloc::IsManagedByPartitionAlloc(
           reinterpret_cast<uintptr_t>(address))) {
     // The object pointed to by `address` is not allocated by the
@@ -436,8 +645,9 @@ size_t PartitionGetSizeEstimate(const AllocatorDispatch*,
 #endif  // BUILDFLAG(IS_APPLE)
 
   // TODO(lizeb): Returns incorrect values for aligned allocations.
-  const size_t size = partition_alloc::ThreadSafePartitionRoot::
-      GetUsableSizeWithMac11MallocSizeHack(address);
+  const size_t size =
+      partition_alloc::PartitionRoot::GetUsableSizeWithMac11MallocSizeHack(
+          address);
 #if BUILDFLAG(IS_APPLE)
   // The object pointed to by `address` is allocated by the PartitionAlloc.
   // So, this function must not return zero so that the malloc zone dispatcher
@@ -496,24 +706,22 @@ void PartitionTryFreeDefault(const AllocatorDispatch*,
     return allocator_shim::TryFreeDefaultFallbackToFindZoneAndFree(address);
   }
 
-  partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(address);
+  partition_alloc::PartitionRoot::FreeNoHooks(address);
 }
 #endif  // BUILDFLAG(IS_APPLE)
 
 // static
-partition_alloc::ThreadSafePartitionRoot* PartitionAllocMalloc::Allocator() {
+partition_alloc::PartitionRoot* PartitionAllocMalloc::Allocator() {
   return ::Allocator();
 }
 
 // static
-partition_alloc::ThreadSafePartitionRoot*
-PartitionAllocMalloc::OriginalAllocator() {
+partition_alloc::PartitionRoot* PartitionAllocMalloc::OriginalAllocator() {
   return ::OriginalAllocator();
 }
 
 // static
-partition_alloc::ThreadSafePartitionRoot*
-PartitionAllocMalloc::AlignedAllocator() {
+partition_alloc::PartitionRoot* PartitionAllocMalloc::AlignedAllocator() {
   return ::AlignedAllocator();
 }
 
@@ -593,7 +801,7 @@ void ConfigurePartitions(
   // ConfigurePartitions() is invoked explicitly from Chromium code, so this
   // shouldn't bite us here. Mentioning just in case we move this code earlier.
   static partition_alloc::internal::base::NoDestructor<
-      partition_alloc::ThreadSafePartitionRoot>
+      partition_alloc::PartitionRoot>
       new_main_partition(partition_alloc::PartitionOptions{
           .aligned_alloc =
               !use_dedicated_aligned_partition
@@ -613,14 +821,14 @@ void ConfigurePartitions(
                   ? partition_alloc::PartitionOptions::MemoryTagging::kEnabled
                   : partition_alloc::PartitionOptions::MemoryTagging::
                         kDisabled});
-  partition_alloc::ThreadSafePartitionRoot* new_root = new_main_partition.get();
+  partition_alloc::PartitionRoot* new_root = new_main_partition.get();
 
-  partition_alloc::ThreadSafePartitionRoot* new_aligned_root;
+  partition_alloc::PartitionRoot* new_aligned_root;
   if (use_dedicated_aligned_partition) {
     // TODO(bartekn): Use the original root instead of creating a new one. It'd
     // result in one less partition, but come at a cost of commingling types.
     static partition_alloc::internal::base::NoDestructor<
-        partition_alloc::ThreadSafePartitionRoot>
+        partition_alloc::PartitionRoot>
         new_aligned_partition(partition_alloc::PartitionOptions{
             .aligned_alloc =
                 partition_alloc::PartitionOptions::AlignedAlloc::kAllowed,
diff --git a/shim/allocator_shim_default_dispatch_to_partition_alloc.h b/shim/allocator_shim_default_dispatch_to_partition_alloc.h
index f643e0a..1758f26 100644
--- a/shim/allocator_shim_default_dispatch_to_partition_alloc.h
+++ b/shim/allocator_shim_default_dispatch_to_partition_alloc.h
@@ -6,63 +6,68 @@
 #define BASE_ALLOCATOR_PARTITION_ALLOCATOR_SHIM_ALLOCATOR_SHIM_DEFAULT_DISPATCH_TO_PARTITION_ALLOC_H_
 
 #include "base/allocator/partition_allocator/partition_alloc.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "base/allocator/partition_allocator/shim/allocator_shim.h"
-#include "base/base_export.h"
 
 namespace allocator_shim::internal {
 
 void PartitionAllocSetCallNewHandlerOnMallocFailure(bool value);
 
-class BASE_EXPORT PartitionAllocMalloc {
+class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PartitionAllocMalloc {
  public:
-  static partition_alloc::ThreadSafePartitionRoot* Allocator();
+  static partition_alloc::PartitionRoot* Allocator();
   // May return |nullptr|, will never return the same pointer as |Allocator()|.
-  static partition_alloc::ThreadSafePartitionRoot* OriginalAllocator();
+  static partition_alloc::PartitionRoot* OriginalAllocator();
   // May return the same pointer as |Allocator()|.
-  static partition_alloc::ThreadSafePartitionRoot* AlignedAllocator();
+  static partition_alloc::PartitionRoot* AlignedAllocator();
 };
 
-BASE_EXPORT void* PartitionMalloc(const AllocatorDispatch*,
-                                  size_t size,
-                                  void* context);
-
-BASE_EXPORT void* PartitionMallocUnchecked(const AllocatorDispatch*,
-                                           size_t size,
-                                           void* context);
-
-BASE_EXPORT void* PartitionCalloc(const AllocatorDispatch*,
-                                  size_t n,
-                                  size_t size,
-                                  void* context);
-
-BASE_EXPORT void* PartitionMemalign(const AllocatorDispatch*,
-                                    size_t alignment,
-                                    size_t size,
-                                    void* context);
-
-BASE_EXPORT void* PartitionAlignedAlloc(const AllocatorDispatch* dispatch,
-                                        size_t size,
-                                        size_t alignment,
-                                        void* context);
-
-BASE_EXPORT void* PartitionAlignedRealloc(const AllocatorDispatch* dispatch,
-                                          void* address,
-                                          size_t size,
-                                          size_t alignment,
-                                          void* context);
-
-BASE_EXPORT void* PartitionRealloc(const AllocatorDispatch*,
-                                   void* address,
-                                   size_t size,
-                                   void* context);
-
-BASE_EXPORT void PartitionFree(const AllocatorDispatch*,
-                               void* object,
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionMalloc(const AllocatorDispatch*, size_t size, void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionMallocUnchecked(const AllocatorDispatch*,
+                               size_t size,
                                void* context);
 
-BASE_EXPORT size_t PartitionGetSizeEstimate(const AllocatorDispatch*,
-                                            void* address,
-                                            void* context);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionCalloc(const AllocatorDispatch*,
+                      size_t n,
+                      size_t size,
+                      void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionMemalign(const AllocatorDispatch*,
+                        size_t alignment,
+                        size_t size,
+                        void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionAlignedAlloc(const AllocatorDispatch* dispatch,
+                            size_t size,
+                            size_t alignment,
+                            void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionAlignedRealloc(const AllocatorDispatch* dispatch,
+                              void* address,
+                              size_t size,
+                              size_t alignment,
+                              void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* PartitionRealloc(const AllocatorDispatch*,
+                       void* address,
+                       size_t size,
+                       void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void PartitionFree(const AllocatorDispatch*, void* object, void* context);
+
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+size_t PartitionGetSizeEstimate(const AllocatorDispatch*,
+                                void* address,
+                                void* context);
 
 }  // namespace allocator_shim::internal
 
diff --git a/shim/allocator_shim_default_dispatch_to_partition_alloc_unittest.cc b/shim/allocator_shim_default_dispatch_to_partition_alloc_unittest.cc
index 9ad49f2..bf00055 100644
--- a/shim/allocator_shim_default_dispatch_to_partition_alloc_unittest.cc
+++ b/shim/allocator_shim_default_dispatch_to_partition_alloc_unittest.cc
@@ -175,14 +175,14 @@ TEST(PartitionAllocAsMalloc, Realloc) {
 // crbug.com/1141752
 TEST(PartitionAllocAsMalloc, Alignment) {
   EXPECT_EQ(0u, reinterpret_cast<uintptr_t>(PartitionAllocMalloc::Allocator()) %
-                    alignof(partition_alloc::ThreadSafePartitionRoot));
+                    alignof(partition_alloc::PartitionRoot));
   // This works fine even if nullptr is returned.
   EXPECT_EQ(0u, reinterpret_cast<uintptr_t>(
                     PartitionAllocMalloc::OriginalAllocator()) %
-                    alignof(partition_alloc::ThreadSafePartitionRoot));
+                    alignof(partition_alloc::PartitionRoot));
   EXPECT_EQ(0u, reinterpret_cast<uintptr_t>(
                     PartitionAllocMalloc::AlignedAllocator()) %
-                    alignof(partition_alloc::ThreadSafePartitionRoot));
+                    alignof(partition_alloc::PartitionRoot));
 }
 
 // crbug.com/1297945
diff --git a/shim/allocator_shim_override_libc_symbols.h b/shim/allocator_shim_override_libc_symbols.h
index bb07170..d471281 100644
--- a/shim/allocator_shim_override_libc_symbols.h
+++ b/shim/allocator_shim_override_libc_symbols.h
@@ -24,6 +24,18 @@
 
 extern "C" {
 
+void* __libc_malloc(size_t size);
+void* __libc_calloc(size_t n, size_t size);
+void* __libc_realloc(void* address, size_t size);
+void* __libc_memalign(size_t alignment, size_t size);
+void __libc_free(void* ptr);
+
+void* __real_malloc(size_t size);
+void* __real_calloc(size_t n, size_t size);
+void* __real_realloc(void* address, size_t size);
+void* __real_memalign(size_t alignment, size_t size);
+void __real_free(void* ptr);
+
 // WARNING: Whenever a new function is added there (which, surprisingly enough,
 // happens. For instance glibc 2.33 introduced mallinfo2(), which we don't
 // support... yet?), it MUST be added to build/linux/chrome.map.
@@ -78,7 +90,8 @@ SHIM_ALWAYS_EXPORT size_t malloc_size(const void* address) __THROW {
 }
 
 SHIM_ALWAYS_EXPORT size_t malloc_usable_size(void* address) __THROW {
-  return ShimGetSizeEstimate(address, nullptr);
+  size_t tmp = ShimGetSizeEstimate(address, nullptr);
+  return tmp;
 }
 
 // The default dispatch translation unit has to define also the following
diff --git a/shim/malloc_zone_functions_mac.h b/shim/malloc_zone_functions_mac.h
index 1f8c179..05c331b 100644
--- a/shim/malloc_zone_functions_mac.h
+++ b/shim/malloc_zone_functions_mac.h
@@ -8,9 +8,9 @@
 #include <malloc/malloc.h>
 #include <stddef.h>
 
+#include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/immediate_crash.h"
 #include "base/allocator/partition_allocator/third_party/apple_apsl/malloc.h"
-#include "base/base_export.h"
 
 namespace allocator_shim {
 
@@ -57,10 +57,12 @@ struct MallocZoneFunctions {
   const ChromeMallocZone* context;
 };
 
-BASE_EXPORT void StoreZoneFunctions(const ChromeMallocZone* zone,
-                                    MallocZoneFunctions* functions);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void StoreZoneFunctions(const ChromeMallocZone* zone,
+                        MallocZoneFunctions* functions);
 static constexpr int kMaxZoneCount = 30;
-BASE_EXPORT extern MallocZoneFunctions g_malloc_zones[kMaxZoneCount];
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+extern MallocZoneFunctions g_malloc_zones[kMaxZoneCount];
 
 // The array g_malloc_zones stores all information about malloc zones before
 // they are shimmed. This information needs to be accessed during dispatch back
@@ -84,14 +86,16 @@ BASE_EXPORT extern MallocZoneFunctions g_malloc_zones[kMaxZoneCount];
 // default allocator is stored as the first MallocZoneFunctions.
 //
 // Returns whether the zone was successfully stored.
-BASE_EXPORT bool StoreMallocZone(ChromeMallocZone* zone);
-BASE_EXPORT bool IsMallocZoneAlreadyStored(ChromeMallocZone* zone);
-BASE_EXPORT bool DoesMallocZoneNeedReplacing(
-    ChromeMallocZone* zone,
-    const MallocZoneFunctions* functions);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+bool StoreMallocZone(ChromeMallocZone* zone);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+bool IsMallocZoneAlreadyStored(ChromeMallocZone* zone);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+bool DoesMallocZoneNeedReplacing(ChromeMallocZone* zone,
+                                 const MallocZoneFunctions* functions);
 
-BASE_EXPORT int GetMallocZoneCountForTesting();
-BASE_EXPORT void ClearAllMallocZonesForTesting();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) int GetMallocZoneCountForTesting();
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void ClearAllMallocZonesForTesting();
 
 inline MallocZoneFunctions& GetFunctionsForZone(void* zone) {
   for (unsigned int i = 0; i < kMaxZoneCount; ++i) {
diff --git a/shim/nonscannable_allocator.cc b/shim/nonscannable_allocator.cc
index 1ca730a..2489f69 100644
--- a/shim/nonscannable_allocator.cc
+++ b/shim/nonscannable_allocator.cc
@@ -35,6 +35,9 @@ NonScannableAllocatorImpl<quarantinable>::Instance() {
 
 template <bool quarantinable>
 void* NonScannableAllocatorImpl<quarantinable>::Alloc(size_t size) {
+
+
+
 #if BUILDFLAG(USE_STARSCAN)
   // TODO(bikineev): Change to LIKELY once PCScan is enabled by default.
   if (PA_UNLIKELY(pcscan_enabled_.load(std::memory_order_acquire))) {
@@ -50,7 +53,7 @@ void* NonScannableAllocatorImpl<quarantinable>::Alloc(size_t size) {
 
 template <bool quarantinable>
 void NonScannableAllocatorImpl<quarantinable>::Free(void* ptr) {
-  partition_alloc::ThreadSafePartitionRoot::FreeNoHooks(ptr);
+  partition_alloc::PartitionRoot::FreeNoHooks(ptr);
 }
 
 template <bool quarantinable>
diff --git a/shim/nonscannable_allocator.h b/shim/nonscannable_allocator.h
index 869bcb5..1390391 100644
--- a/shim/nonscannable_allocator.h
+++ b/shim/nonscannable_allocator.h
@@ -44,7 +44,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) NonScannableAllocatorImpl final {
 
   // Returns PartitionRoot corresponding to the allocator, or nullptr if the
   // allocator is not enabled.
-  partition_alloc::ThreadSafePartitionRoot* root() {
+  partition_alloc::PartitionRoot* root() {
 #if BUILDFLAG(USE_STARSCAN)
     if (!allocator_.get()) {
       return nullptr;
diff --git a/shim/winheap_stubs_win.h b/shim/winheap_stubs_win.h
index eebefa2..acbb00c 100644
--- a/shim/winheap_stubs_win.h
+++ b/shim/winheap_stubs_win.h
@@ -11,7 +11,7 @@
 
 #include <stdint.h>
 
-#include "base/base_export.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 
 namespace allocator_shim {
 
@@ -35,11 +35,11 @@ bool WinCallNewHandler(size_t size);
 
 // Wrappers to implement the interface for the _aligned_* functions on top of
 // the CRT's Windows heap. Exported for tests.
-BASE_EXPORT void* WinHeapAlignedMalloc(size_t size, size_t alignment);
-BASE_EXPORT void* WinHeapAlignedRealloc(void* ptr,
-                                        size_t size,
-                                        size_t alignment);
-BASE_EXPORT void WinHeapAlignedFree(void* ptr);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* WinHeapAlignedMalloc(size_t size, size_t alignment);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC)
+void* WinHeapAlignedRealloc(void* ptr, size_t size, size_t alignment);
+PA_COMPONENT_EXPORT(PARTITION_ALLOC) void WinHeapAlignedFree(void* ptr);
 
 }  // namespace allocator_shim
 
diff --git a/starscan/metadata_allocator.cc b/starscan/metadata_allocator.cc
index bb1bd7d..d73a3b5 100644
--- a/starscan/metadata_allocator.cc
+++ b/starscan/metadata_allocator.cc
@@ -16,9 +16,8 @@ constexpr PartitionOptions kConfig{};
 }  // namespace
 
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
-ThreadSafePartitionRoot& PCScanMetadataAllocator() {
-  static internal::base::NoDestructor<ThreadSafePartitionRoot> allocator(
-      kConfig);
+PartitionRoot& PCScanMetadataAllocator() {
+  static internal::base::NoDestructor<PartitionRoot> allocator(kConfig);
   return *allocator;
 }
 
diff --git a/starscan/metadata_allocator.h b/starscan/metadata_allocator.h
index abb1069..2270838 100644
--- a/starscan/metadata_allocator.h
+++ b/starscan/metadata_allocator.h
@@ -14,7 +14,7 @@
 namespace partition_alloc::internal {
 
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
-ThreadSafePartitionRoot& PCScanMetadataAllocator();
+PartitionRoot& PCScanMetadataAllocator();
 void ReinitPCScanMetadataAllocatorForTesting();
 
 // STL allocator which is needed to keep internal data structures required by
diff --git a/starscan/pcscan_internal.cc b/starscan/pcscan_internal.cc
index 9d31abd..000c398 100644
--- a/starscan/pcscan_internal.cc
+++ b/starscan/pcscan_internal.cc
@@ -961,12 +961,12 @@ void UnmarkInCardTable(uintptr_t slot_start, SlotSpanMetadata* slot_span) {
   return slot_span->bucket->slot_size;
 }
 
-[[maybe_unused]] void SweepSuperPage(ThreadSafePartitionRoot* root,
+[[maybe_unused]] void SweepSuperPage(PartitionRoot* root,
                                      uintptr_t super_page,
                                      size_t epoch,
                                      SweepStat& stat) {
   auto* bitmap = StateBitmapFromAddr(super_page);
-  ThreadSafePartitionRoot::FromFirstSuperPage(super_page);
+  PartitionRoot::FromFirstSuperPage(super_page);
   bitmap->IterateUnmarkedQuarantined(epoch, [root,
                                              &stat](uintptr_t slot_start) {
     auto* slot_span = SlotSpanMetadata::FromSlotStart(slot_start);
@@ -975,7 +975,7 @@ void UnmarkInCardTable(uintptr_t slot_start, SlotSpanMetadata* slot_span) {
 }
 
 [[maybe_unused]] void SweepSuperPageAndDiscardMarkedQuarantine(
-    ThreadSafePartitionRoot* root,
+    PartitionRoot* root,
     uintptr_t super_page,
     size_t epoch,
     SweepStat& stat) {
@@ -1005,11 +1005,10 @@ void UnmarkInCardTable(uintptr_t slot_start, SlotSpanMetadata* slot_span) {
   });
 }
 
-[[maybe_unused]] void SweepSuperPageWithBatchedFree(
-    ThreadSafePartitionRoot* root,
-    uintptr_t super_page,
-    size_t epoch,
-    SweepStat& stat) {
+[[maybe_unused]] void SweepSuperPageWithBatchedFree(PartitionRoot* root,
+                                                    uintptr_t super_page,
+                                                    size_t epoch,
+                                                    SweepStat& stat) {
   using SlotSpan = SlotSpanMetadata;
 
   auto* bitmap = StateBitmapFromAddr(super_page);
@@ -1017,7 +1016,9 @@ void UnmarkInCardTable(uintptr_t slot_start, SlotSpanMetadata* slot_span) {
   internal::PartitionFreelistEntry* freelist_tail = nullptr;
   internal::PartitionFreelistEntry* freelist_head = nullptr;
   size_t freelist_entries = 0;
-
+#if BUILDFLAG(TMEMK_ENABLE)
+  XXX unsupported. we dont have a tagged slot start here, so we cannot/shouldnt emplace it. or maybe we do if this is only slots that have the (respective) overflow tag.
+#endif
   const auto bitmap_iterator = [&](uintptr_t slot_start) {
     SlotSpan* current_slot_span = SlotSpan::FromSlotStart(slot_start);
     auto* entry = PartitionFreelistEntry::EmplaceAndInitNull(slot_start);
@@ -1071,17 +1072,18 @@ void PCScanTask::SweepQuarantine() {
   StarScanSnapshot::SweepingView sweeping_view(*snapshot_);
   sweeping_view.VisitNonConcurrently(
       [this, &stat, should_discard](uintptr_t super_page) {
-        auto* root = ThreadSafePartitionRoot::FromFirstSuperPage(super_page);
+        auto* root = PartitionRoot::FromFirstSuperPage(super_page);
 
 #if PA_CONFIG(STARSCAN_BATCHED_FREE)
         SweepSuperPageWithBatchedFree(root, super_page, pcscan_epoch_, stat);
         (void)should_discard;
 #else
-        if (PA_UNLIKELY(should_discard && !root->settings.allow_cookie))
+        if (PA_UNLIKELY(should_discard && !root->settings.use_cookie)) {
           SweepSuperPageAndDiscardMarkedQuarantine(root, super_page,
                                                    pcscan_epoch_, stat);
-        else
+        } else {
           SweepSuperPage(root, super_page, pcscan_epoch_, stat);
+        }
 #endif  // PA_CONFIG(STARSCAN_BATCHED_FREE)
       });
 
diff --git a/starscan/pcscan_scheduling.h b/starscan/pcscan_scheduling.h
index 1c6a2a8..f5466a4 100644
--- a/starscan/pcscan_scheduling.h
+++ b/starscan/pcscan_scheduling.h
@@ -33,11 +33,19 @@ struct QuarantineData final {
   std::atomic<size_t> epoch{0u};
 };
 
+// No virtual destructor to allow constant initialization of PCScan as
+// static global which directly embeds LimitBackend as default backend.
+#if defined(__clang__)
+#pragma clang diagnostic push
+#pragma clang diagnostic ignored "-Wnon-virtual-dtor"
+#endif
 class PA_COMPONENT_EXPORT(PARTITION_ALLOC) PCScanSchedulingBackend {
+#if defined(__clang__)
+#pragma clang diagnostic pop
+#endif
+
  public:
   inline constexpr explicit PCScanSchedulingBackend(PCScanScheduler&);
-  // No virtual destructor to allow constant initialization of PCScan as
-  // static global which directly embeds LimitBackend as default backend.
 
   PCScanSchedulingBackend(const PCScanSchedulingBackend&) = delete;
   PCScanSchedulingBackend& operator=(const PCScanSchedulingBackend&) = delete;
diff --git a/starscan/pcscan_unittest.cc b/starscan/pcscan_unittest.cc
index be000c8..06a2b03 100644
--- a/starscan/pcscan_unittest.cc
+++ b/starscan/pcscan_unittest.cc
@@ -97,8 +97,8 @@ class PartitionAllocPCScanTestBase : public testing::Test {
     return StateBitmapFromAddr(slot_start)->IsQuarantined(slot_start);
   }
 
-  ThreadSafePartitionRoot& root() { return *allocator_.root(); }
-  const ThreadSafePartitionRoot& root() const { return *allocator_.root(); }
+  PartitionRoot& root() { return *allocator_.root(); }
+  const PartitionRoot& root() const { return *allocator_.root(); }
 
  private:
   // Leverage the already-templated version outside `internal::`.
@@ -119,7 +119,7 @@ class PartitionAllocPCScanTest : public PartitionAllocPCScanTestBase {
   }
 };
 
-using SlotSpan = ThreadSafePartitionRoot::SlotSpan;
+using SlotSpan = PartitionRoot::SlotSpan;
 
 struct FullSlotSpanAllocation {
   SlotSpan* slot_span;
@@ -128,14 +128,14 @@ struct FullSlotSpanAllocation {
 };
 
 // Assumes heap is purged.
-FullSlotSpanAllocation GetFullSlotSpan(ThreadSafePartitionRoot& root,
+FullSlotSpanAllocation GetFullSlotSpan(PartitionRoot& root,
                                        size_t object_size) {
   PA_CHECK(0u == root.get_total_size_of_committed_pages());
 
   const size_t raw_size = root.AdjustSizeForExtrasAdd(object_size);
   const size_t bucket_index =
       root.SizeToBucketIndex(raw_size, root.GetBucketDistribution());
-  ThreadSafePartitionRoot::Bucket& bucket = root.buckets[bucket_index];
+  PartitionRoot::Bucket& bucket = root.buckets[bucket_index];
   const size_t num_slots = (bucket.get_bytes_per_span()) / bucket.slot_size;
 
   uintptr_t first = 0;
@@ -190,7 +190,7 @@ template <size_t Size, size_t Alignment = 0>
 struct List final : ListBase {
   char buffer[Size];
 
-  static List* Create(ThreadSafePartitionRoot& root, ListBase* next = nullptr) {
+  static List* Create(PartitionRoot& root, ListBase* next = nullptr) {
     List* list;
     if (Alignment) {
       list = static_cast<List*>(
@@ -202,9 +202,7 @@ struct List final : ListBase {
     return list;
   }
 
-  static void Destroy(ThreadSafePartitionRoot& root, List* list) {
-    root.Free(list);
-  }
+  static void Destroy(PartitionRoot& root, List* list) { root.Free(list); }
 };
 
 TEST_F(PartitionAllocPCScanTest, ArbitraryObjectInQuarantine) {
@@ -246,7 +244,7 @@ template <typename SourceList, typename ValueList>
 void TestDanglingReference(PartitionAllocPCScanTest& test,
                            SourceList* source,
                            ValueList* value,
-                           ThreadSafePartitionRoot& value_root) {
+                           PartitionRoot& value_root) {
   {
     // Free |value| and leave the dangling reference in |source|.
     ValueList::Destroy(value_root, value);
@@ -272,7 +270,7 @@ void TestDanglingReference(PartitionAllocPCScanTest& test,
 
 void TestDanglingReferenceNotVisited(PartitionAllocPCScanTest& test,
                                      void* value,
-                                     ThreadSafePartitionRoot& value_root) {
+                                     PartitionRoot& value_root) {
   value_root.Free(value);
   // Check that |value| is in the quarantine now.
   EXPECT_TRUE(test.IsInQuarantine(value));
@@ -395,9 +393,8 @@ TEST_F(PartitionAllocPCScanTest, DanglingReferenceFromFullPage) {
   // Assert that the first and the last objects are in different slot spans but
   // in the same bucket.
   SlotSpan* source_slot_span =
-      ThreadSafePartitionRoot::SlotSpan::FromObject(source_buffer);
-  SlotSpan* value_slot_span =
-      ThreadSafePartitionRoot::SlotSpan::FromObject(value_buffer);
+      PartitionRoot::SlotSpan::FromObject(source_buffer);
+  SlotSpan* value_slot_span = PartitionRoot::SlotSpan::FromObject(value_buffer);
   ASSERT_NE(source_slot_span, value_slot_span);
   ASSERT_EQ(source_slot_span->bucket, value_slot_span->bucket);
 
@@ -416,14 +413,13 @@ struct ListWithInnerReference {
   char* volatile next = nullptr;
   char buffer2[Size];
 
-  static ListWithInnerReference* Create(ThreadSafePartitionRoot& root) {
+  static ListWithInnerReference* Create(PartitionRoot& root) {
     auto* list = static_cast<ListWithInnerReference*>(
         root.Alloc(sizeof(ListWithInnerReference), nullptr));
     return list;
   }
 
-  static void Destroy(ThreadSafePartitionRoot& root,
-                      ListWithInnerReference* list) {
+  static void Destroy(PartitionRoot& root, ListWithInnerReference* list) {
     root.Free(list);
   }
 };
@@ -463,11 +459,11 @@ TEST_F(PartitionAllocPCScanTest, DanglingInterPartitionReference) {
   using SourceList = List<64>;
   using ValueList = SourceList;
 
-  ThreadSafePartitionRoot source_root(PartitionOptions{
+  PartitionRoot source_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   source_root.UncapEmptySlotSpanMemoryForTesting();
-  ThreadSafePartitionRoot value_root(PartitionOptions{
+  PartitionRoot value_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   value_root.UncapEmptySlotSpanMemoryForTesting();
@@ -488,11 +484,11 @@ TEST_F(PartitionAllocPCScanTest, DanglingReferenceToNonScannablePartition) {
   using SourceList = List<64>;
   using ValueList = SourceList;
 
-  ThreadSafePartitionRoot source_root(PartitionOptions{
+  PartitionRoot source_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   source_root.UncapEmptySlotSpanMemoryForTesting();
-  ThreadSafePartitionRoot value_root(PartitionOptions{
+  PartitionRoot value_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   value_root.UncapEmptySlotSpanMemoryForTesting();
@@ -513,11 +509,11 @@ TEST_F(PartitionAllocPCScanTest, DanglingReferenceFromNonScannablePartition) {
   using SourceList = List<64>;
   using ValueList = SourceList;
 
-  ThreadSafePartitionRoot source_root(PartitionOptions{
+  PartitionRoot source_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   source_root.UncapEmptySlotSpanMemoryForTesting();
-  ThreadSafePartitionRoot value_root(PartitionOptions{
+  PartitionRoot value_root(PartitionOptions{
       .quarantine = PartitionOptions::Quarantine::kAllowed,
   });
   value_root.UncapEmptySlotSpanMemoryForTesting();
@@ -549,7 +545,7 @@ template <typename SourceList, typename ValueList>
 void TestDanglingReferenceWithSafepoint(PartitionAllocPCScanTest& test,
                                         SourceList* source,
                                         ValueList* value,
-                                        ThreadSafePartitionRoot& value_root) {
+                                        PartitionRoot& value_root) {
   {
     // Free |value| and leave the dangling reference in |source|.
     ValueList::Destroy(value_root, value);
diff --git a/tagging.cc b/tagging.cc
index 6f0d49d..719e361 100644
--- a/tagging.cc
+++ b/tagging.cc
@@ -9,8 +9,11 @@
 #include "base/allocator/partition_allocator/partition_alloc_check.h"
 #include "base/allocator/partition_allocator/partition_alloc_config.h"
 #include "build/build_config.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
+#include "base/allocator/partition_allocator/partition_superpage_extent_entry.h"
+#include "base/allocator/partition_allocator/partition_page.h"
 
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_ENABLE)
 #include <arm_acle.h>
 #include <asm/hwcap.h>
 #include <sys/auxv.h>
@@ -37,17 +40,38 @@
 #define PR_MTE_TCF_MASK (3UL << PR_MTE_TCF_SHIFT)
 #define PR_MTE_TAG_SHIFT 3
 #define PR_MTE_TAG_MASK (0xffffUL << PR_MTE_TAG_SHIFT)
+#define HWCAP2_MTE (1 << 18)
 #endif
-#endif
+#endif /* PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_ENABLE) */
 
 #if BUILDFLAG(IS_ANDROID)
 #include "base/allocator/partition_allocator/partition_alloc_base/files/file_path.h"
 #include "base/allocator/partition_allocator/partition_alloc_base/native_library.h"
 #endif  // BUILDFLAG(IS_ANDROID)
 
+
+
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE
+#endif
+#include <unistd.h>
+#include <sys/syscall.h>   /* For SYS_xxx definitions */
+// #include <pthread.h> /*for pthread_mutex */
+// #include <emmintrin.h> /* fpr __m128i, _mm_stream_si128, _mm_mfence  */
+// #include <sys/mman.h> /* msync */
+#include <linux/perf_event.h> /* perf */
+#include <sys/ioctl.h> /* perf */
+#include <stdio.h>
+#include <string.h>
+#include <sys/mman.h> /* PROT_* */
+#include <fcntl.h>     // fallocate
+#include <linux/memfd.h>
+#include <errno.h>
+#include <ctype.h>
+
 namespace partition_alloc {
 
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_ENABLE)
 namespace {
 void ChangeMemoryTaggingModeInternal(unsigned prctl_mask) {
   if (internal::base::CPU::GetInstanceNoAllocation().has_mte()) {
@@ -59,7 +83,7 @@ void ChangeMemoryTaggingModeInternal(unsigned prctl_mask) {
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
 
 void ChangeMemoryTaggingModeForCurrentThread(TagViolationReportingMode m) {
-#if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && !BUILDFLAG(TMEMK_ENABLE)
   if (m == TagViolationReportingMode::kSynchronous) {
     ChangeMemoryTaggingModeInternal(PR_TAGGED_ADDR_ENABLE | PR_MTE_TCF_SYNC |
                                     (0xfffe << PR_MTE_TAG_SHIFT));
@@ -74,6 +98,220 @@ void ChangeMemoryTaggingModeForCurrentThread(TagViolationReportingMode m) {
 
 namespace internal {
 
+#if BUILDFLAG(TMEMK_ENABLE)
+// replacement for keyutils:
+#define KEY_SPEC_THREAD_KEYRING		-1	/* - key ID for thread-specific keyring */
+#define KEY_SPEC_PROCESS_KEYRING	-2	/* - key ID for process-specific keyring */
+#define KEY_SPEC_SESSION_KEYRING	-3	/* - key ID for session-specific keyring */
+#define KEY_SPEC_USER_KEYRING		-4	/* - key ID for UID-specific keyring */
+#define KEY_SPEC_USER_SESSION_KEYRING	-5	/* - key ID for UID-session keyring */
+#define KEY_SPEC_GROUP_KEYRING		-6	/* - key ID for GID-specific keyring */
+#define KEY_SPEC_REQKEY_AUTH_KEY	-7	/* - key ID for assumed request_key auth key */
+
+#if BUILDFLAG(TMEMK_ENCRYPTION)
+static inline int32_t tmemk_add_key(const char *type,
+			    const char *description,
+			    const void *payload,
+			    size_t plen,
+			    int32_t ringid) {
+	return syscall(__NR_add_key, type, description, payload, plen, ringid);
+}
+static int encrypt_mprotect(void* addr, size_t len, int prot, int32_t serial){
+  static const int SYS_encrypt_mprotect = 451;
+  int ret = syscall(SYS_encrypt_mprotect, addr, len, prot, serial);
+  return ret;
+}
+static int32_t _tmemk_key_handles[TMEMK_KEY_OVERFLOW+1] = {0, };
+#endif /*BUILDFLAG(TMEMK_ENCRYPTION)*/
+
+#if BUILDFLAG(TMEMK_THREAD_ISOLATION)
+thread_local uint16_t g_thread_key = 0;
+std::atomic<uint64_t> g_thread_key_last{0};
+#endif // BUILDFLAG(TMEMK_THREAD_ISOLATION)
+
+static int _mktme_fd = 0;
+int tmemk_get_memfd(){
+  //Assumption: first time this is called is for regular pool
+  static int counter = 0;
+  counter++;
+  if(counter != 1){
+    return -1;
+  }
+  return _mktme_fd;
+}
+#else /* BUILDFLAG(TMEMK_ENABLE) */
+int tmemk_get_memfd(){ return -1; }
+#endif /* BUILDFLAG(TMEMK_ENABLE) */
+
+#if BUILDFLAG(TMEMK_ENABLE)
+static int _nullfd = -1;
+#endif
+void tmemk_create_aliases_for_memory(uintptr_t address, size_t size, int access_flag, int map_flags, int fd, size_t offset, int commit){
+    // __debug("tmemk_create_aliases_for_memory address %p size 0x%lx access_flag %d map_flags %d fd %d offset 0x%lx commit %d", address, size, access_flag, map_flags, fd, offset, commit);
+#if BUILDFLAG(TMEMK_ENABLE)
+  if(_nullfd == -1){
+    _nullfd = open("/dev/random", O_WRONLY);
+  }
+
+#if BUILDFLAG(TMEMK_NOALIAS)
+  if(commit){
+    return;
+  }else{
+    [[maybe_unused]] void* ret = mmap((void*)address, size, PROT_NONE,
+                     MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
+  }
+#endif /*BUILDFLAG(TMEMK_NOALIAS)*/
+
+  if(!commit){
+    for (size_t key_num = 0; key_num <= TMEMK_KEY_OVERFLOW; key_num++) {
+      uintptr_t p1 = (uintptr_t)tmemk_add_tag_to_ptr((void*)address, key_num);
+      if(key_num == 0 && write(_nullfd, (void*)p1, 8) >= 0){
+        //page exists, let's zero it just in case
+
+#if BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+        internal::MemZeroWithMovdir64B_q(p1, size);
+#endif // BUILDFLAG(TMEMK_MEMZERO_BEFORE_DECOMMIT)
+      }
+      if(key_num){ //always unmap keyid0, but check for others
+        if (write(_nullfd, (void*)p1, 8) < 0){
+          // __debug("tmemk_create_aliases_for_memory: alias dont exist. skipping removing them");
+          break;
+        }
+      }
+
+      uintptr_t p2 = (uintptr_t)mmap((void*)p1, size, PROT_NONE, MAP_FIXED | MAP_SHARED | MAP_NORESERVE, _mktme_fd, offset);
+      PA_CHECK(p1 == p2);
+      // uintptr_t p2 = (uintptr_t)munmap((void*)p1, size);
+      // PA_CHECK(p1 == p2);
+    }
+    fallocate(_mktme_fd, FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE, offset, size);
+  }else{
+    PA_CHECK(access_flag == (PROT_READ | PROT_WRITE));
+    // __debug("tmemk_create_aliases_for_memory(%p, size=0x%lx, offset=0x%lx)", address, size, offset);
+    fallocate(_mktme_fd, FALLOC_FL_KEEP_SIZE, offset, size);
+    for (size_t key_num = 1; key_num <= TMEMK_KEY_OVERFLOW; key_num++) {
+      uintptr_t p1 = (uintptr_t)tmemk_add_tag_to_ptr((void*)address, key_num);
+
+      int ret_test = mprotect((void*)p1, size, PROT_READ | PROT_WRITE);
+      if(ret_test == 0){
+
+        return;
+      }
+
+      // __debug("tmemk_create_aliases_for_memory key_num=%d mapping to %p", key_num, p1);
+      // __debug("mapping(%p, size=%zu, key_num=%d)", p1, size, key_num);
+      [[maybe_unused]] uintptr_t p2 = (uintptr_t)mmap((void*)p1, size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_FIXED, _mktme_fd, offset);
+      if(p1 != p2){
+        __debug_always("errno %d", errno);
+        __debug_always("tmemk_create_aliases_for_memory key_num=%d mapping to %p", key_num, p1);
+        __debug_always("p1 = %p p2 = %p", p1, p2);
+      }
+      PA_CHECK(p1 == p2);
+      PA_CHECK(p2 != (uintptr_t)MAP_FAILED);
+      PA_CHECK(key_num != 0);
+#if BUILDFLAG(TMEMK_ENCRYPTION)
+      PA_CHECK(_tmemk_key_handles[key_num] != 0 && _tmemk_key_handles[key_num] != -1);
+      int32_t key_handle = _tmemk_key_handles[key_num];
+      int ret = encrypt_mprotect((void*)p2, size, PROT_READ | PROT_WRITE, key_handle);
+      if (ret != 0) {
+        __debug("ERROR encrypt_mprotect failed");
+      }
+#endif /*BUILDFLAG(TMEMK_ENCRYPTION)*/
+    }
+  }
+#endif
+}
+
+void tmemk_allocate_keys(size_t length){
+#if BUILDFLAG(TMEMK_ENABLE)
+  static bool _keys_initialized = 0;
+  if(_keys_initialized){
+    return;
+  }
+  _keys_initialized = true;
+
+#if BUILDFLAG(TMEMK_ENCRYPTION)
+  #if BUILDFLAG(TMEMK_INTEGRITY)
+    const char * options  = "algorithm=aes-xts-128-i type=cpu";
+  #else
+    const char * options  = "algorithm=aes-xts-128 type=cpu";
+  #endif
+
+  int fallback = 0;
+  static const char * fname = "/tmp/tmemk_keys";
+  int fd = syscall(__NR_open, fname, O_RDONLY, 0);
+  struct stat st = {0,};
+  [[maybe_unused]] int r1 = syscall(__NR_stat, fname, &st);
+  if(fd && st.st_size != 0){
+    PA_CHECK(st.st_size);
+    char file_buf[st.st_size];
+    char * file_buf_ptr = file_buf;
+    [[maybe_unused]] int r2 = syscall(__NR_read, fd, file_buf, st.st_size);
+    size_t key_num = 1;
+    do{
+      while(!isdigit((int)*file_buf_ptr) && file_buf_ptr < file_buf + st.st_size){file_buf_ptr++;};
+      if(file_buf_ptr >= file_buf + st.st_size){
+        break;
+      }
+      char* endptr;
+      long int key_handle = strtol(file_buf_ptr, &endptr, 10);
+      _tmemk_key_handles[key_num] = (int32_t)key_handle;
+      // __debug("_tmemk_key_handles[%d] = %d\n", key_num, _tmemk_key_handles[key_num]);
+      key_num++;
+      file_buf_ptr = endptr;
+    }while(file_buf_ptr < file_buf + st.st_size && key_num <= TMEMK_KEY_OVERFLOW);
+  };
+  syscall(__NR_close, fd);
+  for (size_t key_num = 1; key_num <= TMEMK_KEY_OVERFLOW; key_num++) {
+    if(_tmemk_key_handles[key_num] == -1 || _tmemk_key_handles[key_num] == 0){
+      fallback = 1;
+    }
+  }
+  // PA_CHECK(0);
+  PA_CHECK(fallback == 0);
+  if(fallback){
+    for (size_t key_num = 1; key_num <= TMEMK_KEY_OVERFLOW; key_num++) {
+      char description[256];
+      //NOTE: key descriptions absolutely have to be unique
+      memset(description, 0, sizeof(description));
+      strcat(description, "pa-tmemk-key_");
+      sprintf(description+strlen(description), "%zu", key_num);
+      int32_t key_handle = tmemk_add_key("mktme", description, options, strlen(options), KEY_SPEC_PROCESS_KEYRING);
+      if (key_handle == -1) {
+        __debug("add_key failed");
+        exit(1);
+      }
+      _tmemk_key_handles[key_num] = key_handle;
+      // __debug("_tmemk_key_handles[%d] = %d", key_num, key_handle);
+    }
+  }
+#endif /*BUILDFLAG(TMEMK_ENCRYPTION)*/
+
+
+  // __debug("allocating memfd");
+  int flags = MFD_CLOEXEC;
+// #if defined(MKTME_HUGE_2M)
+//   flags |= (MFD_HUGETLB | MFD_HUGE_2MB);
+// #elif defined(MKTME_HUGE_1G)
+//   flags |= (MFD_HUGETLB | MFD_HUGE_1GB);
+// #endif
+  int memfd = memfd_create("tmemk_heap", flags);
+  if(memfd == -1){
+    __debug("memfd_create failed");
+    exit(1);
+  }
+  PA_CHECK(length == partition_alloc::internal::kPoolMaxSize);
+  int err = ftruncate(memfd, length);
+  if(err != 0){
+    __debug("ftruncate failed");
+    exit(1);
+  }
+  _mktme_fd = memfd;
+
+
+#endif
+}
+
 #if BUILDFLAG(IS_ANDROID)
 void ChangeMemoryTaggingModeForAllThreadsPerProcess(
     TagViolationReportingMode m) {
@@ -117,11 +355,223 @@ namespace {
   // Check that ptr and size are correct for MTE
   uintptr_t ptr_as_uint = reinterpret_cast<uintptr_t>(ptr);
   bool ret = (ptr_as_uint % kMemTagGranuleSize == 0) &&
-             (sz % kMemTagGranuleSize == 0) && sz;
+             (sz % kMemTagGranuleSize == 0) && sz &&
+             (sz <= kMaxMemoryTaggingSize);
   return ret;
 }
 
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if BUILDFLAG(TMEMK_ENABLE)
+
+[[maybe_unused]] uint32_t inline _get_random(size_t min, size_t max){
+  if(min == max){
+    return min;
+  }
+  // all slots within a span/superpage are initialized/tagged in order
+  // thus we remember the last value to ensure that no two adjacent slots get
+  // (initially) tagged with the same tag.
+  static uint32_t rnd_previous = 0;
+  uint64_t rnd = 0;
+  do {
+    // int ok = _rdrand32_step(&rnd);
+    // int ok = __builtin_ia32_rdrand64_step(&rnd);
+    unsigned char ok = 1;
+    /* rdrand32 edx */
+    // asm volatile(".byte 0x0f,0xc7,0xf0; setc %1" : "=a" (rnd), "=qm" (ok) : : "edx" );
+    //rdrand64
+    __asm__ __volatile__
+    (
+        "1:\n"
+        ".byte 0x48, 0x0f, 0xc7, 0xf0;\n"
+        "jnc 1b;\n"
+        : "=a" (rnd)
+        : : "cc"
+    );
+    rnd = (rnd % (max + 1 - min)) + min;
+
+    if(ok == 1 && rnd >= min && rnd <= max && rnd != rnd_previous){
+      break;
+    }
+  } while(1);
+  rnd_previous = rnd;
+  return (uint32_t)rnd;
+}
+
+[[maybe_unused]] uint32_t inline _address_to_starting_key(void* address){
+#if !BUILDFLAG(TMEMK_STARTING_TAGS)
+  PA_CHECK(0); // we should not call this function without starting tags
+#endif
+#if BUILDFLAG(TMEMK_NOTAGGING)
+  return 0;
+#else
+
+    uintptr_t address_untagged = UntagPtr(address);
+    SlotSpanMetadata * slot_span = SlotSpanMetadata::FromAddr((uintptr_t)address_untagged);
+    PartitionSuperPageExtentEntry* superPageExtent = slot_span->ToSuperPageExtent();
+    // auto* page = PartitionPage::FromAddr(address_untagged);
+    size_t slot_size = slot_span->GetUtilizedSlotSize(); //basically just returns bucket->slot_size
+
+    // const bool use_tagging = root->IsMemoryTaggingEnabled() && ...
+    // settings.memory_tagging_enabled_
+    const bool use_tagging = slot_size <= kMaxMemoryTaggingSize;
+    if(!use_tagging){
+      PA_DCHECK(0);
+      return 0;
+    }
+
+    //assuming PartitionSuperPageExtentEntry is zero initialized
+    if(superPageExtent->tmemk_tag1 == 0 || superPageExtent->tmemk_tag2 == 0){
+      #if BUILDFLAG(TMEMK_STARTING_TAGS_RANDOM)
+        if(TMEMK_KEY_MIN == TMEMK_KEY_MAX || TMEMK_KEY_MAX == 1){
+          //cannot have different starting tags if we only have 1 key
+          superPageExtent->tmemk_tag1 = TMEMK_KEY_MIN;
+          superPageExtent->tmemk_tag2 = TMEMK_KEY_MAX;
+        }else{
+          PA_DCHECK(TMEMK_KEY_MAX-1 >= TMEMK_KEY_MIN);
+          superPageExtent->tmemk_tag1 = _get_random(TMEMK_KEY_MIN, TMEMK_KEY_MAX-1);
+          superPageExtent->tmemk_tag2 = superPageExtent->tmemk_tag1 + 1;
+          //even/odd keys so we can always +=2 and always have different adjacent tags without storing metadata.
+        }
+      #else
+        superPageExtent->tmemk_tag1 = TMEMK_KEY_MIN;
+        superPageExtent->tmemk_tag2 = (TMEMK_KEY_MIN <= 2 && 2 <= TMEMK_KEY_MAX) ? 2 : superPageExtent->tmemk_tag1;
+      #endif
+      #if BUILDFLAG(TMEMK_STARTING_TAGS_SAME)
+        superPageExtent->tmemk_tag2 = superPageExtent->tmemk_tag1;
+      #endif
+    }
+  uint32_t tag = (((size_t)address_untagged / slot_size) % 2) ? superPageExtent->tmemk_tag1 : superPageExtent->tmemk_tag2;
+  PA_DCHECK(tag != 0);
+  PA_DCHECK(superPageExtent->tmemk_tag1 != 0);
+  PA_DCHECK(superPageExtent->tmemk_tag2 != 0);
+  return tag;
+#endif
+}
+
+void* TagRegionRandomlyForMTE(void* ptr, size_t sz, uint64_t mask) {
+  // Randomly tag a region (MTE-enabled systems only). The first 16-byte
+  // granule is randomly tagged, all other granules in the region are
+  // then assigned that initial tag via __arm_mte_set_tag.
+  if (!CheckTagRegionParameters(ptr, sz)) {
+    return nullptr;
+  }
+
+#if BUILDFLAG(TMEMK_NOTAGGING)
+  return ptr;
+#else
+  PA_DCHECK(sz <= kMaxMemoryTaggingSize);
+
+  uint32_t tag = 0;
+#if BUILDFLAG(TMEMK_THREAD_ISOLATION)
+  if(!g_thread_key){
+    // g_thread_key = _get_random(TMEMK_KEY_MIN, TMEMK_KEY_MAX);
+    g_thread_key = g_thread_key_last.fetch_add(1, std::memory_order_relaxed);
+    g_thread_key = (g_thread_key % (TMEMK_KEY_MAX + 1 - TMEMK_KEY_MIN)) + TMEMK_KEY_MIN;
+  }
+  tag = g_thread_key;
+  PA_DCHECK(tag);
+#elif BUILDFLAG(TMEMK_STARTING_TAGS)
+  tag = _address_to_starting_key(ptr);
+#else
+  tag = _get_random(TMEMK_KEY_MIN, TMEMK_KEY_MAX);
+#endif
+
+  void * nptr = static_cast<void*>(tmemk_add_tag_to_ptr(ptr, tag));
+  PA_CHECK((uintptr_t)nptr % kMemTagGranuleSize == 0 && sz % kMemTagGranuleSize == 0);
+  MemZeroWithMovdir64B_q((uintptr_t)nptr, sz);
+  // __debug("TagRegionRandomlyForMTE(%p, %lu). returning %p", ptr, sz, nptr);
+  return nptr;
+#endif
+}
+
+[[maybe_unused]] uint64_t inline _getNextTag(void* ptr){
+#if ! BUILDFLAG(TMEMK_QUARANTINING)
+  [[maybe_unused]] const bool has_quarantine = false;
+#else
+  // [[maybe_unused]] const bool has_quarantine = ShouldQuarantine();
+  [[maybe_unused]] const bool has_quarantine = true;
+#endif
+
+#if BUILDFLAG(TMEMK_THREAD_ISOLATION)
+  // return 0;
+  uint64_t tag_old = tmemk_get_tag_from_ptr(ptr);
+  return tag_old;
+#elif BUILDFLAG(TMEMK_NEVER_INCREMENT)
+  uint64_t tag_old = tmemk_get_tag_from_ptr(ptr);
+  return tag_old;
+#elif ! BUILDFLAG(TMEMK_STARTING_TAGS)
+  uint64_t tag_old = tmemk_get_tag_from_ptr(ptr);
+  uint64_t tag_new = ((tag_old + 1 - TMEMK_KEY_MIN) % (TMEMK_KEY_MAX + 1 - TMEMK_KEY_MIN)) + TMEMK_KEY_MIN;
+  if((tag_old >= TMEMK_KEY_MAX || TMEMK_KEY_MIN == TMEMK_KEY_MAX) && has_quarantine){
+    tag_new = TMEMK_KEY_OVERFLOW;
+  }
+  PA_DCHECK(tag_new);
+  return tag_new;
+#else // TMEMK_STARTING_TAGS
+  uint64_t tag_initial = _address_to_starting_key(ptr);
+  uint64_t tag_old = tmemk_get_tag_from_ptr(ptr);
+#if BUILDFLAG(TMEMK_STARTING_TAGS_SAME)
+  uint64_t increment = 1;
+#else
+  uint64_t increment = 2;
+
+#endif
+  uint64_t tag_new = ((tag_old + increment - TMEMK_KEY_MIN) % (TMEMK_KEY_MAX + 1 - TMEMK_KEY_MIN)) + TMEMK_KEY_MIN;
+  // __debug_always("_getNextTag(%p) increment=%zu tag_initial=%zu tag_old=%zu tag_new=%zu TMEMK_KEY_MIN=%zu TMEMK_KEY_MAX=%zu", ptr, increment, tag_initial, tag_old, tag_new, TMEMK_KEY_MIN, TMEMK_KEY_MAX);
+
+  //if starting tags arent the same we want to make sure that adjacent tags are never the same
+  //we already made sure that the starting keys are even/odd for even/odd slots, so we just need to += 2
+  if(tag_new == tag_initial && has_quarantine){
+    tag_new = TMEMK_KEY_OVERFLOW;
+    // __debug("_getNextTag(%p) setting tag to TMEMK_KEY_OVERFLOW", ptr);
+  }
+  PA_DCHECK(tag_new);
+  return tag_new;
+#endif
+}
+
+void* TagRegionIncrementForMTE(void* ptr, size_t sz) {
+  // Increment a region's tag (MTE-enabled systems only), using the tag of the
+  // first granule.
+  //Note this is only called during free. So if TMEMK_THREAD_ISOLATION is enabled we just set it back to a default tag.
+
+  if (!CheckTagRegionParameters(ptr, sz)) {
+    return nullptr;
+  }
+#if BUILDFLAG(TMEMK_NOTAGGING)
+  return ptr;
+#else /* BUILDFLAG(TMEMK_NOTAGGING) */
+  uint64_t tag_new = _getNextTag(ptr);
+  void* ptr_new = tmemk_add_tag_to_ptr(ptr, tag_new);
+  // __debug_always("TagRegionIncrementForMTE(%p, %lu):%d returning %p. (old tag %lu new tag %lu)", ptr, sz, __LINE__, ptr_new, tmemk_get_tag_from_ptr(ptr), tmemk_get_tag_from_ptr(ptr_new));
+
+#if BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+  // memset happens not at free (here) but at allocation instead (if zeroed memory was requested)
+#elif BUILDFLAG(TMEMK_THREAD_ISOLATION) && !BUILDFLAG(TMEMK_MEMZERO_INSTEAD_OF_MEMCPY_AT_FREE)
+  PA_CHECK(ptr == ptr_new);
+  // No need to memcopy if the tag didnt change.
+#else // BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+  // Note: MTE implementation does not memset to zero because it would wipe the extras/cookie.
+  //   PartitionFreeListEntry shouldnt be an issue since that gets created afterwards. but care must be taken to memset the size of PartitionFreeListEntry when returning the memory again.
+  PA_CHECK((uintptr_t)ptr_new % kMemTagGranuleSize == 0 && sz % kMemTagGranuleSize == 0);
+  #if BUILDFLAG(TMEMK_FLUSH_OLD_AFTER_FREE)
+    MemFlush(ptr, sz);
+  #endif // BUILDFLAG(TMEMK_FLUSH_OLD_AFTER_FREE)
+  #if BUILDFLAG(TMEMK_MEMZERO_INSTEAD_OF_MEMCPY_AT_FREE)
+    MemZeroWithMovdir64B_q(ptr_new, sz);
+  #else
+    MemCopyInPlaceWithMovdir64B_q(ptr_new, ptr, sz);
+  #endif
+#endif // BUILDFLAG(TMEMK_NEVER_INCREMENT) && BUILDFLAG(TMEMK_SKIP_MEMSET_WHEN_NEVER_INCREMENT)
+  return ptr_new;
+#endif /* BUILDFLAG(TMEMK_NOTAGGING) */
+}
+
+void* RemaskVoidPtrForMTE(void* ptr) {
+  PA_CHECK(0); //cannot read keys
+  return nullptr;
+}
+#else  // BUILDFLAG(TMEMK_ENABLE)
 void* TagRegionRandomlyForMTE(void* ptr, size_t sz, uint64_t mask) {
   // Randomly tag a region (MTE-enabled systems only). The first 16-byte
   // granule is randomly tagged, all other granules in the region are
@@ -162,20 +612,21 @@ void* RemaskVoidPtrForMTE(void* ptr) {
   }
   return nullptr;
 }
+#endif  // BUILDFLAG(TMEMK_ENABLE)
 
-void* TagRegionIncrementNoOp(void* ptr, size_t sz) {
+[[maybe_unused]] void* TagRegionIncrementNoOp(void* ptr, size_t sz) {
   // Region parameters are checked even on non-MTE systems to check the
   // intrinsics are used correctly.
   return ptr;
 }
 
-void* TagRegionRandomlyNoOp(void* ptr, size_t sz, uint64_t mask) {
+[[maybe_unused]] void* TagRegionRandomlyNoOp(void* ptr, size_t sz, uint64_t mask) {
   // Verifies a 16-byte aligned tagging granule, size tagging granule (all
   // architectures).
   return ptr;
 }
 
-void* RemaskVoidPtrNoOp(void* ptr) {
+[[maybe_unused]] void* RemaskVoidPtrNoOp(void* ptr) {
   return ptr;
 }
 #endif
@@ -190,6 +641,22 @@ using TagMemoryRangeRandomlyInternalFn = void*(void* ptr,
                                                size_t size,
                                                uint64_t mask);
 
+#if BUILDFLAG(TMEMK_ENABLE)
+extern "C" TagMemoryRangeIncrementInternalFn(
+    *ResolveTagMemoryRangeIncrement(uint64_t hwcap, struct __ifunc_arg_t* hw)) {
+  return TagRegionIncrementForMTE;
+}
+
+extern "C" TagMemoryRangeRandomlyInternalFn(
+    *ResolveTagMemoryRandomly(uint64_t hwcap, struct __ifunc_arg_t* hw)) {
+  return TagRegionRandomlyForMTE;
+}
+
+extern "C" RemaskPtrInternalFn(
+    *ResolveRemaskPointer(uint64_t hwcap, struct __ifunc_arg_t* hw)) {
+  return RemaskVoidPtrForMTE;
+}
+#else  // BUILDFLAG(TMEMK_ENABLE)
 extern "C" TagMemoryRangeIncrementInternalFn(
     *ResolveTagMemoryRangeIncrement(uint64_t hwcap, struct __ifunc_arg_t* hw)) {
   if ((hwcap & _IFUNC_ARG_HWCAP) && (hw->_hwcap2 & HWCAP2_MTE)) {
@@ -213,6 +680,7 @@ extern "C" RemaskPtrInternalFn(
   }
   return RemaskVoidPtrNoOp;
 }
+#endif  // BUILDFLAG(TMEMK_ENABLE)
 
 void* TagMemoryRangeIncrementInternal(void* ptr, size_t size)
     __attribute__((ifunc("ResolveTagMemoryRangeIncrement")));
@@ -223,6 +691,9 @@ void* RemaskPointerInternal(void* ptr)
 #endif // PA_CONFIG(HAS_MEMORY_TAGGING)
 
 TagViolationReportingMode GetMemoryTaggingModeForCurrentThread() {
+#if PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(TMEMK_ENABLE)
+  return TagViolationReportingMode::kUndefined;
+#else
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
   base::CPU cpu;
   if (!cpu.has_mte()) {
@@ -242,6 +713,7 @@ TagViolationReportingMode GetMemoryTaggingModeForCurrentThread() {
 #else
   return TagViolationReportingMode::kUndefined;
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
+#endif  // PA_CONFIG(HAS_MEMORY_TAGGING) && BUILDFLAG(TMEMK_ENABLE)
 }
 
 }  // namespace internal
diff --git a/tagging.h b/tagging.h
index 858c5c3..091f8ec 100644
--- a/tagging.h
+++ b/tagging.h
@@ -15,6 +15,10 @@
 #include "base/allocator/partition_allocator/partition_alloc_base/component_export.h"
 #include "base/allocator/partition_allocator/partition_alloc_config.h"
 #include "build/build_config.h"
+#include "base/allocator/partition_allocator/partition_alloc_base/logging.h"
+#include "base/allocator/partition_allocator/partition_alloc_check.h" // PA_CHECK
+
+// #include "base/allocator/partition_allocator/partition_freelist_entry.h"
 
 namespace partition_alloc {
 
@@ -35,16 +39,116 @@ enum class TagViolationReportingMode {
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 void ChangeMemoryTaggingModeForCurrentThread(TagViolationReportingMode);
 
+struct TaggedSlot {
+  void* __value;
+  template <typename T> explicit TaggedSlot( T new_value ) {
+    __value = (void*)new_value;
+  }
+  void* value(){
+    return __value;
+  }
+};
+struct UntaggedSlot {
+  uintptr_t __value;
+  template <typename T> explicit UntaggedSlot( T new_value ) {
+    __value = (uintptr_t)new_value;
+  }
+  uintptr_t value(){
+    return __value;
+  }
+};
+
 namespace internal {
 
+#if BUILDFLAG(ENABLE_BACKUP_REF_PTR_SUPPORT) || \
+    BUILDFLAG(ENABLE_BACKUP_REF_PTR_SLOW_CHECKS) || \
+    BUILDFLAG(ENABLE_BACKUP_REF_PTR_FEATURE_FLAG) || \
+    BUILDFLAG(ENABLE_DANGLING_RAW_PTR_CHECKS) || \
+    BUILDFLAG(ENABLE_DANGLING_RAW_PTR_FEATURE_FLAG) || \
+    BUILDFLAG(ENABLE_DANGLING_RAW_PTR_PERF_EXPERIMENT) || \
+    BUILDFLAG(ENABLE_POINTER_SUBTRACTION_CHECK) || \
+    BUILDFLAG(BACKUP_REF_PTR_POISON_OOB_PTR) || \
+    BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT) || \
+    BUILDFLAG(USE_ASAN_BACKUP_REF_PTR) || \
+    BUILDFLAG(USE_ASAN_UNOWNED_PTR) || \
+    BUILDFLAG(USE_HOOKABLE_RAW_PTR) || \
+    BUILDFLAG(ENABLE_GWP_ASAN_SUPPORT) || \
+    BUILDFLAG(FORCIBLY_ENABLE_BACKUP_REF_PTR_IN_ALL_PROCESSES) || \
+    BUILDFLAG(FORCE_ENABLE_RAW_PTR_EXCLUSION) || \
+    BUILDFLAG(RECORD_ALLOC_INFO) || \
+    BUILDFLAG(USE_FREESLOT_BITMAP) || \
+    BUILDFLAG(GLUE_CORE_POOLS) || \
+    BUILDFLAG(ENABLE_POINTER_COMPRESSION) || \
+    BUILDFLAG(ENABLE_SHADOW_METADATA_FOR_64_BITS_POINTERS) || \
+    BUILDFLAG(USE_STARSCAN) || \
+    BUILDFLAG(PCSCAN_STACK_SUPPORTED) || \
+    0
+  XXX all of these options probably do not work with TMEMK_ENABLE. untested.
+#endif
+
+
+
+#if BUILDFLAG(TMEMK_ENABLE)
+#if BUILDFLAG(TMEMK_PADDING)
+constexpr int kMemTagGranuleSize = 64u;
+#else  // BUILDFLAG(TMEMK_PADDING)
 constexpr int kMemTagGranuleSize = 16u;
+static_assert( ! BUILDFLAG(TMEMK_INTEGRITY), "cannot have integrity with 16B");
+#endif  // BUILDFLAG(TMEMK_PADDING)
+#else  // BUILDFLAG(TMEMK_ENABLE)
+constexpr int kMemTagGranuleSize = 16u;
+#endif  // BUILDFLAG(TMEMK_ENABLE)
+
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
+#if BUILDFLAG(TMEMK_ENABLE)
+static_assert(BUILDFLAG(TMEMK_KEYBITS) > 0, "keybits must be > 0");
+static_assert(BUILDFLAG(TMEMK_KEYBITS) > 1 || ! BUILDFLAG(TMEMK_QUARANTINING), "keybits must be > 0");
+
+constexpr uint64_t TMEMK_KEY_BITS = BUILDFLAG(TMEMK_KEYBITS); // 2;
+constexpr uint64_t TMEMK_KEY_OVERFLOW = ((1uLL << TMEMK_KEY_BITS) - 1);
+constexpr uint64_t TMEMK_KEY_MAX = TMEMK_KEY_OVERFLOW > 1 ? (TMEMK_KEY_OVERFLOW - 1) : 1;
+constexpr uint64_t TMEMK_KEY_MIN = 1;
+// constexpr uint64_t kPtrTagMask = 0xff00000000000000uLL;
+constexpr uint64_t kPtrTagShift = 40;
+constexpr uint64_t kPtrTagMask   = 0x00003f0000000000uLL; // bits 45:40
+static_assert(BUILDFLAG(TMEMK_KEYBITS) <= 6, "keybits must be <= 6 otherwise fix kPtrTagMask and aslr thingy");
+
+#else  // BUILDFLAG(TMEMK_ENABLE)
 constexpr uint64_t kPtrTagMask = 0xff00000000000000uLL;
+constexpr uint64_t kPtrTagShift = 64-8; //uppermost 8 bits
+#endif  // BUILDFLAG(TMEMK_ENABLE)
 #else
+constexpr uint64_t kPtrTagShift = 0;
 constexpr uint64_t kPtrTagMask = 0;
 #endif  // PA_CONFIG(HAS_MEMORY_TAGGING)
+
 constexpr uint64_t kPtrUntagMask = ~kPtrTagMask;
 
+void tmemk_allocate_keys(size_t length);
+int tmemk_get_memfd();
+void tmemk_create_aliases_for_memory(uintptr_t address, size_t size, int access_flag, int map_flags, int fd, size_t offset, int commit);
+
+PA_ALWAYS_INLINE void* tmemk_add_tag_to_ptr(const void* ptr, size_t key){
+  uint64_t ptr_untagged = (uint64_t)ptr & ~kPtrTagMask;
+
+#if BUILDFLAG(TMEMK_ENABLE)
+  if(key){
+    PA_DCHECK( (key << kPtrTagShift) == ((key << kPtrTagShift) & kPtrTagMask) );
+    PA_DCHECK(key >= TMEMK_KEY_MIN);
+    PA_DCHECK(key <= TMEMK_KEY_OVERFLOW);
+  }
+#endif
+
+  void * p = (void*)((uintptr_t)ptr_untagged | (key << kPtrTagShift));
+  return p;
+}
+
+PA_ALWAYS_INLINE uint64_t tmemk_get_tag_from_ptr(const void* ptr){
+  uint64_t tag = ((uint64_t)ptr & kPtrTagMask) >> kPtrTagShift;
+  return tag;
+}
+
+
 #if BUILDFLAG(IS_ANDROID)
 // Changes the memory tagging mode for all threads in the current process.
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
@@ -56,8 +160,8 @@ void ChangeMemoryTaggingModeForAllThreadsPerProcess(TagViolationReportingMode);
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 TagViolationReportingMode GetMemoryTaggingModeForCurrentThread();
 
-// These forward-defined functions do not really exist in tagging.cc, they're resolved
-// by the dynamic linker to MTE-capable versions on the right hardware.
+// These forward-defined functions do not really exist in tagging.cc, they're
+// resolved by the dynamic linker to MTE-capable versions on the right hardware.
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
 PA_COMPONENT_EXPORT(PARTITION_ALLOC)
 void* TagMemoryRangeIncrementInternal(void* ptr, size_t size);
@@ -70,39 +174,39 @@ void* RemaskPointerInternal(void* ptr);
 // Increments the tag of the memory range ptr. Useful for provable revocations
 // (e.g. free). Returns the pointer with the new tag. Ensures that the entire
 // range is set to the same tag.
-// TODO(bartekn): Remove the T* variant.
-// TODO(bartekn): Consider removing the return value.
-template <typename T>
-PA_ALWAYS_INLINE T* TagMemoryRangeIncrement(T* ptr, size_t size) {
+PA_ALWAYS_INLINE void* TagMemoryRangeIncrement(void* ptr, size_t size) {
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
-  return reinterpret_cast<T*>(TagMemoryRangeIncrementInternal(ptr, size));
+  return TagMemoryRangeIncrementInternal(ptr, size);
 #else
   return ptr;
 #endif
 }
-PA_ALWAYS_INLINE void* TagMemoryRangeIncrement(uintptr_t ptr, size_t size) {
-  return TagMemoryRangeIncrement(reinterpret_cast<void*>(ptr), size);
+
+PA_ALWAYS_INLINE void* TagMemoryRangeIncrement(uintptr_t address, size_t size) {
+  return TagMemoryRangeIncrement(reinterpret_cast<void*>(address), size);
 }
 
 // Randomly changes the tag of the ptr memory range. Useful for initial random
 // initialization. Returns the pointer with the new tag. Ensures that the entire
 // range is set to the same tag.
-// TODO(bartekn): Remove the T* variant.
-template <typename T>
-PA_ALWAYS_INLINE T* TagMemoryRangeRandomly(T* ptr,
-                                           size_t size,
-                                           uint64_t mask = 0u) {
+PA_ALWAYS_INLINE void* TagMemoryRangeRandomly(uintptr_t address,
+                                              size_t size,
+                                              uint64_t mask = 0u) {
+  void* ptr = reinterpret_cast<void*>(address);
 #if PA_CONFIG(HAS_MEMORY_TAGGING)
-  return reinterpret_cast<T*>(TagMemoryRangeRandomlyInternal(ptr, size, mask));
+  return reinterpret_cast<void*>(
+      TagMemoryRangeRandomlyInternal(ptr, size, mask));
 #else
   return ptr;
 #endif
 }
-PA_ALWAYS_INLINE void* TagMemoryRangeRandomly(uintptr_t ptr,
-                                              size_t size,
-                                              uint64_t mask = 0u) {
-  return TagMemoryRangeRandomly(reinterpret_cast<void*>(ptr), size, mask);
+
+#if BUILDFLAG(TMEMK_THREAD_ISOLATION)
+PA_ALWAYS_INLINE TaggedSlot TagRegionForThread(uintptr_t address, size_t sz){
+  // note: logic for key per thread inside TagMemoryRangeRandomly
+  return (TaggedSlot)TagMemoryRangeRandomly(address, sz);
 }
+#endif
 
 // Gets a version of ptr that's safe to dereference.
 template <typename T>
@@ -129,6 +233,126 @@ PA_ALWAYS_INLINE uintptr_t UntagAddr(uintptr_t address) {
 #endif
 }
 
+#if BUILDFLAG(TMEMK_ENABLE)
+[[maybe_unused]] uint32_t _address_to_starting_key(void* address);
+[[maybe_unused]] uint64_t _getNextTag(void* ptr);
+
+PA_ALWAYS_INLINE bool HasOverflowTag(void* object) {
+  // The tag with which the slot is put to quarantine.
+  uint64_t tag_current = tmemk_get_tag_from_ptr(object);
+  bool ret = (tag_current == TMEMK_KEY_OVERFLOW);
+  // __debug("HasOverflowTag(%p) %d", object, ret);
+  return ret;
+}
+
+[[maybe_unused]] static PA_ALWAYS_INLINE void movdir64b_q(uintptr_t dst, const uintptr_t src){
+	PA_DCHECK((uintptr_t)src == (uintptr_t)((uintptr_t)src & ~(64-1)));
+	PA_DCHECK((uintptr_t)dst == (uintptr_t)((uintptr_t)dst & ~(64-1)));
+
+  typedef struct T64B { char _[64]; } T64B;
+
+	const T64B * __src = reinterpret_cast<T64B *>(src);
+	T64B * __dst = reinterpret_cast<T64B *>(dst);
+
+	asm volatile(".byte 0x66, 0x0f, 0x38, 0xf8, 0x02" //MOVDIR64B rdx -> rax
+			 : "+m" (*__dst)
+			 :  "m" (*__src), "a" (__dst), "d" (__src)
+			 : "memory"
+			 );
+}
+[[maybe_unused]] PA_ALWAYS_INLINE void MemZeroWithMovdir64B_q(uintptr_t dst, uint64_t len){
+	__attribute__((aligned(64))) const uint8_t ZeroBlock[64] = { 0 };
+	PA_DCHECK((uintptr_t)ZeroBlock == (uintptr_t)((uintptr_t)ZeroBlock & ~(64-1)));
+	PA_DCHECK((uintptr_t)dst == (uintptr_t)((uintptr_t)dst & ~(64-1)));
+	PA_DCHECK((len % 64) == 0);
+	asm volatile("mfence" : : : "memory");
+	for (uint32_t Idx = 0; Idx < (len / 64); Idx++) {
+		movdir64b_q(dst, (uintptr_t)ZeroBlock);
+		dst += 64;
+	}
+	asm volatile("mfence" : : : "memory");
+}
+[[maybe_unused]] PA_ALWAYS_INLINE void MemSetWithMovdir64B_q(uintptr_t dst, int c, uint64_t len){
+	__attribute__((aligned(64))) uint8_t constantBlock[64] = { 0 };
+	PA_DCHECK((uintptr_t)constantBlock == (uintptr_t)((uintptr_t)constantBlock & ~(64-1)));
+	PA_DCHECK((uintptr_t)dst == (uintptr_t)((uintptr_t)dst & ~(64-1)));
+	PA_DCHECK((len % 64) == 0);
+	asm volatile("mfence" : : : "memory");
+  for (size_t i = 0; i < 64; i++){
+    constantBlock[i] = (uint8_t)c;
+  }
+  // memset(constantBlock, c, _len);
+  for (uint32_t Idx = 0; Idx < (len / 64); Idx++) {
+    movdir64b_q(dst, (uintptr_t)constantBlock);
+    dst += 64;
+  }
+  asm volatile("mfence" : : : "memory");
+}
+[[maybe_unused]] PA_ALWAYS_INLINE void * MemCopyWithMovdir64B_q(uintptr_t dst, uintptr_t src, uint64_t len){
+	PA_DCHECK(dst != src);
+	PA_DCHECK((uintptr_t)src == (uintptr_t)((uintptr_t)src & ~(64-1)));
+	PA_DCHECK((uintptr_t)dst == (uintptr_t)((uintptr_t)dst & ~(64-1)));
+	PA_DCHECK((len % 64) == 0);
+  // __debug("MemCopyWithMovdir64B_q(%p, %p, %zu)", dst, src, len);
+  asm volatile("mfence" : : : "memory");
+  for (uint32_t Idx = 0; Idx < (len / 64); Idx++) {
+    movdir64b_q(dst, src);
+    src += 64;
+    dst += 64;
+  }
+  asm volatile("mfence" : : : "memory");
+  return reinterpret_cast<void*>(dst);
+}
+[[maybe_unused]] PA_ALWAYS_INLINE void * MemCopyInPlaceWithMovdir64B_q(uintptr_t dst, uintptr_t src, uint64_t len){
+
+	// PA_DCHECK(dst != src);
+	PA_DCHECK((uintptr_t)src == (uintptr_t)((uintptr_t)src & ~(64-1)));
+	PA_DCHECK((uintptr_t)dst == (uintptr_t)((uintptr_t)dst & ~(64-1)));
+	PA_DCHECK((len % 64) == 0);
+  asm volatile("mfence" : : : "memory");
+  for (uint32_t Idx = 0; Idx < (len / 64); Idx++) {
+    movdir64b_q(dst, src);
+    src += 64;
+    dst += 64;
+  }
+  asm volatile("mfence" : : : "memory");
+  return reinterpret_cast<void*>(dst);
+}
+
+[[maybe_unused]] PA_ALWAYS_INLINE void MemFlush(uintptr_t address, uint64_t len){
+	PA_DCHECK((uintptr_t)address == (uintptr_t)((uintptr_t)address & ~(64-1)));
+	PA_DCHECK((uintptr_t)len == (uintptr_t)((uintptr_t)len & ~(64-1)));
+	asm volatile("mfence" : : : "memory");
+	for (size_t i = 0; i < len; i += 64) {
+    asm volatile("clflushopt (%0)" : : "r"( address + i ) : "memory");
+	}
+	asm volatile("mfence" : : : "memory");
+}
+
+template <typename T>
+[[maybe_unused]] void MemZeroWithMovdir64B_q(T* dst, uint64_t len) {
+  MemZeroWithMovdir64B_q(reinterpret_cast<uintptr_t>(dst), len);
+}
+template <typename T>
+[[maybe_unused]] void MemSetWithMovdir64B_q(T* dst, int c, uint64_t len) {
+  MemSetWithMovdir64B_q(reinterpret_cast<uintptr_t>(dst), c, len);
+}
+template <typename T1, typename T2>
+[[maybe_unused]] T1* MemCopyWithMovdir64B_q(T1* dst, T2* src, uint64_t len) {
+  return reinterpret_cast<T1*>(MemCopyWithMovdir64B_q(reinterpret_cast<uintptr_t>(dst), reinterpret_cast<uintptr_t>(src), len));
+}
+template <typename T1, typename T2>
+[[maybe_unused]] T1* MemCopyInPlaceWithMovdir64B_q(T1* dst, T2* src, uint64_t len) {
+  return reinterpret_cast<T1*>(MemCopyInPlaceWithMovdir64B_q(reinterpret_cast<uintptr_t>(dst), reinterpret_cast<uintptr_t>(src), len));
+}
+template <typename T>
+[[maybe_unused]] void MemFlush(T* address, uint64_t len) {
+  MemFlush(reinterpret_cast<uintptr_t>(address), len);
+}
+
+#endif  // BUILDFLAG(TMEMK_ENABLE)
+
+
 }  // namespace internal
 
 // Strips the tag bits off |ptr|.
@@ -136,6 +360,9 @@ template <typename T>
 PA_ALWAYS_INLINE uintptr_t UntagPtr(T* ptr) {
   return internal::UntagAddr(reinterpret_cast<uintptr_t>(ptr));
 }
+PA_ALWAYS_INLINE UntaggedSlot UntagSlot(TaggedSlot slot) {
+  return (UntaggedSlot)internal::UntagAddr((uintptr_t)slot.value());
+}
 
 }  // namespace partition_alloc
 
diff --git a/third_party/apple_apsl/README.chromium b/third_party/apple_apsl/README.chromium
index 0a254d6..b85f096 100644
--- a/third_party/apple_apsl/README.chromium
+++ b/third_party/apple_apsl/README.chromium
@@ -2,7 +2,9 @@ Name: Darwin
 URL: https://www.opensource.apple.com/
 Version: unknown
 Security Critical: yes
+Shipped: yes
 License: Apple Public Source License 2.0
+License File: LICENSE
 
 Three files are excerpted here:
 
diff --git a/thread_cache.cc b/thread_cache.cc
index be511ed..eeacced 100644
--- a/thread_cache.cc
+++ b/thread_cache.cc
@@ -460,11 +460,10 @@ ThreadCache* ThreadCache::Create(PartitionRoot* root) {
 
   auto* bucket = root->buckets + PartitionRoot::SizeToBucketIndex(
                                      raw_size, root->GetBucketDistribution());
-  uintptr_t buffer = root->RawAlloc(bucket, AllocFlags::kZeroFill, raw_size,
+  TaggedSlot buffer_tagged = root->RawAlloc(bucket, AllocFlags::kZeroFill, raw_size,
                                     internal::PartitionPageSize(), &usable_size,
                                     &already_zeroed);
-  ThreadCache* tcache =
-      new (internal::SlotStartAddr2Ptr(buffer)) ThreadCache(root);
+  ThreadCache* tcache = new (buffer_tagged.value()) ThreadCache(root);
 
   // This may allocate.
   internal::PartitionTlsSet(internal::g_thread_cache_key, tcache);
@@ -534,7 +533,8 @@ void ThreadCache::Delete(void* tcache_ptr) {
   tcache->~ThreadCache();
   // TreadCache was allocated using RawAlloc() and SlotStartAddr2Ptr(), so it
   // shifted by extras, but is MTE-tagged.
-  root->RawFree(internal::SlotStartPtr2Addr(tcache_ptr));
+  // In other words: it is a TaggedSlot
+  root->RawFree((TaggedSlot)tcache_ptr);
 
 #if BUILDFLAG(IS_WIN)
   // On Windows, allocations do occur during thread/process teardown, make sure
@@ -615,7 +615,7 @@ void ThreadCache::FillBucket(size_t bucket_index) {
     // |raw_size| is set to the slot size, as we don't know it. However, it is
     // only used for direct-mapped allocations and single-slot ones anyway,
     // which are not handled here.
-    uintptr_t slot_start = root_->AllocFromBucket(
+    TaggedSlot slot_start_tagged = root_->AllocFromBucket(
         &root_->buckets[bucket_index],
         AllocFlags::kFastPathOrReturnNull | AllocFlags::kReturnNull,
         root_->buckets[bucket_index].slot_size /* raw_size */,
@@ -626,12 +626,12 @@ void ThreadCache::FillBucket(size_t bucket_index) {
     // some objects, then the allocation will be handled normally. Otherwise,
     // this goes to the central allocator, which will service the allocation,
     // return nullptr or crash.
-    if (!slot_start) {
+    if (!slot_start_tagged.value()) {
       break;
     }
 
     allocated_slots++;
-    PutInBucket(bucket, slot_start);
+    PutInBucket(bucket, slot_start_tagged);
   }
 
   cached_memory_ += allocated_slots * bucket.slot_size;
@@ -698,9 +698,10 @@ void ThreadCache::FreeAfter(internal::PartitionFreelistEntry* head,
   // acquisitions can be expensive.
   internal::ScopedGuard guard(internal::PartitionRootLock(root_));
   while (head) {
-    uintptr_t slot_start = internal::SlotStartPtr2Addr(head);
+    void* slot_start_tagged = head;
+    // uintptr_t slot_start = internal::SlotStartPtr2Addr(head);
     head = head->GetNextForThreadCache<crash_on_corruption>(slot_size);
-    root_->RawFreeLocked(slot_start);
+    root_->RawFreeLocked((TaggedSlot)slot_start_tagged);
   }
 }
 
diff --git a/thread_cache.h b/thread_cache.h
index 9ff1ba1..a02a971 100644
--- a/thread_cache.h
+++ b/thread_cache.h
@@ -276,7 +276,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) ThreadCache {
   // Returns true if the slot was put in the cache, and false otherwise. This
   // can happen either because the cache is full or the allocation was too
   // large.
-  PA_ALWAYS_INLINE bool MaybePutInCache(uintptr_t slot_start,
+  PA_ALWAYS_INLINE bool MaybePutInCache(TaggedSlot slot_start,
                                         size_t bucket_index,
                                         size_t* slot_size);
 
@@ -285,7 +285,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) ThreadCache {
   //
   // Has the same behavior as RawAlloc(), that is: no cookie nor ref-count
   // handling. Sets |slot_size| to the allocated size upon success.
-  PA_ALWAYS_INLINE uintptr_t GetFromCache(size_t bucket_index,
+  PA_ALWAYS_INLINE TaggedSlot GetFromCache(size_t bucket_index,
                                           size_t* slot_size);
 
   // Asks this cache to trigger |Purge()| at a later point. Can be called from
@@ -381,7 +381,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) ThreadCache {
   template <bool crash_on_corruption>
   void ClearBucketHelper(Bucket& bucket, size_t limit);
   void ClearBucket(Bucket& bucket, size_t limit);
-  PA_ALWAYS_INLINE void PutInBucket(Bucket& bucket, uintptr_t slot_start);
+  PA_ALWAYS_INLINE void PutInBucket(Bucket& bucket, TaggedSlot slot_start);
   void ResetForTesting();
   // Releases the entire freelist starting at |head| to the root.
   template <bool crash_on_corruption>
@@ -466,7 +466,7 @@ class PA_COMPONENT_EXPORT(PARTITION_ALLOC) ThreadCache {
   PA_FRIEND_TEST_ALL_PREFIXES(PartitionAllocThreadCacheTest, ClearFromTail);
 };
 
-PA_ALWAYS_INLINE bool ThreadCache::MaybePutInCache(uintptr_t slot_start,
+PA_ALWAYS_INLINE bool ThreadCache::MaybePutInCache(TaggedSlot slot_start,
                                                    size_t bucket_index,
                                                    size_t* slot_size) {
   PA_REENTRANCY_GUARD(is_in_thread_cache_);
@@ -481,6 +481,7 @@ PA_ALWAYS_INLINE bool ThreadCache::MaybePutInCache(uintptr_t slot_start,
 
   PA_DCHECK(bucket.count != 0 || bucket.freelist_head == nullptr);
 
+  // PA_CHECK(internal::UntagAddr(slot_start) != slot_start); // slot_start must be tagged already
   PutInBucket(bucket, slot_start);
   cached_memory_ += bucket.slot_size;
   PA_INCREMENT_COUNTER(stats_.cache_fill_hits);
@@ -503,7 +504,7 @@ PA_ALWAYS_INLINE bool ThreadCache::MaybePutInCache(uintptr_t slot_start,
   return true;
 }
 
-PA_ALWAYS_INLINE uintptr_t ThreadCache::GetFromCache(size_t bucket_index,
+PA_ALWAYS_INLINE TaggedSlot ThreadCache::GetFromCache(size_t bucket_index,
                                                      size_t* slot_size) {
 #if PA_CONFIG(THREAD_CACHE_ALLOC_STATS)
   stats_.allocs_per_bucket_[bucket_index]++;
@@ -515,7 +516,7 @@ PA_ALWAYS_INLINE uintptr_t ThreadCache::GetFromCache(size_t bucket_index,
   if (PA_UNLIKELY(bucket_index > largest_active_bucket_index_)) {
     PA_INCREMENT_COUNTER(stats_.alloc_miss_too_large);
     PA_INCREMENT_COUNTER(stats_.alloc_misses);
-    return 0;
+    return (TaggedSlot)NULL;
   }
 
   auto& bucket = buckets_[bucket_index];
@@ -531,7 +532,7 @@ PA_ALWAYS_INLINE uintptr_t ThreadCache::GetFromCache(size_t bucket_index,
     // Very unlikely, means that the central allocator is out of memory. Let it
     // deal with it (may return 0, may crash).
     if (PA_UNLIKELY(!bucket.freelist_head)) {
-      return 0;
+      return (TaggedSlot)NULL;
     }
   }
 
@@ -565,13 +566,14 @@ PA_ALWAYS_INLINE uintptr_t ThreadCache::GetFromCache(size_t bucket_index,
   PA_DCHECK(cached_memory_ >= bucket.slot_size);
   cached_memory_ -= bucket.slot_size;
 
-  return internal::SlotStartPtr2Addr(entry);
+  // return internal::SlotStartPtr2Addr(entry);
+  return (TaggedSlot)entry;
 }
 
 PA_ALWAYS_INLINE void ThreadCache::PutInBucket(Bucket& bucket,
-                                               uintptr_t slot_start) {
+                                               TaggedSlot slot_start) {
 #if PA_CONFIG(HAS_FREELIST_SHADOW_ENTRY) && defined(ARCH_CPU_X86_64) && \
-    BUILDFLAG(HAS_64_BIT_POINTERS)
+    BUILDFLAG(HAS_64_BIT_POINTERS) && !BUILDFLAG(TMEMK_ENABLE) //NOTE: "temporarily" disabled when using TMEMK in the hope we can get a tiny bit of extra performance by not doing this seemingly unnecessary poisoning.
   // We see freelist corruption crashes happening in the wild.  These are likely
   // due to out-of-bounds accesses in the previous slot, or to a Use-After-Free
   // somewhere in the code.
@@ -585,7 +587,11 @@ PA_ALWAYS_INLINE void ThreadCache::PutInBucket(Bucket& bucket,
   // TODO(lizeb): Make sure this does not hurt performance.
 
   // Everything below requires this alignment.
+#if BUILDFLAG(TMEMK_ENABLE) || 1
+  static_assert(internal::kAlignment >= 16, "");
+#else
   static_assert(internal::kAlignment == 16, "");
+#endif
 
   // The pointer is always 16 bytes aligned, so its start address is always == 0
   // % 16. Its distance to the next cacheline is
@@ -593,34 +599,54 @@ PA_ALWAYS_INLINE void ThreadCache::PutInBucket(Bucket& bucket,
   static_assert(
       internal::kPartitionCachelineSize == 64,
       "The computation below assumes that cache lines are 64 bytes long.");
-  int distance_to_next_cacheline_in_16_bytes = 4 - ((slot_start >> 4) & 3);
-  int slot_size_remaining_in_16_bytes =
+#if BUILDFLAG(TMEMK_ENABLE) || 1
+  int distance_to_next_cacheline_in_kAlignment_bytes = 64;
+#else
+  int distance_to_next_cacheline_in_kAlignment_bytes = 4 - (((uintptr_t)slot_start.value() >> 4) & 3);
+#endif
+  int slot_size_remaining_in_kAlignment_bytes =
 #if BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
       // When BRP is on in the "previous slot" mode, this slot may have a BRP
       // ref-count of the next, potentially allocated slot. Make sure we don't
       // overwrite it.
-      (bucket.slot_size - sizeof(PartitionRefCount)) / 16;
+      (bucket.slot_size - sizeof(PartitionRefCount)) / internal::kAlignment;
+#if BUILDFLAG(TMEMK_ENABLE) || 1
+      static_assert(sizeof(PartitionRefCount) % internal::kAlignment == 0 );
+      XXX is PUT_REF_COUNT_IN_PREVIOUS_SLOT enabled???
+#endif
+
 #else
-      bucket.slot_size / 16;
+      bucket.slot_size / internal::kAlignment;
 #endif  // BUILDFLAG(PUT_REF_COUNT_IN_PREVIOUS_SLOT)
 
-  slot_size_remaining_in_16_bytes = std::min(
-      slot_size_remaining_in_16_bytes, distance_to_next_cacheline_in_16_bytes);
+  slot_size_remaining_in_kAlignment_bytes = std::min(
+      slot_size_remaining_in_kAlignment_bytes, distance_to_next_cacheline_in_kAlignment_bytes);
+
+  __attribute__((aligned(64)))  static const uint32_t poison_64_bytes[16] = {
+    0xbadbad00, 0xbadbad00, 0xbadbad00, 0xbadbad00,
+    0xbadbad00, 0xbadbad00, 0xbadbad00, 0xbadbad00,
+    0xbadbad00, 0xbadbad00, 0xbadbad00, 0xbadbad00,
+    0xbadbad00, 0xbadbad00, 0xbadbad00, 0xbadbad00
+  };
+  static_assert(internal::kAlignment <= sizeof(poison_64_bytes));
 
-  static const uint32_t poison_16_bytes[4] = {0xbadbad00, 0xbadbad00,
-                                              0xbadbad00, 0xbadbad00};
   // Give a hint to the compiler in hope it'll vectorize the loop.
+  // PA_CHECK(internal::UntagAddr(slot_start) != slot_start); // slot_start must be tagged already
 #if PA_HAS_BUILTIN(__builtin_assume_aligned)
-  void* slot_start_tagged = __builtin_assume_aligned(
-      internal::SlotStartAddr2Ptr(slot_start), internal::kAlignment);
+  void* address_aligned = __builtin_assume_aligned(slot_start.value(), internal::kAlignment);
 #else
-  void* slot_start_tagged = internal::SlotStartAddr2Ptr(slot_start);
+  void* address_aligned = slot_start.value();
 #endif
-  uint32_t* address_aligned = static_cast<uint32_t*>(slot_start_tagged);
-  for (int i = 0; i < slot_size_remaining_in_16_bytes; i++) {
+  PA_DCHECK((uintptr_t)address_aligned % internal::kAlignment == 0);
+  for (int i = 0; i < slot_size_remaining_in_kAlignment_bytes; i++) {
+#if BUILDFLAG(TMEMK_ENABLE)
+    internal::MemCopyWithMovdir64B_q(address_aligned, poison_64_bytes, internal::kAlignment);
+    //TODO poisoning should not be necessary
+#else
     // Clang will expand the memcpy to a 16-byte write (movups on x86).
-    memcpy(address_aligned, poison_16_bytes, sizeof(poison_16_bytes));
-    address_aligned += 4;
+    memcpy(address_aligned, poison_64_bytes, internal::kAlignment);
+#endif
+    address_aligned = (void*)((uintptr_t)address_aligned + internal::kAlignment);
   }
 #endif  // PA_CONFIG(HAS_FREELIST_SHADOW_ENTRY) && defined(ARCH_CPU_X86_64) &&
         // BUILDFLAG(HAS_64_BIT_POINTERS)
diff --git a/thread_cache_unittest.cc b/thread_cache_unittest.cc
index aca2e31..7be1cd2 100644
--- a/thread_cache_unittest.cc
+++ b/thread_cache_unittest.cc
@@ -30,10 +30,10 @@
 
 namespace partition_alloc {
 
-using BucketDistribution = ThreadSafePartitionRoot::BucketDistribution;
+using BucketDistribution = PartitionRoot::BucketDistribution;
 namespace {
 
-constexpr size_t kSmallSize = 12;
+constexpr size_t kSmallSize = 33;  // Must be large enough to fit extras.
 constexpr size_t kDefaultCountForSmallBucket =
     ThreadCache::kSmallBucketBaseCount * ThreadCache::kDefaultMultiplier;
 constexpr size_t kFillCountForSmallBucket =
@@ -68,7 +68,6 @@ std::unique_ptr<PartitionAllocatorForTesting> CreateAllocator() {
     .thread_cache = PartitionOptions::ThreadCache::kEnabled,
 #endif  // BUILDFLAG(USE_PARTITION_ALLOC_AS_MALLOC)
     .quarantine = PartitionOptions::Quarantine::kAllowed,
-    .cookie = PartitionOptions::Cookie::kDisallowed,
   });
   allocator->root()->UncapEmptySlotSpanMemoryForTesting();
 
@@ -94,7 +93,7 @@ class PartitionAllocThreadCacheTest
 
  protected:
   void SetUp() override {
-    ThreadSafePartitionRoot* root = allocator_->root();
+    PartitionRoot* root = allocator_->root();
     switch (GetParam()) {
       case BucketDistribution::kDefault:
         root->ResetBucketDistributionForTesting();
@@ -131,14 +130,13 @@ class PartitionAllocThreadCacheTest
               GetBucketSizeForThreadCache());
   }
 
-  ThreadSafePartitionRoot* root() { return allocator_->root(); }
+  PartitionRoot* root() { return allocator_->root(); }
 
   // Returns the size of the smallest bucket fitting an allocation of
   // |sizeof(ThreadCache)| bytes.
   size_t GetBucketSizeForThreadCache() {
     size_t tc_bucket_index = root()->SizeToBucketIndex(
-        sizeof(ThreadCache),
-        ThreadSafePartitionRoot::BucketDistribution::kDefault);
+        sizeof(ThreadCache), PartitionRoot::BucketDistribution::kDefault);
     auto* tc_bucket = &root()->buckets[tc_bucket_index];
     return tc_bucket->slot_size;
   }
@@ -147,12 +145,13 @@ class PartitionAllocThreadCacheTest
     return PartitionRoot::SizeToBucketIndex(size, GetParam());
   }
 
-  size_t FillThreadCacheAndReturnIndex(size_t size, size_t count = 1) {
-    uint16_t bucket_index = SizeToIndex(size);
+  size_t FillThreadCacheAndReturnIndex(size_t raw_size, size_t count = 1) {
+    uint16_t bucket_index = SizeToIndex(raw_size);
     std::vector<void*> allocated_data;
 
     for (size_t i = 0; i < count; ++i) {
-      allocated_data.push_back(root()->Alloc(size, ""));
+      allocated_data.push_back(
+          root()->Alloc(root()->AdjustSizeForExtrasSubtract(raw_size), ""));
     }
     for (void* ptr : allocated_data) {
       root()->Free(ptr);
@@ -163,10 +162,9 @@ class PartitionAllocThreadCacheTest
 
   void FillThreadCacheWithMemory(size_t target_cached_memory) {
     for (int batch : {1, 2, 4, 8, 16}) {
-      for (size_t allocation_size = 1;
-           allocation_size <= ThreadCache::kLargeSizeThreshold;
-           allocation_size++) {
-        FillThreadCacheAndReturnIndex(allocation_size, batch);
+      for (size_t raw_size = root()->AdjustSizeForExtrasAdd(1);
+           raw_size <= ThreadCache::kLargeSizeThreshold; raw_size++) {
+        FillThreadCacheAndReturnIndex(raw_size, batch);
 
         if (ThreadCache::Get()->CachedMemory() >= target_cached_memory) {
           return;
@@ -192,7 +190,8 @@ TEST_P(PartitionAllocThreadCacheTest, Simple) {
   EXPECT_TRUE(tcache);
   DeltaCounter batch_fill_counter{tcache->stats_.batch_fill_count};
 
-  void* ptr = root()->Alloc(kSmallSize, "");
+  void* ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSmallSize), "");
   ASSERT_TRUE(ptr);
 
   uint16_t index = SizeToIndex(kSmallSize);
@@ -203,7 +202,8 @@ TEST_P(PartitionAllocThreadCacheTest, Simple) {
   // Freeing fills the thread cache.
   EXPECT_EQ(kFillCountForSmallBucket, tcache->bucket_count_for_testing(index));
 
-  void* ptr2 = root()->Alloc(kSmallSize, "");
+  void* ptr2 =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSmallSize), "");
   // MTE-untag, because Free() changes tag.
   EXPECT_EQ(UntagPtr(ptr), UntagPtr(ptr2));
   // Allocated from the thread cache.
@@ -216,7 +216,8 @@ TEST_P(PartitionAllocThreadCacheTest, Simple) {
 }
 
 TEST_P(PartitionAllocThreadCacheTest, InexactSizeMatch) {
-  void* ptr = root()->Alloc(kSmallSize, "");
+  void* ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSmallSize), "");
   ASSERT_TRUE(ptr);
 
   // There is a cache.
@@ -231,7 +232,8 @@ TEST_P(PartitionAllocThreadCacheTest, InexactSizeMatch) {
   // Freeing fills the thread cache.
   EXPECT_EQ(kFillCountForSmallBucket, tcache->bucket_count_for_testing(index));
 
-  void* ptr2 = root()->Alloc(kSmallSize + 1, "");
+  void* ptr2 =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSmallSize + 1), "");
   // MTE-untag, because Free() changes tag.
   EXPECT_EQ(UntagPtr(ptr), UntagPtr(ptr2));
   // Allocated from the thread cache.
@@ -277,14 +279,15 @@ TEST_P(PartitionAllocThreadCacheTest, NoCrossPartitionCache) {
   });
 
   size_t bucket_index = FillThreadCacheAndReturnIndex(kSmallSize);
-  void* ptr = allocator.root()->Alloc(kSmallSize, "");
+  void* ptr = allocator.root()->Alloc(
+      allocator.root()->AdjustSizeForExtrasSubtract(kSmallSize), "");
   ASSERT_TRUE(ptr);
 
   auto* tcache = root()->thread_cache_for_testing();
   EXPECT_EQ(kFillCountForSmallBucket,
             tcache->bucket_count_for_testing(bucket_index));
 
-  ThreadSafePartitionRoot::Free(ptr);
+  PartitionRoot::Free(ptr);
   EXPECT_EQ(kFillCountForSmallBucket,
             tcache->bucket_count_for_testing(bucket_index));
 }
@@ -325,18 +328,22 @@ TEST_P(PartitionAllocThreadCacheTest, DirectMappedReallocMetrics) {
             root()->get_total_size_of_allocated_bytes());
   EXPECT_EQ(expected_allocated_size, root()->get_max_size_of_allocated_bytes());
 
-  void* ptr = root()->Alloc(10 * internal::kMaxBucketed, "");
+  void* ptr = root()->Alloc(
+      root()->AdjustSizeForExtrasSubtract(10 * internal::kMaxBucketed), "");
 
   EXPECT_EQ(expected_allocated_size + 10 * internal::kMaxBucketed,
             root()->get_total_size_of_allocated_bytes());
 
-  void* ptr2 = root()->Realloc(ptr, 9 * internal::kMaxBucketed, "");
+  void* ptr2 = root()->Realloc(
+      ptr, root()->AdjustSizeForExtrasSubtract(9 * internal::kMaxBucketed), "");
 
   ASSERT_EQ(ptr, ptr2);
   EXPECT_EQ(expected_allocated_size + 9 * internal::kMaxBucketed,
             root()->get_total_size_of_allocated_bytes());
 
-  ptr2 = root()->Realloc(ptr, 10 * internal::kMaxBucketed, "");
+  ptr2 = root()->Realloc(
+      ptr, root()->AdjustSizeForExtrasSubtract(10 * internal::kMaxBucketed),
+      "");
 
   ASSERT_EQ(ptr, ptr2);
   EXPECT_EQ(expected_allocated_size + 10 * internal::kMaxBucketed,
@@ -347,7 +354,7 @@ TEST_P(PartitionAllocThreadCacheTest, DirectMappedReallocMetrics) {
 
 namespace {
 
-size_t FillThreadCacheAndReturnIndex(ThreadSafePartitionRoot* root,
+size_t FillThreadCacheAndReturnIndex(PartitionRoot* root,
                                      size_t size,
                                      BucketDistribution bucket_distribution,
                                      size_t count = 1) {
@@ -356,7 +363,8 @@ size_t FillThreadCacheAndReturnIndex(ThreadSafePartitionRoot* root,
   std::vector<void*> allocated_data;
 
   for (size_t i = 0; i < count; ++i) {
-    allocated_data.push_back(root->Alloc(size, ""));
+    allocated_data.push_back(
+        root->Alloc(root->AdjustSizeForExtrasSubtract(size), ""));
   }
   for (void* ptr : allocated_data) {
     root->Free(ptr);
@@ -374,7 +382,7 @@ class ThreadDelegateForMultipleThreadCaches
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
   ThreadDelegateForMultipleThreadCaches(ThreadCache* parent_thread_cache,
-                                        ThreadSafePartitionRoot* root,
+                                        PartitionRoot* root,
                                         BucketDistribution bucket_distribution)
       : parent_thread_tcache_(parent_thread_cache),
         root_(root),
@@ -391,7 +399,7 @@ class ThreadDelegateForMultipleThreadCaches
 
  private:
   ThreadCache* parent_thread_tcache_ = nullptr;
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   PartitionRoot::BucketDistribution bucket_distribution_;
 };
 
@@ -416,20 +424,20 @@ namespace {
 class ThreadDelegateForThreadCacheReclaimedWhenThreadExits
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
-  ThreadDelegateForThreadCacheReclaimedWhenThreadExits(
-      ThreadSafePartitionRoot* root,
-      void*& other_thread_ptr)
+  ThreadDelegateForThreadCacheReclaimedWhenThreadExits(PartitionRoot* root,
+                                                       void*& other_thread_ptr)
       : root_(root), other_thread_ptr_(other_thread_ptr) {}
 
   void ThreadMain() override {
     EXPECT_FALSE(root_->thread_cache_for_testing());  // No allocations yet.
-    other_thread_ptr_ = root_->Alloc(kMediumSize, "");
+    other_thread_ptr_ =
+        root_->Alloc(root_->AdjustSizeForExtrasSubtract(kMediumSize), "");
     root_->Free(other_thread_ptr_);
     // |other_thread_ptr| is now in the thread cache.
   }
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   void*& other_thread_ptr_;
 };
 
@@ -442,7 +450,8 @@ TEST_P(PartitionAllocThreadCacheTest, ThreadCacheReclaimedWhenThreadExits) {
   // Allocate enough objects to force a cache fill at the next allocation.
   std::vector<void*> tmp;
   for (size_t i = 0; i < kDefaultCountForMediumBucket / 4; i++) {
-    tmp.push_back(root()->Alloc(kMediumSize, ""));
+    tmp.push_back(
+        root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), ""));
   }
 
   void* other_thread_ptr = nullptr;
@@ -454,7 +463,8 @@ TEST_P(PartitionAllocThreadCacheTest, ThreadCacheReclaimedWhenThreadExits) {
                                                    &thread_handle);
   internal::base::PlatformThreadForTesting::Join(thread_handle);
 
-  void* this_thread_ptr = root()->Alloc(kMediumSize, "");
+  void* this_thread_ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
   // |other_thread_ptr| was returned to the central allocator, and is returned
   // here, as it comes from the freelist.
   EXPECT_EQ(UntagPtr(this_thread_ptr), UntagPtr(other_thread_ptr));
@@ -471,7 +481,7 @@ class ThreadDelegateForThreadCacheRegistry
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
   ThreadDelegateForThreadCacheRegistry(ThreadCache* parent_thread_cache,
-                                       ThreadSafePartitionRoot* root,
+                                       PartitionRoot* root,
                                        BucketDistribution bucket_distribution)
       : parent_thread_tcache_(parent_thread_cache),
         root_(root),
@@ -490,7 +500,7 @@ class ThreadDelegateForThreadCacheRegistry
 
  private:
   ThreadCache* parent_thread_tcache_ = nullptr;
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   BucketDistribution bucket_distribution_;
 };
 
@@ -541,7 +551,8 @@ TEST_P(PartitionAllocThreadCacheTest, RecordStats) {
   DeltaCounter cache_fill_misses_counter{tcache->stats_.cache_fill_misses};
 
   // Cache has been purged, first allocation is a miss.
-  void* data = root()->Alloc(kMediumSize, "");
+  void* data =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
   EXPECT_EQ(1u, alloc_counter.Delta());
   EXPECT_EQ(1u, alloc_miss_counter.Delta());
   EXPECT_EQ(0u, alloc_hits_counter.Delta());
@@ -582,7 +593,7 @@ class ThreadDelegateForMultipleThreadCachesAccounting
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
   ThreadDelegateForMultipleThreadCachesAccounting(
-      ThreadSafePartitionRoot* root,
+      PartitionRoot* root,
       const ThreadCacheStats& wqthread_stats,
       int alloc_count,
       BucketDistribution bucket_distribution)
@@ -613,7 +624,7 @@ class ThreadDelegateForMultipleThreadCachesAccounting
   }
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   BucketDistribution bucket_distribution_;
   const ThreadCacheStats wqthread_stats_;
   const int alloc_count_;
@@ -666,7 +677,7 @@ namespace {
 class ThreadDelegateForPurgeAll
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
-  ThreadDelegateForPurgeAll(ThreadSafePartitionRoot* root,
+  ThreadDelegateForPurgeAll(PartitionRoot* root,
                             ThreadCache*& other_thread_tcache,
                             std::atomic<bool>& other_thread_started,
                             std::atomic<bool>& purge_called,
@@ -691,7 +702,8 @@ class ThreadDelegateForPurgeAll
     EXPECT_EQ(kFillCountForSmallBucket,
               other_thread_tcache_->bucket_count_for_testing(bucket_index_));
     // Allocations do not trigger Purge().
-    void* data = root_->Alloc(kSmallSize, "");
+    void* data =
+        root_->Alloc(root_->AdjustSizeForExtrasSubtract(kSmallSize), "");
     EXPECT_EQ(kFillCountForSmallBucket - 1,
               other_thread_tcache_->bucket_count_for_testing(bucket_index_));
     // But deallocations do.
@@ -701,7 +713,7 @@ class ThreadDelegateForPurgeAll
   }
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   ThreadCache*& other_thread_tcache_;
   std::atomic<bool>& other_thread_started_;
   std::atomic<bool>& purge_called_;
@@ -796,15 +808,16 @@ TEST_P(PartitionAllocThreadCacheTest, PeriodicPurge) {
 
 namespace {
 
-void FillThreadCacheWithMemory(ThreadSafePartitionRoot* root,
+void FillThreadCacheWithMemory(PartitionRoot* root,
                                size_t target_cached_memory,
                                BucketDistribution bucket_distribution) {
   for (int batch : {1, 2, 4, 8, 16}) {
     for (size_t allocation_size = 1;
          allocation_size <= ThreadCache::kLargeSizeThreshold;
          allocation_size++) {
-      FillThreadCacheAndReturnIndex(root, allocation_size, bucket_distribution,
-                                    batch);
+      FillThreadCacheAndReturnIndex(
+          root, root->AdjustSizeForExtrasAdd(allocation_size),
+          bucket_distribution, batch);
 
       if (ThreadCache::Get()->CachedMemory() >= target_cached_memory) {
         return;
@@ -819,7 +832,7 @@ class ThreadDelegateForPeriodicPurgeSumsOverAllThreads
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
   ThreadDelegateForPeriodicPurgeSumsOverAllThreads(
-      ThreadSafePartitionRoot* root,
+      PartitionRoot* root,
       std::atomic<int>& allocations_done,
       std::atomic<bool>& can_finish,
       BucketDistribution bucket_distribution)
@@ -840,7 +853,7 @@ class ThreadDelegateForPeriodicPurgeSumsOverAllThreads
   }
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   std::atomic<int>& allocations_done_;
   std::atomic<bool>& can_finish_;
   BucketDistribution bucket_distribution_;
@@ -921,7 +934,8 @@ TEST_P(PartitionAllocThreadCacheTest, MAYBE_DynamicCountPerBucket) {
       ThreadCache::kDefaultMultiplier / 2);
   // No immediate batch deallocation.
   EXPECT_EQ(kDefaultCountForMediumBucket, tcache->buckets_[bucket_index].count);
-  void* data = root()->Alloc(kMediumSize, "");
+  void* data =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
   // Not triggered by allocations.
   EXPECT_EQ(kDefaultCountForMediumBucket - 1,
             tcache->buckets_[bucket_index].count);
@@ -985,7 +999,7 @@ class ThreadDelegateForDynamicCountPerBucketMultipleThreads
     : public internal::base::PlatformThreadForTesting::Delegate {
  public:
   ThreadDelegateForDynamicCountPerBucketMultipleThreads(
-      ThreadSafePartitionRoot* root,
+      PartitionRoot* root,
       std::atomic<bool>& other_thread_started,
       std::atomic<bool>& threshold_changed,
       int bucket_index,
@@ -1008,7 +1022,8 @@ class ThreadDelegateForDynamicCountPerBucketMultipleThreads
     while (!threshold_changed_.load(std::memory_order_acquire)) {
     }
 
-    void* data = root_->Alloc(kSmallSize, "");
+    void* data =
+        root_->Alloc(root_->AdjustSizeForExtrasSubtract(kSmallSize), "");
     // Deallocations trigger limit enforcement.
     root_->Free(data);
     // Since the bucket is too full, it gets halved by batched deallocation.
@@ -1017,7 +1032,7 @@ class ThreadDelegateForDynamicCountPerBucketMultipleThreads
   }
 
  private:
-  ThreadSafePartitionRoot* root_ = nullptr;
+  PartitionRoot* root_ = nullptr;
   std::atomic<bool>& other_thread_started_;
   std::atomic<bool>& threshold_changed_;
   const int bucket_index_;
@@ -1166,8 +1181,7 @@ TEST_P(PartitionAllocThreadCacheTest, MAYBE_Bookkeeping) {
   // The ThreadCache is allocated before we change buckets, so its size is
   // always based on the sparser distribution.
   size_t tc_bucket_index = root()->SizeToBucketIndex(
-      sizeof(ThreadCache),
-      ThreadSafePartitionRoot::BucketDistribution::kDefault);
+      sizeof(ThreadCache), PartitionRoot::BucketDistribution::kDefault);
   auto* tc_bucket = &root()->buckets[tc_bucket_index];
   size_t expected_allocated_size =
       tc_bucket->slot_size;  // For the ThreadCache itself.
@@ -1181,7 +1195,8 @@ TEST_P(PartitionAllocThreadCacheTest, MAYBE_Bookkeeping) {
             root()->get_total_size_of_allocated_bytes());
   EXPECT_EQ(expected_allocated_size, root()->get_max_size_of_allocated_bytes());
 
-  void* ptr = root()->Alloc(kMediumSize, "");
+  void* ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
 
   auto* medium_bucket = root()->buckets + SizeToIndex(kMediumSize);
   size_t medium_alloc_size = medium_bucket->slot_size;
@@ -1200,7 +1215,8 @@ TEST_P(PartitionAllocThreadCacheTest, MAYBE_Bookkeeping) {
 
   // These allocations all come from the thread-cache.
   for (size_t i = 0; i < kFillCountForMediumBucket; i++) {
-    arr[i] = root()->Alloc(kMediumSize, "");
+    arr[i] =
+        root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
     EXPECT_EQ(expected_committed_size, root()->total_size_of_committed_pages);
     EXPECT_EQ(expected_committed_size, root()->max_size_of_committed_pages);
     EXPECT_EQ(expected_allocated_size,
@@ -1233,7 +1249,8 @@ TEST_P(PartitionAllocThreadCacheTest, TryPurgeNoAllocs) {
 TEST_P(PartitionAllocThreadCacheTest, TryPurgeMultipleCorrupted) {
   auto* tcache = root()->thread_cache_for_testing();
 
-  void* ptr = root()->Alloc(kMediumSize, "");
+  void* ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kMediumSize), "");
 
   auto* medium_bucket = root()->buckets + SizeToIndex(kMediumSize);
 
@@ -1317,16 +1334,20 @@ TEST_P(PartitionAllocThreadCacheTest, AllocationRecording) {
   const size_t kSingleSlot = internal::PartitionPageSize() + 1;
 
   size_t expected_total_size = 0;
-  void* ptr = root()->Alloc(kSmallSize, "");
+  void* ptr =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSmallSize), "");
   ASSERT_TRUE(ptr);
   expected_total_size += root()->GetUsableSize(ptr);
-  void* ptr2 = root()->Alloc(kBucketedNotCached, "");
+  void* ptr2 = root()->Alloc(
+      root()->AdjustSizeForExtrasSubtract(kBucketedNotCached), "");
   ASSERT_TRUE(ptr2);
   expected_total_size += root()->GetUsableSize(ptr2);
-  void* ptr3 = root()->Alloc(kDirectMapped, "");
+  void* ptr3 =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kDirectMapped), "");
   ASSERT_TRUE(ptr3);
   expected_total_size += root()->GetUsableSize(ptr3);
-  void* ptr4 = root()->Alloc(kSingleSlot, "");
+  void* ptr4 =
+      root()->Alloc(root()->AdjustSizeForExtrasSubtract(kSingleSlot), "");
   ASSERT_TRUE(ptr4);
   expected_total_size += root()->GetUsableSize(ptr4);
 
@@ -1453,7 +1474,7 @@ TEST_P(PartitionAllocThreadCacheTest, AllocationRecordingRealloc) {
 // once we have enabled features.
 TEST(AlternateBucketDistributionTest, SwitchBeforeAlloc) {
   std::unique_ptr<PartitionAllocatorForTesting> allocator(CreateAllocator());
-  ThreadSafePartitionRoot* root = allocator->root();
+  PartitionRoot* root = allocator->root();
 
   root->SwitchToDenserBucketDistribution();
   constexpr size_t n = (1 << 12) * 3 / 2;
@@ -1476,7 +1497,7 @@ TEST(AlternateBucketDistributionTest, SwitchAfterAlloc) {
   EXPECT_NE(internal::BucketIndexLookup::GetIndex(n),
             internal::BucketIndexLookup::GetIndexForDefaultBuckets(n));
 
-  ThreadSafePartitionRoot* root = allocator->root();
+  PartitionRoot* root = allocator->root();
   void* ptr = root->Alloc(n, "");
 
   root->SwitchToDenserBucketDistribution();
